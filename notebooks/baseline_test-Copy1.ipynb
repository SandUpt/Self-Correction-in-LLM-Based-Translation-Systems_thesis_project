{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ef05db-13c9-4edf-8cab-ffa2ed322d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "import pandas as pd\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961ac604-f0fd-4ee3-be1d-20404637d237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f348a534-dcd4-4741-84d2-03db50ca3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"  # Replace with your actual token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856d336f-db73-4779-81c2-61969941ab06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "de_en_dataset = load_dataset(\"wmt14\", \"de-en\")\n",
    "\n",
    "hi_en_dataset = load_dataset(\"cfilt/iitb-english-hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf304bb-9276-4eaf-a058-d4600a2915ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en_model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "hi_en_model_name = \"Helsinki-NLP/opus-mt-hi-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d94d53d-6871-49bb-a647-5b3554d694ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea35fb8-e61e-452b-b0ee-8ea85804be3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA 2 model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c8bf0fff0949dab6edfc3b7e53449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading LLaMA 2 model and tokenizer...\")\n",
    "llama_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name, padding_side='left')\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03b7488-1505-4137-8e2d-00ddbef611b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_model.config.pad_token_id = llama_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ef6c3-250c-43ac-b258-471a03db2f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a1621bc-0cde-4001-9558-6c96de3eb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading German-English model and tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading German-English model and tokenizer...\")\n",
    "de_en_tokenizer = AutoTokenizer.from_pretrained(de_en_model_name)\n",
    "de_en_model = AutoModelForSeq2SeqLM.from_pretrained(de_en_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09daff82-a765-4b76-9a01-e73340a8c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hindi-English model and tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Hindi-English model and tokenizer...\")\n",
    "hi_en_tokenizer = AutoTokenizer.from_pretrained(hi_en_model_name)\n",
    "hi_en_model = AutoModelForSeq2SeqLM.from_pretrained(hi_en_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6808c4-1881-4bb0-8ca5-ec83429b951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing German-English data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing German-English data...\")\n",
    "de_train_data = de_en_dataset['train']\n",
    "german_sentences = [example['translation']['de'] for example in de_train_data]\n",
    "english_de_references = [example['translation']['en'] for example in de_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26ed640-2e07-43c7-bef9-b32935f4276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hindi-English dataset...\n",
      "Dataset structure: {'translation': {'en': Value(dtype='string', id=None), 'hi': Value(dtype='string', id=None)}}\n",
      "Sample data: {'translation': {'en': 'Give your application an accessibility workout', 'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Hindi-English dataset...\")\n",
    "hi_en_dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "print(\"Dataset structure:\", hi_en_dataset['train'].features)\n",
    "print(\"Sample data:\", hi_en_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3082281d-59c5-4657-abd9-f99786dd4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hindi-English data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Hindi-English data...\")\n",
    "hi_train_data = hi_en_dataset['train']\n",
    "hindi_sentences = [example['translation']['hi'] for example in hi_train_data]\n",
    "english_hi_references = [example['translation']['en'] for example in hi_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670e6b9f-cc8a-46cd-b51d-85affda51fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 2000\n",
    "german_subset = german_sentences[:subset_size]\n",
    "english_de_subset = english_de_references[:subset_size]\n",
    "hindi_subset = hindi_sentences[:subset_size]\n",
    "english_hi_subset = english_hi_references[:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "880b4d48-69ac-431d-851f-655a541e1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(sentences, model, tokenizer, batch_size=16):\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs = model.generate(**inputs)\n",
    "        translated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        translations.extend(translated_texts)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fddcec8-75d6-4267-93f9-fdf3ac3aa361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch_llama(sentences, model, tokenizer, batch_size=16):\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        prompts = [f\"Translate the following text to English: {text}\\nTranslation: \" for text in batch]\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,  \n",
    "            temperature=None,  \n",
    "            top_p=None,       \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        translated_texts = [\n",
    "            tokenizer.decode(output[inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            for output in outputs\n",
    "        ]\n",
    "        translations.extend(translated_texts)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e338117b-c400-410c-9739-6820e55b5bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating German to English...\n",
      "Translating Hindi to English...\n"
     ]
    }
   ],
   "source": [
    "print(\"Translating German to English...\")\n",
    "de_en_translations = translate_batch(german_subset, de_en_model, de_en_tokenizer)\n",
    "print(\"Translating Hindi to English...\")\n",
    "hi_en_translations = translate_batch(hindi_subset, hi_en_model, hi_en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a6bbffa-69d8-4fe0-b7d5-5416b5f671ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating German to English using LLaMA 2\n",
      "Translating Hindi to English using LLaMA 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Translating German to English using LLaMA 2\")\n",
    "llama_de_en_translations = translate_batch_llama(german_subset, llama_model, llama_tokenizer)\n",
    "\n",
    "print(\"Translating Hindi to English using LLaMA 2\")\n",
    "llama_hi_en_translations = translate_batch_llama(hindi_subset, llama_model, llama_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff2ac307-6b2d-411d-aead-3e207cb62ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU scores...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating BLEU scores...\")\n",
    "de_en_bleu = corpus_bleu(de_en_translations, [english_de_subset])\n",
    "hi_en_bleu = corpus_bleu(hi_en_translations, [english_hi_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c88b2af0-3813-49a1-8fa1-b2e185920ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German-English BLEU Score: 30.73252691188548\n",
      "Hindi-English BLEU Score: 27.678089024616547\n"
     ]
    }
   ],
   "source": [
    "print(f\"German-English BLEU Score: {de_en_bleu.score}\")\n",
    "print(f\"Hindi-English BLEU Score: {hi_en_bleu.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3501eb31-c784-4ce5-8062-beee42563c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BLEU scores for LLaMA 2\n",
      "LLaMA 2 German-English BLEU Score: 5.865690850642541\n",
      "LLaMA 2 Hindi-English BLEU Score: 0.3542982006686051\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating BLEU scores for LLaMA 2\")\n",
    "llama_de_en_bleu = corpus_bleu(llama_de_en_translations, [english_de_subset])\n",
    "llama_hi_en_bleu = corpus_bleu(llama_hi_en_translations, [english_hi_subset])\n",
    "\n",
    "print(f\"LLaMA 2 German-English BLEU Score: {llama_de_en_bleu.score}\")\n",
    "print(f\"LLaMA 2 Hindi-English BLEU Score: {llama_hi_en_bleu.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd939b1-1ca8-4163-9c6f-d2b7734ea324",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en_results = pd.DataFrame({\n",
    "    \"source\": german_subset,\n",
    "    \"reference\": english_de_subset,\n",
    "    \"translated\": de_en_translations\n",
    "})\n",
    "de_en_results.to_csv(\"translated_subset_results_de_en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "456ffa13-1627-4a8d-8f0c-e3f3530ec3c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hi_en_results = pd.DataFrame({\n",
    "    \"source\": hindi_subset,\n",
    "    \"reference\": english_hi_subset,\n",
    "    \"translated\": hi_en_translations\n",
    "})\n",
    "hi_en_results.to_csv(\"translated_subset_results_hi_en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69215f5a-4f91-44fd-ab31-325c51d13e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_de_en_results = pd.DataFrame({\n",
    "    \"source\": german_subset,\n",
    "    \"reference\": english_de_subset,\n",
    "    \"translated\": llama_de_en_translations\n",
    "})\n",
    "llama_de_en_results.to_csv(\"translated_subset_results_llama_de_en.csv\", index=False)\n",
    "\n",
    "llama_hi_en_results = pd.DataFrame({\n",
    "    \"source\": hindi_subset,\n",
    "    \"reference\": english_hi_subset,\n",
    "    \"translated\": llama_hi_en_translations\n",
    "})\n",
    "llama_hi_en_results.to_csv(\"translated_subset_results_llama_hi_en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da2dee1e-83c5-410d-a38c-088de2dcc9ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample German-English translations:\n",
      "\n",
      "Source (DE): Wiederaufnahme der Sitzungsperiode\n",
      "Translation (EN): Resumption of the session\n",
      "Reference (EN): Resumption of the session\n",
      "\n",
      "Source (DE): Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\n",
      "Translation (EN): I declare resumed the session of the European Parliament adjourned on Friday, 17 December, and wish you once again all the best for the turn of the year and hope that you have had a good holiday.\n",
      "Reference (EN): I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "\n",
      "Source (DE): Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\n",
      "Translation (EN): As you have seen, the dreaded 'millenium bug' has not occurred, but citizens of some of our Member States have been the victims of terrible natural disasters.\n",
      "Reference (EN): Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "\n",
      "Sample Hindi-English translations:\n",
      "\n",
      "Source (HI): अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें\n",
      "Translation (EN): Benefit Your Application's Extinction Exercise\n",
      "Reference (EN): Give your application an accessibility workout\n",
      "\n",
      "Source (HI): एक्सेर्साइसर पहुंचनीयता अन्वेषक\n",
      "Translation (EN): Exororororororphal\n",
      "Reference (EN): Accerciser Accessibility Explorer\n",
      "\n",
      "Source (HI): निचले पटल के लिए डिफोल्ट प्लग-इन खाका\n",
      "Translation (EN): Difftol plug-in layout for the bottom pane\n",
      "Reference (EN): The default plugin layout for the bottom panel\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample German-English translations:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nSource (DE): {german_subset[i]}\")\n",
    "    print(f\"Translation (EN): {de_en_translations[i]}\")\n",
    "    print(f\"Reference (EN): {english_de_subset[i]}\")\n",
    "\n",
    "print(\"\\nSample Hindi-English translations:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nSource (HI): {hindi_subset[i]}\")\n",
    "    print(f\"Translation (EN): {hi_en_translations[i]}\")\n",
    "    print(f\"Reference (EN): {english_hi_subset[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f4ece8d-36c4-444f-862c-68bcaf537dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading COMET model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b466732d94e546f9b3fa39174f480ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt20-comet-da/snapshots/4c372befe4d603e6d0363f434248ecad66945607/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading COMET model...\")\n",
    "model_path = download_model(\"Unbabel/wmt20-comet-da\")\n",
    "comet_model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa4702f6-131a-4103-b445-0e92753c4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing COMET evaluation data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing COMET evaluation data...\")\n",
    "de_en_comet_data = [\n",
    "    {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
    "    for src, mt, ref in zip(german_subset, de_en_translations, english_de_subset)\n",
    "]\n",
    "\n",
    "hi_en_comet_data = [\n",
    "    {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
    "    for src, mt, ref in zip(hindi_subset, hi_en_translations, english_hi_subset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85e6c3fe-8fb8-436e-a3d9-b14709071926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing COMET evaluation data for LLaMA 2...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing COMET evaluation data for LLaMA 2...\")\n",
    "llama_de_en_comet_data = [\n",
    "    {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
    "    for src, mt, ref in zip(german_subset, llama_de_en_translations, english_de_subset)\n",
    "]\n",
    "\n",
    "llama_hi_en_comet_data = [\n",
    "    {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
    "    for src, mt, ref in zip(hindi_subset, llama_hi_en_translations, english_hi_subset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1e451e4-1a75-4131-a0af-594c5a51eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running COMET evaluation for German-English...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████| 250/250 [00:21<00:00, 11.46it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running COMET evaluation for Hindi-English...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████| 250/250 [00:17<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "German-English System COMET Score: 0.5341249912087805\n",
      "Hindi-English System COMET Score: 0.4155202876748517\n"
     ]
    }
   ],
   "source": [
    "print(\"Running COMET evaluation for German-English...\")\n",
    "de_en_comet_scores = comet_model.predict(de_en_comet_data, batch_size=8, gpus=1)\n",
    "\n",
    "print(\"Running COMET evaluation for Hindi-English...\")\n",
    "hi_en_comet_scores = comet_model.predict(hi_en_comet_data, batch_size=8, gpus=1)\n",
    "\n",
    "print(f\"\\nGerman-English System COMET Score: {de_en_comet_scores['system_score']}\")\n",
    "print(f\"Hindi-English System COMET Score: {hi_en_comet_scores['system_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94278361-0d08-4035-9ebb-35f6ab768438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running COMET evaluation for LLaMA 2 German-English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████| 250/250 [00:31<00:00,  7.95it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running COMET evaluation for LLaMA 2 Hindi-English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████| 250/250 [00:24<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLaMA 2 German-English System COMET Score: -0.6147800029972568\n",
      "LLaMA 2 Hindi-English System COMET Score: -1.5059212676659226\n"
     ]
    }
   ],
   "source": [
    "print(\"Running COMET evaluation for LLaMA 2 German-English\")\n",
    "llama_de_en_comet_scores = comet_model.predict(llama_de_en_comet_data, batch_size=8, gpus=1)\n",
    "\n",
    "print(\"Running COMET evaluation for LLaMA 2 Hindi-English\")\n",
    "llama_hi_en_comet_scores = comet_model.predict(llama_hi_en_comet_data, batch_size=8, gpus=1)\n",
    "\n",
    "print(f\"\\nLLaMA 2 German-English System COMET Score: {llama_de_en_comet_scores['system_score']}\")\n",
    "print(f\"LLaMA 2 Hindi-English System COMET Score: {llama_hi_en_comet_scores['system_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cec2d82-6485-4b4b-9a90-1f2984a33a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en_comet_results = pd.DataFrame({\n",
    "    \"source\": german_subset,\n",
    "    \"reference\": english_de_subset,\n",
    "    \"translated\": de_en_translations,\n",
    "    \"comet_score\": de_en_comet_scores['scores']\n",
    "})\n",
    "de_en_comet_results.to_csv(\"comet_results_de_en.csv\", index=False)\n",
    "\n",
    "hi_en_comet_results = pd.DataFrame({\n",
    "    \"source\": hindi_subset,\n",
    "    \"reference\": english_hi_subset,\n",
    "    \"translated\": hi_en_translations,\n",
    "    \"comet_score\": hi_en_comet_scores['scores']\n",
    "})\n",
    "hi_en_comet_results.to_csv(\"comet_results_hi_en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d69a614-b828-4694-9dea-b8e2e1e6ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_de_en_comet_results = pd.DataFrame({\n",
    "    \"source\": german_subset,\n",
    "    \"reference\": english_de_subset,\n",
    "    \"translated\": llama_de_en_translations,\n",
    "    \"comet_score\": llama_de_en_comet_scores['scores']\n",
    "})\n",
    "llama_de_en_comet_results.to_csv(\"comet_results_llama_de_en.csv\", index=False)\n",
    "\n",
    "llama_hi_en_comet_results = pd.DataFrame({\n",
    "    \"source\": hindi_subset,\n",
    "    \"reference\": english_hi_subset,\n",
    "    \"translated\": llama_hi_en_translations,\n",
    "    \"comet_score\": llama_hi_en_comet_scores['scores']\n",
    "})\n",
    "llama_hi_en_comet_results.to_csv(\"comet_results_llama_hi_en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92028b9b-81d4-459e-b255-caeeec177920",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_de_en_comet_results = pd.DataFrame({\n",
    "    \"source\": german_subset,\n",
    "    \"reference\": english_de_subset,\n",
    "    \"translated\": llama_de_en_translations,\n",
    "    \"comet_score\": llama_de_en_comet_scores['scores']\n",
    "})\n",
    "llama_de_en_comet_results.to_csv(\"comet_results_llama_de_en.csv\", index=False)\n",
    "\n",
    "llama_hi_en_comet_results = pd.DataFrame({\n",
    "    \"source\": hindi_subset,\n",
    "    \"reference\": english_hi_subset,\n",
    "    \"translated\": llama_hi_en_translations,\n",
    "    \"comet_score\": llama_hi_en_comet_scores['scores']\n",
    "})\n",
    "llama_hi_en_comet_results.to_csv(\"comet_results_llama_hi_en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad7a9e-c761-46e3-8225-17684ff85c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6a314f1-77c2-4c6c-91db-b3633e739243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating error analysis...\n",
      "\n",
      "Summary Statistics:\n",
      "German-English translations below threshold: 723\n",
      "Hindi-English translations below threshold: 954\n",
      "Total translations analyzed: 2000\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "de_en_low_score_data = [\n",
    "    {\n",
    "        \"source\": src,\n",
    "        \"translated\": mt,\n",
    "        \"reference\": ref,\n",
    "        \"comet_score\": score\n",
    "    }\n",
    "    for src, mt, ref, score in zip(german_subset, de_en_translations, english_de_subset, de_en_comet_scores['scores'])\n",
    "    if score < threshold\n",
    "]\n",
    "\n",
    "hi_en_low_score_data = [\n",
    "    {\n",
    "        \"source\": src,\n",
    "        \"translated\": mt,\n",
    "        \"reference\": ref,\n",
    "        \"comet_score\": score\n",
    "    }\n",
    "    for src, mt, ref, score in zip(hindi_subset, hi_en_translations, english_hi_subset, hi_en_comet_scores['scores'])\n",
    "    if score < threshold\n",
    "]\n",
    "\n",
    "def annotate_errors(data, language_pair):\n",
    "    annotated_data = []\n",
    "    for row in data:\n",
    "        source = row[\"source\"]\n",
    "        translated = row[\"translated\"]\n",
    "        reference = row[\"reference\"]\n",
    "\n",
    "        if len(translated.split()) < len(reference.split()):\n",
    "            error = f\"<bad>{translated}</bad> (Under-translation: missing content)\"\n",
    "        elif len(translated.split()) > len(reference.split()):\n",
    "            error = f\"<bad>{translated}</bad> (Over-translation: extra content)\"\n",
    "        else:\n",
    "            error = f\"<bad>{translated}</bad> (Possible semantic or grammatical issues)\"\n",
    "\n",
    "        annotated_data.append({\n",
    "            \"source\": source,\n",
    "            \"translated\": translated,\n",
    "            \"reference\": reference,\n",
    "            \"comet_score\": row[\"comet_score\"],\n",
    "            \"error_annotation\": error,\n",
    "            \"language_pair\": language_pair\n",
    "        })\n",
    "    return annotated_data\n",
    "\n",
    "print(\"\\nGenerating error analysis...\")\n",
    "de_en_annotated = annotate_errors(de_en_low_score_data, \"German-English\")\n",
    "hi_en_annotated = annotate_errors(hi_en_low_score_data, \"Hindi-English\")\n",
    "\n",
    "all_annotated_results = pd.DataFrame(de_en_annotated + hi_en_annotated)\n",
    "all_annotated_results.to_csv(\"Annotated_Low_Score_Translations_Both_Pairs.csv\", index=False)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"German-English translations below threshold: {len(de_en_low_score_data)}\")\n",
    "print(f\"Hindi-English translations below threshold: {len(hi_en_low_score_data)}\")\n",
    "print(f\"Total translations analyzed: {len(german_subset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd4f60b4-f927-47cd-810e-3d0b3dad59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning dataset saved to 'fine_tuning_dataset_both_pairs.csv'\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_data = []\n",
    "for annot in de_en_annotated + hi_en_annotated:\n",
    "    fine_tuning_data.append({\n",
    "        \"source\": annot['source'],\n",
    "        \"target\": annot['error_annotation'],\n",
    "        \"reference\": annot['reference'],\n",
    "        \"language_pair\": annot['language_pair']\n",
    "    })\n",
    "\n",
    "fine_tuning_df = pd.DataFrame(fine_tuning_data)\n",
    "fine_tuning_df.to_csv(\"fine_tuning_dataset_both_pairs.csv\", index=False)\n",
    "print(\"\\nFine-tuning dataset saved to 'fine_tuning_dataset_both_pairs.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949a4f3-e347-4734-b0d4-116b9a43fea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80409cd4-b459-4251-9e2a-44348efc3063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b825b15-caa9-46cd-acb1-04fc8b54b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf543edd-4247-4dfd-a6d7-105c9a600b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54bb0ee0-4119-4978-ba56-44c170316d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69f2c981021481f9fd50d7bf9783a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA 2 Translated Text: This is a test sentence for translation.\n",
      "This is a test sentence for translation. This is a test sentence for translation. This is a test sentence for translation. This is a test sentence for translation. This is a test sentence for translation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"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\") \n",
    "\n",
    "llama_model_name = \"meta-llama/Llama-2-7b-hf\"  \n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(llama_model_name)\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(llama_model_name, device_map=\"auto\")\n",
    "\n",
    "input_text_llama = \"This is a test sentence for translation.\"\n",
    "\n",
    "tokens_llama = tokenizer_llama(input_text_llama, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs_llama = model_llama.generate(**tokens_llama, max_length=50)\n",
    "\n",
    "translated_text_llama = tokenizer_llama.decode(outputs_llama[0], skip_special_tokens=True)\n",
    "print(\"LLaMA 2 Translated Text:\", translated_text_llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9acc4b63-5b56-42c3-84aa-179d08742afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI package imported and configured\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "\n",
    "print(\"OpenAI package imported and configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9af307f-7ecc-49bf-842a-422a7f026116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /chat/completions in 0.453463 seconds\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /chat/completions in 0.816318 seconds\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     15\u001b[0m test_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test sentence for translation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m gpt_translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT Translated Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpt_translated_text)\n",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m, in \u001b[0;36mtranslate_with_gpt\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_with_gpt\u001b[39m(input_text):\n\u001b[1;32m      2\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslate the following English text to German. Only provide the translation.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_text}\n\u001b[1;32m      5\u001b[0m     ]\n\u001b[0;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "def translate_with_gpt(input_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Translate the following English text to German. Only provide the translation.\"},\n",
    "        {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "test_input = \"This is a test sentence for translation.\"\n",
    "gpt_translated_text = translate_with_gpt(test_input)\n",
    "\n",
    "print(\"GPT Translated Text:\", gpt_translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8c413-c680-4045-ae52-51c167423bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999e80c-4359-4447-8213-bf6021587e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bcfc52-45a3-41f2-8a67-2004c7805b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
