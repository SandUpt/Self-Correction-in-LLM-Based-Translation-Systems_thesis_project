{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ef05db-13c9-4edf-8cab-ffa2ed322d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "import pandas as pd\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961ac604-f0fd-4ee3-be1d-20404637d237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717084da-080f-4df5-93cf-005b8f167f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: Dies ist ein Testsatz für die Übersetzung.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "input_text = \"This is a test sentence for translation.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Translated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6808c4-1881-4bb0-8ca5-ec83429b951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train', 'validation', 'test'])\n",
      "Sample Data: {'translation': {'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
    "\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "print(\"Sample Data:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3082281d-59c5-4657-abd9-f99786dd4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4508785\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "english_sentences = [example['translation']['en'] for example in train_data]\n",
    "german_sentences = [example['translation']['de'] for example in train_data]\n",
    "\n",
    "print(f\"Number of samples: {len(english_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670e6b9f-cc8a-46cd-b51d-85affda51fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 100  \n",
    "english_subset = english_sentences[:subset_size]\n",
    "german_subset = german_sentences[:subset_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880b4d48-69ac-431d-851f-655a541e1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(sentences, batch_size=16):\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        translated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        translations.extend(translated_texts)\n",
    "    return translations\n",
    "\n",
    "translated_subset = translate_batch(english_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2ac307-6b2d-411d-aead-3e207cb62ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 25.59905338780902\n"
     ]
    }
   ],
   "source": [
    "bleu_score = corpus_bleu(translated_subset, [german_subset])\n",
    "print(f\"BLEU Score: {bleu_score.score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd939b1-1ca8-4163-9c6f-d2b7734ea324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to translated_subset_results.csv\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"source\": english_subset,\n",
    "    \"reference\": german_subset,\n",
    "    \"translated\": translated_subset\n",
    "})\n",
    "results.to_csv(\"translated_subset_results.csv\", index=False)\n",
    "print(\"Results saved to translated_subset_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456ffa13-1627-4a8d-8f0c-e3f3530ec3c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations with refrences:\n",
      "                                              source  \\\n",
      "0                          Resumption of the session   \n",
      "1  I declare resumed the session of the European ...   \n",
      "2  Although, as you will have seen, the dreaded '...   \n",
      "3  You have requested a debate on this subject in...   \n",
      "4  In the meantime, I should like to observe a mi...   \n",
      "5     Please rise, then, for this minute' s silence.   \n",
      "6  (The House rose and observed a minute' s silence)   \n",
      "7              Madam President, on a point of order.   \n",
      "8  You will be aware from the press and televisio...   \n",
      "9  One of the people assassinated very recently i...   \n",
      "\n",
      "                                           reference  \\\n",
      "0                 Wiederaufnahme der Sitzungsperiode   \n",
      "1  Ich erkläre die am Freitag, dem 17. Dezember u...   \n",
      "2  Wie Sie feststellen konnten, ist der gefürchte...   \n",
      "3  Im Parlament besteht der Wunsch nach einer Aus...   \n",
      "4  Heute möchte ich Sie bitten - das ist auch der...   \n",
      "5  Ich bitte Sie, sich zu einer Schweigeminute zu...   \n",
      "6  (Das Parlament erhebt sich zu einer Schweigemi...   \n",
      "7            Frau Präsidentin, zur Geschäftsordnung.   \n",
      "8  Wie Sie sicher aus der Presse und dem Fernsehe...   \n",
      "9  Zu den Attentatsopfern, die es in jüngster Zei...   \n",
      "\n",
      "                                          translated  \n",
      "0                 Wiederaufnahme der Sitzungsperiode  \n",
      "1  Ich erkläre die am Freitag, dem 17. Dezember 1...  \n",
      "2  Obwohl, wie Sie sicher gesehen haben werden, d...  \n",
      "3  Sie haben in den nächsten Tagen, während diese...  \n",
      "4  In der Zwischenzeit möchte ich eine Schweigemi...  \n",
      "5         Bitte erheben Sie sich zur Schweigeminute.  \n",
      "6  (Das Parlament erhebt sich zu einer Schweigemi...  \n",
      "7            Frau Präsidentin, zur Geschäftsordnung.  \n",
      "8  Sie werden von der Presse und dem Fernsehen wi...  \n",
      "9  Einer der kürzlich in Sri Lanka ermordeten Men...  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(\"translated_subset_results.csv\")\n",
    "\n",
    "print(\"Sample translations with refrences:\")\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dee1e-83c5-410d-a38c-088de2dcc9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4ece8d-36c4-444f-862c-68bcaf537dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c743cf46232c460e86a384d3813b0443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt20-comet-da/snapshots/4c372befe4d603e6d0363f434248ecad66945607/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMET Scores: Prediction([('scores', [1.2063143253326416, 0.712995707988739, 0.23980432748794556, 0.20518940687179565, 0.5901339650154114, 0.7375058531761169, 1.026737093925476, 1.093149185180664, 0.6110635995864868, 0.2614041268825531, 0.5620208978652954, 0.6270941495895386, 0.6581820249557495, 1.093149185180664, 0.6186625957489014, 0.695594310760498, 0.6161750555038452, 0.404382586479187, 0.7712704539299011, 0.22877958416938782, 0.654088020324707, 0.25909215211868286, 0.7377778887748718, 0.38653793931007385, 0.7547405362129211, 0.795854389667511, 0.6559374928474426, 0.6802090406417847, 1.1843135356903076, 0.734921395778656, 0.6526519656181335, 0.560311496257782, 0.5602272152900696, 0.2370995283126831, 0.5649402141571045, 0.6802854537963867, 0.1352175772190094, 0.583405077457428, 0.7225415110588074, 0.20118583738803864, 0.7264267206192017, 0.7179704308509827, 0.6021649241447449, 0.5071976780891418, 0.5306804776191711, 0.22495430707931519, 0.6968742609024048, 0.6516973376274109, 0.6726261377334595, 0.6077427864074707, 0.687061607837677, 0.660920262336731, 0.7082540392875671, 0.5605458617210388, -0.03457202762365341, 0.6162779927253723, 0.575115978717804, 0.4399797022342682, 0.7463166117668152, 0.7178254723548889, 0.6974818110466003, 0.5358044505119324, 0.7507449984550476, 1.1127605438232422, 0.7785833477973938, 0.26318204402923584, -0.685547411441803, 0.5576217770576477, 0.5183404088020325, 1.2155094146728516, 0.5768324136734009, 0.390306293964386, 0.6719205379486084, 0.4428691864013672, 0.27609914541244507, 0.015087833628058434, 0.6151180267333984, 0.7044589519500732, 0.3351389765739441, 0.5844680070877075, -0.06116955727338791, 0.2741473615169525, 0.4827539324760437, 0.2945431172847748, 0.7154949307441711, 1.0042349100112915, 0.6324989199638367, 0.7132174968719482, 0.47405844926834106, 0.7280233502388, 0.7281490564346313, 0.4302924573421478, 0.49548882246017456, 0.5724461078643799, 0.48434877395629883, 0.5934526324272156, 0.5942948460578918, 1.1414660215377808, 0.6242503523826599, 0.7106717228889465]), ('system_score', 0.5830645172856748)])\n"
     ]
    }
   ],
   "source": [
    "model_path = download_model(\"Unbabel/wmt20-comet-da\")\n",
    "comet_model = load_from_checkpoint(model_path)\n",
    "\n",
    "data_for_comet = [\n",
    "    {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
    "    for src, mt, ref in zip(english_subset, translated_subset, german_subset)\n",
    "]\n",
    "\n",
    "comet_scores = comet_model.predict(data_for_comet, batch_size=8, gpus=1)\n",
    "print(\"COMET Scores:\", comet_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa4702f6-131a-4103-b445-0e92753c4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System COMET Score: 0.5830645172856748\n"
     ]
    }
   ],
   "source": [
    "comet_results = pd.DataFrame({\n",
    "    \"source\": english_subset,\n",
    "    \"reference\": german_subset,\n",
    "    \"translated\": translated_subset,\n",
    "    \"comet_score\": comet_scores['scores']\n",
    "})\n",
    "\n",
    "system_score = comet_scores['system_score']\n",
    "comet_results.to_csv(\"comet_results_with_system_score.csv\", index=False)\n",
    "print(f\"System COMET Score: {system_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a314f1-77c2-4c6c-91db-b3633e739243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated results saved to 'Annotated_Low_Score_Translations.csv'\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5 \n",
    "\n",
    "low_score_data = [\n",
    "    {\n",
    "        \"source\": src,\n",
    "        \"translated\": mt,\n",
    "        \"reference\": ref,\n",
    "        \"comet_score\": score\n",
    "    }\n",
    "    for src, mt, ref, score in zip(english_subset, translated_subset, german_subset, comet_scores['scores'])\n",
    "    if score < threshold\n",
    "]\n",
    "\n",
    "def annotate_errors(data):\n",
    "    annotated_data = []\n",
    "    for row in data:\n",
    "        source = row[\"source\"]\n",
    "        translated = row[\"translated\"]\n",
    "        reference = row[\"reference\"]\n",
    "\n",
    "        if len(translated.split()) < len(reference.split()):\n",
    "            error = f\"<bad>{translated}</bad> (Under-translation: missing words)\"\n",
    "        elif len(translated.split()) > len(reference.split()):\n",
    "            error = f\"<bad>{translated}</bad> (Over-translation: extra words)\"\n",
    "        else:\n",
    "            error = f\"<bad>{translated}</bad> (Possible terminology or grammar issues)\"\n",
    "\n",
    "        annotated_data.append({\n",
    "            \"source\": source,\n",
    "            \"translated\": translated,\n",
    "            \"reference\": reference,\n",
    "            \"comet_score\": row[\"comet_score\"],\n",
    "            \"error_annotation\": error\n",
    "        })\n",
    "    return annotated_data\n",
    "\n",
    "annotated_results = annotate_errors(low_score_data)\n",
    "\n",
    "annotated_results_df = pd.DataFrame(annotated_results)\n",
    "\n",
    "annotated_results_df.to_csv(\"Annotated_Low_Score_Translations.csv\", index=False)\n",
    "\n",
    "print(\"Annotated results saved to 'Annotated_Low_Score_Translations.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4f60b4-f927-47cd-810e-3d0b3dad59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning dataset saved to fine_tuning_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_data = []\n",
    "for index, row in annotated_results_df.iterrows():\n",
    "    source = row['source']\n",
    "    target = row['error_annotation']  \n",
    "    reference = row['reference']  \n",
    "\n",
    "    fine_tuning_data.append({\"source\": source, \"target\": target, \"reference\": reference})\n",
    "\n",
    "fine_tuning_df = pd.DataFrame(fine_tuning_data)\n",
    "\n",
    "fine_tuning_file = \"fine_tuning_dataset.csv\"\n",
    "fine_tuning_df.to_csv(fine_tuning_file, index=False)\n",
    "print(f\"Fine-tuning dataset saved to {fine_tuning_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949a4f3-e347-4734-b0d4-116b9a43fea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80409cd4-b459-4251-9e2a-44348efc3063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b825b15-caa9-46cd-acb1-04fc8b54b534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf543edd-4247-4dfd-a6d7-105c9a600b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8c413-c680-4045-ae52-51c167423bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999e80c-4359-4447-8213-bf6021587e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bcfc52-45a3-41f2-8a67-2004c7805b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
