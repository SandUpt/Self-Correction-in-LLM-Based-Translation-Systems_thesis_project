{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b624eade-f0ef-4678-b58b-ddb106cced58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n",
      "NumPy version: 1.26.4\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "#CELL 1\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73153aae-bf4c-4706-8b91-402d38f6bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6585d4cd229046f4907043d46dbce3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving base model...\n",
      "Base model saved to ./saved_models/base_llama\n",
      "Base model and tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_save_path = \"./saved_models/base_llama\"\n",
    "\n",
    "def load_or_download_base_model():\n",
    "    if os.path.exists(base_model_save_path):\n",
    "        print(\"Loading saved base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_save_path)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_save_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Downloading base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        print(\"Saving base model...\")\n",
    "        tokenizer.save_pretrained(base_model_save_path)\n",
    "        base_model.save_pretrained(base_model_save_path)\n",
    "        print(f\"Base model saved to {base_model_save_path}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer, base_model\n",
    "\n",
    "tokenizer, base_model = load_or_download_base_model()\n",
    "print(\"Base model and tokenizer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce09c1f-6e51-43e3-bde7-db4ab7af4ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL 3\n",
    "import re\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sacrebleu import corpus_bleu\n",
    "from datasets import Dataset\n",
    "\n",
    "def parse_sgm_to_list(sgm_file):\n",
    "    \"\"\"\n",
    "    Reads an SGM file (e.g. newstest2018-deen-src.de.sgm)\n",
    "    and returns a list of lines, one per <seg id=\\\"...\\\"> block.\n",
    "    \"\"\"\n",
    "    with open(sgm_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    segments = re.findall(r'<seg id=\\\"\\d+\\\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    segments = [seg.strip() for seg in segments]\n",
    "    return segments\n",
    "\n",
    "def load_sgm_parallel(de_sgm_file, en_sgm_file):\n",
    "    \"\"\"\n",
    "    Pairs up lines from a German .sgm file and an English .sgm file\n",
    "    into a list of dicts: {\\\"translation\\\": {\\\"de\\\": ..., \\\"en\\\": ...}}.\n",
    "    \"\"\"\n",
    "    de_lines = parse_sgm_to_list(de_sgm_file)\n",
    "    en_lines = parse_sgm_to_list(en_sgm_file)\n",
    "    assert len(de_lines) == len(en_lines), (\n",
    "        f\"Mismatch in line counts: {len(de_lines)} vs {len(en_lines)}\"\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    for de_text, en_text in zip(de_lines, en_lines):\n",
    "        data.append({\n",
    "            \"translation\": {\n",
    "                \"de\": de_text,\n",
    "                \"en\": en_text\n",
    "            }\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german_text}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    num_examples=100,\n",
    "    debug_print=5,\n",
    "    description=\"Model\"\n",
    "):\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"English translation:\" in full_output_text:\n",
    "            pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text.split(\"German text:\")[-1].strip()\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef0e0f-4b22-47ad-955d-42da4211bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def count_sgm_segments(sgm_file):\n",
    "    \"\"\"\n",
    "    Count how many <seg id=\"...\"> blocks are in the given .sgm file.\n",
    "    \"\"\"\n",
    "    with open(sgm_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    segments = re.findall(r'<seg id=\"\\d+\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    return len(segments)\n",
    "de_file = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "en_file = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "\n",
    "de_count = count_sgm_segments(de_file)\n",
    "en_count = count_sgm_segments(en_file)\n",
    "\n",
    "print(f\"German file has {de_count} segments.\")\n",
    "print(f\"English file has {en_count} segments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc3bad-16bf-4eba-aa93-2753d996aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3.5 - Dataset Analysis\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_tsv_file(file_path):\n",
    "    \"\"\"Analyze a TSV parallel corpus file\"\"\"\n",
    "    print(f\"\\nAnalyzing training data: {file_path}\")\n",
    "    \n",
    "    # Get file size\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    # Count lines and gather statistics\n",
    "    num_lines = 0\n",
    "    total_de_chars = 0\n",
    "    total_en_chars = 0\n",
    "    max_de_length = 0\n",
    "    max_en_length = 0\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Analyzing lines\"):\n",
    "            num_lines += 1\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                de_text, en_text = parts\n",
    "                \n",
    "                # Update character counts\n",
    "                total_de_chars += len(de_text)\n",
    "                total_en_chars += len(en_text)\n",
    "                \n",
    "                # Update max lengths\n",
    "                max_de_length = max(max_de_length, len(de_text))\n",
    "                max_en_length = max(max_en_length, len(en_text))\n",
    "    \n",
    "    print(f\"\\nTraining Dataset Statistics:\")\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"Number of sentence pairs: {num_lines:,}\")\n",
    "    print(f\"Average German sentence length: {total_de_chars/num_lines:.1f} characters\")\n",
    "    print(f\"Average English sentence length: {total_en_chars/num_lines:.1f} characters\")\n",
    "    print(f\"Longest German sentence: {max_de_length:,} characters\")\n",
    "    print(f\"Longest English sentence: {max_en_length:,} characters\")\n",
    "    \n",
    "    return num_lines\n",
    "\n",
    "def analyze_sgm_file(file_path):\n",
    "    \"\"\"Count segments in an SGM file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    segments = re.findall(r'<seg id=\"\\d+\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    return len(segments)\n",
    "\n",
    "print(\"Analyzing all datasets...\")\n",
    "\n",
    "# Analyze training data\n",
    "train_file = \"wmt_dataset/train/europarl-v9.de-en.tsv\"\n",
    "if os.path.exists(train_file):\n",
    "    num_train = analyze_tsv_file(train_file)\n",
    "else:\n",
    "    print(\"\\nTraining file not found!\")\n",
    "    num_train = 0\n",
    "\n",
    "# Analyze dev data\n",
    "dev_de = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "dev_en = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "if os.path.exists(dev_de) and os.path.exists(dev_en):\n",
    "    num_dev = analyze_sgm_file(dev_de)\n",
    "    print(f\"\\nDev Dataset Size:\")\n",
    "    print(f\"Number of sentence pairs: {num_dev:,}\")\n",
    "else:\n",
    "    print(\"\\nDev files not found!\")\n",
    "    num_dev = 0\n",
    "\n",
    "# Analyze test data\n",
    "test_de = \"wmt_dataset/test/sgm/newstest2019-deen-src.de.sgm\"\n",
    "test_en = \"wmt_dataset/test/sgm/newstest2019-deen-ref.en.sgm\"\n",
    "if os.path.exists(test_de) and os.path.exists(test_en):\n",
    "    num_test = analyze_sgm_file(test_de)\n",
    "    print(f\"\\nTest Dataset Size:\")\n",
    "    print(f\"Number of sentence pairs: {num_test:,}\")\n",
    "else:\n",
    "    print(\"\\nTest files not found!\")\n",
    "    num_test = 0\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Training pairs: {num_train:,}\")\n",
    "print(f\"Dev pairs: {num_dev:,}\")\n",
    "print(f\"Test pairs: {num_test:,}\")\n",
    "\n",
    "if num_train > 0:\n",
    "    recommended_size = min(num_train, 100000) \n",
    "    print(f\"\\nRecommended max_examples for training: {recommended_size:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452c598-3393-4026-9938-bd2dd3eadfe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL 4 (Updated with Collator)\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"English translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"English translation:\", 1)\n",
    "                prompt_part = prompt_part + \"English translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "def load_tsv_parallel(tsv_file):\n",
    "    \"\"\"\n",
    "    Reads a tab-separated parallel corpus file and returns a list of translation pairs.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(tsv_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Split on tab, strip whitespace\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:  # Ensure we have both source and target\n",
    "                data.append({\n",
    "                    \"translation\": {\n",
    "                        \"de\": parts[0].strip(),\n",
    "                        \"en\": parts[1].strip()\n",
    "                    }\n",
    "                })\n",
    "    return data\n",
    "\n",
    "def load_local_wmt(data_type=\"train\", max_examples=None):\n",
    "    \"\"\"\n",
    "    Loads WMT data from local files.\n",
    "    data_type: \"train\", \"dev\", or \"test\"\n",
    "    max_examples: if set, limits the number of examples loaded\n",
    "    \"\"\"\n",
    "    if data_type == \"dev\":\n",
    "        de_path = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "        en_path = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "        data_list = load_sgm_parallel(de_path, en_path)\n",
    "    elif data_type == \"test\":\n",
    "        de_path = \"wmt_dataset/test/sgm/newstest2019-deen-src.de.sgm\"\n",
    "        en_path = \"wmt_dataset/test/sgm/newstest2019-deen-ref.en.sgm\"\n",
    "        data_list = load_sgm_parallel(de_path, en_path)\n",
    "    elif data_type == \"train\":\n",
    "        tsv_path = \"wmt_dataset/train/europarl-v9.de-en.tsv\"\n",
    "        print(f\"Loading training data from {tsv_path}...\")\n",
    "        data_list = load_tsv_parallel(tsv_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data_type: {data_type}\")\n",
    "    \n",
    "    # Limit examples if requested\n",
    "    if max_examples is not None:\n",
    "        data_list = data_list[:max_examples]\n",
    "    \n",
    "    return Dataset.from_list(data_list)\n",
    "\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(data_type=\"train\", max_examples=None): \n",
    "    print(f\"Loading local WMT data ({data_type})...\")\n",
    "    dataset = load_local_wmt(data_type, max_examples)\n",
    "    \n",
    "    # Shuffle training data\n",
    "    if data_type == \"train\":\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        build_full_text,\n",
    "        desc=\"Building prompt + target text\",\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "training_data = load_and_format_wmt(data_type=\"train\", max_examples=5000)  \n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(f\"Training data prepared with prompt masking. Total examples: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea2c22-9e8e-4dfa-88d7-c4920ba810e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4.5\n",
    "# Load and evaluate baseline model on test data\n",
    "print(\"Evaluating baseline model on test data...\")\n",
    "test_dataset = load_and_format_wmt(data_type=\"test\", max_examples=100)\n",
    "baseline_results_path = \"./results/baseline_results.json\"\n",
    "\n",
    "print(\"Running baseline evaluation...\")\n",
    "baseline_debug_results = debug_evaluate_model(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=test_dataset,\n",
    "    num_examples=100,\n",
    "    debug_print=5,\n",
    "    description=\"Baseline LLaMA (Test)\"\n",
    ")\n",
    "with open(baseline_results_path, 'w') as f:\n",
    "    json.dump(baseline_debug_results, f)\n",
    "\n",
    "print(\"Baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afa863-4e0e-43bb-bb3a-a8e16e93c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel \n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False \n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                   \n",
    "        lora_alpha=32,         \n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,   \n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \n",
    "            \"v_proj\", \n",
    "            \"k_proj\", \n",
    "            \"o_proj\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    for name, param in lora_model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "print(\"LoRA model is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7263710-7008-4c6e-9bac-80419f38d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6\n",
    "import shutil\n",
    "if os.path.exists(\"./my_results\"):\n",
    "    print(\"Cleaning up old results directory...\")\n",
    "    shutil.rmtree(\"./my_results\")\n",
    "os.makedirs(\"./my_results\")\n",
    "\n",
    "# Load dev data for monitoring training\n",
    "monitor_dataset = load_and_format_wmt(data_type=\"dev\", max_examples=1000)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./my_results\",\n",
    "    num_train_epochs=4,          \n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    save_steps=100,              \n",
    "    logging_steps=20,            \n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=True,  \n",
    "    evaluation_strategy=\"steps\",  \n",
    "    eval_steps=100,              \n",
    "    save_total_limit=2,          \n",
    "    metric_for_best_model=\"loss\" \n",
    ")\n",
    "\n",
    "print(f\"Training arguments set. Will train for {train_args.num_train_epochs} epochs on ~{len(training_data)} examples.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=monitor_dataset,  \n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./my_results/lora_7b\")\n",
    "print(\"Fine-tuning done. Model saved at ./my_results/lora_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b31f58-871c-438e-9c82-67539e2d8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model_save_path = \"./saved_models/base_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba704a9-af91-4fe7-87e2-e8563dd04290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def load_lora_model(checkpoint_path=\"./my_results/lora_7b\"):\n",
    "    print(f\"Loading LoRA model from {checkpoint_path}...\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_save_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    lora_model_loaded = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    lora_model_loaded = lora_model_loaded.merge_and_unload()\n",
    "    return lora_model_loaded\n",
    "\n",
    "print(\"\\nEvaluating LoRA-Fine-Tuned Model on test data...\")\n",
    "merged_model = load_lora_model(\"./my_results/lora_7b\")\n",
    "\n",
    "# We can reuse the test_dataset from Cell 3\n",
    "print(f\"Using {len(test_dataset)} test examples from newstest2019\")\n",
    "\n",
    "lora_test_results = debug_evaluate_model(\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=test_dataset,\n",
    "    num_examples=100,\n",
    "    debug_print=5,\n",
    "    description=\"LoRA Fine-Tuned (Test)\"\n",
    ")\n",
    "\n",
    "print(\"\\nFinal comparison on test set:\")\n",
    "print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "print(f\"LoRA     -> BLEU = {lora_test_results['bleu']:.2f}, COMET = {lora_test_results['comet']:.3f}\")\n",
    "\n",
    "# Save LoRA results\n",
    "lora_results_path = \"./results/lora_test_results.json\"\n",
    "with open(lora_results_path, 'w') as f:\n",
    "    json.dump(lora_test_results, f)\n",
    "print(f\"LoRA test results saved to {lora_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87d621-fd30-4b2a-ac69-e071c8963312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8\n",
    "import json\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_samples = 50\n",
    "data_for_labeling = load_dataset(\"wmt19\", \"de-en\", split=\"validation\").select(range(num_samples))\n",
    "\n",
    "print(f\"Loaded {len(data_for_labeling)} examples for labeling.\")\n",
    "\n",
    "comet_metric = evaluate.load(\"comet\")\n",
    "\n",
    "def get_quality_label(score):\n",
    "    if score < 0.2:\n",
    "        return \"Bad\"\n",
    "    elif score < 0.6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Good\"\n",
    "\n",
    "labeled_examples = []\n",
    "for ex in tqdm(data_for_labeling, desc=\"Generating & Labeling\"):\n",
    "    src_de = ex[\"translation\"][\"de\"]\n",
    "    ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "    prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).to(base_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=4,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"English translation:\" in full_output_text:\n",
    "        pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "    else:\n",
    "        pred_en = full_output_text.strip()\n",
    "\n",
    "    comet_scores = comet_metric.compute(\n",
    "        predictions=[pred_en],\n",
    "        references=[ref_en],\n",
    "        sources=[src_de],\n",
    "        gpus=0,\n",
    "        progress_bar=False\n",
    "    )\n",
    "    score = comet_scores[\"scores\"][0]\n",
    "\n",
    "    label = get_quality_label(score)\n",
    "\n",
    "    labeled_examples.append({\n",
    "        \"source_de\": src_de,\n",
    "        \"reference_en\": ref_en,\n",
    "        \"baseline_translation\": pred_en,\n",
    "        \"comet_score\": float(score),\n",
    "        \"quality_label\": label\n",
    "    })\n",
    "\n",
    "output_file = \"./results/baseline_labeled_translations.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labeled_examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nDone. Saved {len(labeled_examples)} labeled examples to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c67e3-b718-42fd-83cb-b89dcfcc8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def export_to_csv(\n",
    "    input_file=\"./results/baseline_labeled_translations.json\",\n",
    "    output_file=\"./results/errorful_data.csv\"\n",
    "):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_data = json.load(f)\n",
    "\n",
    "    errorful_data = [ex for ex in all_data if ex[\"quality_label\"] in [\"Bad\", \"Medium\"]]\n",
    "    print(f\"Total data: {len(all_data)}\")\n",
    "    print(f\"Bad/Medium examples: {len(errorful_data)}\")\n",
    "\n",
    "    fieldnames = [\n",
    "        \"source_de\",\n",
    "        \"reference_en\",\n",
    "        \"baseline_translation\",\n",
    "        \"comet_score\",\n",
    "        \"quality_label\",\n",
    "        \"error_category\",\n",
    "        \"fix_explanation\",\n",
    "        \"corrected_en\"\n",
    "    ]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for ex in errorful_data:\n",
    "            writer.writerow({\n",
    "                \"source_de\": ex[\"source_de\"],\n",
    "                \"reference_en\": ex[\"reference_en\"],\n",
    "                \"baseline_translation\": ex[\"baseline_translation\"],\n",
    "                \"comet_score\": ex[\"comet_score\"],\n",
    "                \"quality_label\": ex[\"quality_label\"],\n",
    "                \"error_category\": \"\",\n",
    "                \"fix_explanation\": \"\",\n",
    "                \"corrected_en\": \"\"\n",
    "            })\n",
    "\n",
    "    print(f\"\\nCSV created at: {output_file}\")\n",
    "\n",
    "export_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722e995-2ef8-44e5-8b87-7f702ff10144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
