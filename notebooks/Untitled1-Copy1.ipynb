{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b624eade-f0ef-4678-b58b-ddb106cced58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 1: Initial imports and environment setup\n",
    "# --------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73153aae-bf4c-4706-8b91-402d38f6bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f7ba7a0d10440289fefb5924a0ed47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:2869: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7805099aac4243a1f1a78e67b2d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model saved to ./saved_models/base_llama\n",
      "Base model and tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Base Model & Tokenizer (Baseline)\n",
    "# --------------------------------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_save_path = \"./saved_models/base_llama\"\n",
    "\n",
    "def load_or_download_base_model():\n",
    "    if os.path.exists(base_model_save_path):\n",
    "        print(\"Loading saved base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_save_path)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_save_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Downloading base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # Save the model and tokenizer\n",
    "        print(\"Saving base model...\")\n",
    "        tokenizer.save_pretrained(base_model_save_path)\n",
    "        base_model.save_pretrained(base_model_save_path)\n",
    "        print(f\"Base model saved to {base_model_save_path}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer, base_model\n",
    "\n",
    "tokenizer, base_model = load_or_download_base_model()\n",
    "print(\"Base model and tokenizer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce09c1f-6e51-43e3-bde7-db4ab7af4ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) validation data with 50 examples...\n",
      "Evaluating baseline model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c21e88a83342aca654fdd7d94bd27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] Baseline LLaMA on 20 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:   0%|                                                                 | 0/20 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating Baseline LLaMA:   5%|██▊                                                      | 1/20 [00:03<01:11,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29924, 3346, 2724, 29871, 29896, 29947, 29945, 29953, 29901, 23650, 476, 8109, 29892, 762, 306, 13608, 350, 1406, 1622, 762, 5587, 1147, 3140, 824, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nMünchen 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\\n\\nEnglish translation:\\nMunich 1856: Four cards that change your view of the city\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Munich 1856: Four cards that change your view of the city'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Munich 1856: Four maps that will change your view of the city\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:  10%|█████▋                                                   | 2/20 [00:06<00:57,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 6600, 1267, 29899, 2744, 303, 1997, 29892, 8879, 2160, 12843, 19472, 4545, 1812, 387, 4566, 899, 2435, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "\"You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nEine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\\n\\nEnglish translation:\\nAn insane asylum, where today's youth should meet.\\n\"\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "\"An insane asylum, where today's youth should meet.\"\n",
      "--------------[ REFERENCE ]---------------\n",
      "A mental asylum, where today young people are said to meet.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:  15%|████████▌                                                | 3/20 [00:10<00:57,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 5430, 615, 21474, 1808, 29892, 8879, 11923, 1865, 972, 317, 29899, 29933, 5422, 29899, 29911, 16163, 21598, 336, 1785, 4296, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nEine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\\n\\nEnglish translation:\\nA graveyard chapel, where now a S-Bahn tunnel is being dug.\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'A graveyard chapel, where now a S-Bahn tunnel is being dug.'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA: 100%|████████████████████████████████████████████████████████| 20/20 [02:16<00:00,  6.84s/it]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline LLaMA] BLEU = 64.93\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 437478 has 25.80 GiB memory in use. Including non-PyTorch memory, this process has 13.57 GiB memory in use. Of the allocated memory 13.05 GiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating baseline model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     baseline_debug_results \u001b[38;5;241m=\u001b[39m \u001b[43mdebug_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBaseline LLaMA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(baseline_results_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    123\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(baseline_debug_results, f)\n",
      "Cell \u001b[0;32mIn[3], line 84\u001b[0m, in \u001b[0;36mdebug_evaluate_model\u001b[0;34m(model, tokenizer, eval_dataset, num_examples, debug_print, description)\u001b[0m\n\u001b[1;32m     81\u001b[0m bleu \u001b[38;5;241m=\u001b[39m corpus_bleu(predictions, references)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] BLEU = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu\u001b[38;5;241m.\u001b[39mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m comet_results \u001b[38;5;241m=\u001b[39m \u001b[43mcomet_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msources\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] COMET = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomet_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: predictions,\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m\"\u001b[39m: references,\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m: bleu\u001b[38;5;241m.\u001b[39mscore,\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomet\u001b[39m\u001b[38;5;124m\"\u001b[39m: comet_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     96\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/evaluate/module.py:467\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {input_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed):\n\u001b[0;32m--> 467\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--comet/b86db5c8a9228110941c739c62b632e14915c16696d5cb31dacba864077a47c4/comet.py:167\u001b[0m, in \u001b[0;36mCOMET._compute\u001b[0;34m(self, sources, predictions, references, gpus, progress_bar)\u001b[0m\n\u001b[1;32m    165\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(data, t)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39mvalues())]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(comet\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     scores, mean_score \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mscores, output\u001b[38;5;241m.\u001b[39msystem_score\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/comet/models/base.py:654\u001b[0m, in \u001b[0;36mCometModel.predict\u001b[0;34m(self, samples, batch_size, gpus, devices, mc_dropout, progress_bar, accelerator, num_workers, length_batching)\u001b[0m\n\u001b[1;32m    645\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ptl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    646\u001b[0m     devices\u001b[38;5;241m=\u001b[39mdevices,\n\u001b[1;32m    647\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m     enable_progress_bar\u001b[38;5;241m=\u001b[39menable_progress_bar,\n\u001b[1;32m    652\u001b[0m )\n\u001b[1;32m    653\u001b[0m return_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_predictions\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    658\u001b[0m     torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mbarrier()  \u001b[38;5;66;03m# Waits for all processes to finish predict\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:858\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:897\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    894\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n\u001b[0;32m--> 897\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:957\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    956\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:154\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# let the precision plugin convert the module here so that this strategy hook can decide the order\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# of operations\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mconvert_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pytorch_lightning/strategies/single_device.py:79\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.model_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.model must be set before self.model.to()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/lightning_fabric/utilities/device_dtype_mixin.py:55\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     54\u001b[0m _update_properties(\u001b[38;5;28mself\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 14.81 MiB is free. Process 437478 has 25.80 GiB memory in use. Including non-PyTorch memory, this process has 13.57 GiB memory in use. Of the allocated memory 13.05 GiB is allocated by PyTorch, and 15.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Evaluation Data & Get Baseline Scores\n",
    "# --------------------------------------------\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german_text}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"Model\"\n",
    "):\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"English translation:\" in full_output_text:\n",
    "            pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text.split(\"German text:\")[-1].strip()\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }\n",
    "\n",
    "def load_eval_data(num_examples=50):\n",
    "    print(f\"Loading WMT19 (de-en) validation data with {num_examples} examples...\")\n",
    "    eval_data = load_dataset(\"wmt19\", \"de-en\", split=\"validation\")\n",
    "    eval_data = eval_data.select(range(min(num_examples, len(eval_data))))\n",
    "    return eval_data\n",
    "\n",
    "eval_dataset = load_eval_data(num_examples=50)\n",
    "baseline_results_path = \"./results/baseline_results.json\"\n",
    "\n",
    "if os.path.exists(baseline_results_path):\n",
    "    print(\"Loading saved baseline results...\")\n",
    "    with open(baseline_results_path, 'r') as f:\n",
    "        baseline_debug_results = json.load(f)\n",
    "    print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "else:\n",
    "    print(\"Evaluating baseline model...\")\n",
    "    baseline_debug_results = debug_evaluate_model(\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        eval_dataset=eval_dataset,\n",
    "        num_examples=20,\n",
    "        debug_print=3,\n",
    "        description=\"Baseline LLaMA\"\n",
    "    )\n",
    "    with open(baseline_results_path, 'w') as f:\n",
    "        json.dump(baseline_debug_results, f)\n",
    "\n",
    "print(\"Baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9452c598-3393-4026-9938-bd2dd3eadfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) train data with 10000 examples...\n",
      "Training data prepared with prompt masking.\n"
     ]
    }
   ],
   "source": [
    " #--------------------------------------------\n",
    "# Cell 4: Prepare Training Data with Improved Prompt\n",
    "# --------------------------------------------\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(num_examples=10000): \n",
    "    print(f\"Loading WMT19 (de-en) train data with {num_examples} examples...\")\n",
    "    dataset = load_dataset(\"wmt19\", \"de-en\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        build_full_text,\n",
    "        desc=\"Building prompt + target text\",\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"English translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"English translation:\", 1)\n",
    "                prompt_part = prompt_part + \"English translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "training_data = load_and_format_wmt(num_examples=10000)\n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(\"Training data prepared with prompt masking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93afa863-4e0e-43bb-bb3a-a8e16e93c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA model...\n",
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n",
      "LoRA model is ready.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 5: Updated LoRA Configuration\n",
    "# --------------------------------------------\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel \n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False \n",
    "        \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                    \n",
    "        lora_alpha=32,          \n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,    \n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \n",
    "            \"v_proj\", \n",
    "            \"k_proj\", \n",
    "            \"o_proj\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    for name, param in lora_model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "print(\"LoRA model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7263710-7008-4c6e-9bac-80419f38d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up old results directory...\n",
      "Loading WMT19 (de-en) train data with 1000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set. Will train for 4 epochs on ~10000 examples.\n",
      "Starting LoRA fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 1:30:16, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.066750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.056891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.045917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.035986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "Training metrics:\n",
      "TrainOutput(global_step=2500, training_loss=0.5674942597389221, metrics={'train_runtime': 5420.4301, 'train_samples_per_second': 7.379, 'train_steps_per_second': 0.461, 'total_flos': 8.1397196783616e+17, 'train_loss': 0.5674942597389221, 'epoch': 4.0})\n",
      "Fine-tuning done. Model saved at ./my_results/lora_7b\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 6: Updated Training Arguments and Training\n",
    "# --------------------------------------------\n",
    "import shutil\n",
    "if os.path.exists(\"./my_results\"):\n",
    "    print(\"Cleaning up old results directory...\")\n",
    "    shutil.rmtree(\"./my_results\")\n",
    "os.makedirs(\"./my_results\")\n",
    "\n",
    "eval_data = load_and_format_wmt(num_examples=1000)  # Use 1000 examples for evaluation\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./my_results\",\n",
    "    num_train_epochs=4,           \n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=True,  \n",
    "    evaluation_strategy=\"steps\",   \n",
    "    eval_steps=500,               \n",
    "    save_total_limit=2,           \n",
    "    metric_for_best_model=\"loss\"  \n",
    ")\n",
    "\n",
    "print(f\"Training arguments set. Will train for {train_args.num_train_epochs} epochs on ~{len(training_data)} examples.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=eval_data,        \n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./my_results/lora_7b\")\n",
    "print(\"Fine-tuning done. Model saved at ./my_results/lora_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b31f58-871c-438e-9c82-67539e2d8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model_save_path = \"./saved_models/base_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba704a9-af91-4fe7-87e2-e8563dd04290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LoRA-Fine-Tuned Model...\n",
      "Loading LoRA model from ./my_results/lora_7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b663a9c5933409e8ecb5bfbb1f9d680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50c3400aeed4c97a113f7f642a0f4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] LoRA Fine-Tuned on 20 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:   0%|                                                                | 0/20 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating LoRA Fine-Tuned:   5%|██▊                                                     | 1/20 [00:00<00:11,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29924, 3346, 2724, 29871, 29896, 29947, 29945, 29953, 29901, 23650, 476, 8109, 29892, 762, 306, 13608, 350, 1406, 1622, 762, 5587, 1147, 3140, 824, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nMünchen 1856: Vier Karten, die Ihren Blick auf die Stadt verändern\\n\\nEnglish translation: Munich 1856: Four cards that will change your view of the city'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Munich 1856: Four cards that will change your view of the city'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Munich 1856: Four maps that will change your view of the city\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:  10%|█████▌                                                  | 2/20 [00:01<00:08,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 6600, 1267, 29899, 2744, 303, 1997, 29892, 8879, 2160, 12843, 19472, 4545, 1812, 387, 4566, 899, 2435, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nEine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\\n\\nEnglish translation: An insane asylum, where young people are supposed to meet today.'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'An insane asylum, where young people are supposed to meet today.'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A mental asylum, where today young people are said to meet.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned:  15%|████████▍                                               | 3/20 [00:01<00:08,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Eine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 5430, 615, 21474, 1808, 29892, 8879, 11923, 1865, 972, 317, 29899, 29933, 5422, 29899, 29911, 16163, 21598, 336, 1785, 4296, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nEine Gruftkapelle, wo nun für den S-Bahn-Tunnel gegraben wird.\\n\\nEnglish translation: A tomb chapel, where now the S-Bahn-Tunnel is being dug.'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'A tomb chapel, where now the S-Bahn-Tunnel is being dug.'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned: 100%|███████████████████████████████████████████████████████| 20/20 [00:12<00:00,  1.58it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned] BLEU = 76.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned] COMET = 0.818\n",
      "\n",
      "\n",
      "Final comparison:\n",
      "Baseline -> BLEU = 64.93, COMET = 0.788\n",
      "LoRA     -> BLEU = 76.12, COMET = 0.818\n",
      "LoRA results saved to ./results/lora_results.json\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 7: Evaluate Fine-tuned Model\n",
    "# --------------------------------------------\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german_text}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"Model\"\n",
    "):\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"English translation:\" in full_output_text:\n",
    "            pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text.split(\"German text:\")[-1].strip()\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }\n",
    "\n",
    "def load_lora_model(checkpoint_path=\"./my_results/lora_7b\"):\n",
    "    print(f\"Loading LoRA model from {checkpoint_path}...\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_save_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    lora_model_loaded = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    lora_model_loaded = lora_model_loaded.merge_and_unload()\n",
    "    return lora_model_loaded\n",
    "\n",
    "print(\"\\nEvaluating LoRA-Fine-Tuned Model...\")\n",
    "merged_model = load_lora_model(\"./my_results/lora_7b\")\n",
    "lora_debug_results = debug_evaluate_model(\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"LoRA Fine-Tuned\"\n",
    ")\n",
    "\n",
    "print(\"\\nFinal comparison:\")\n",
    "print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "print(f\"LoRA     -> BLEU = {lora_debug_results['bleu']:.2f}, COMET = {lora_debug_results['comet']:.3f}\")\n",
    "\n",
    "# Save LoRA results\n",
    "lora_results_path = \"./results/lora_results.json\"\n",
    "with open(lora_results_path, 'w') as f:\n",
    "    json.dump(lora_debug_results, f)\n",
    "print(f\"LoRA results saved to {lora_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87d621-fd30-4b2a-ac69-e071c8963312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 8: Generate baseline translations & label with COMET\n",
    "# --------------------------------------------\n",
    "import json\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "num_samples = 50\n",
    "data_for_labeling = load_dataset(\"wmt19\", \"de-en\", split=\"validation\").select(range(num_samples))\n",
    "\n",
    "print(f\"Loaded {len(data_for_labeling)} examples for labeling.\")\n",
    "\n",
    "comet_metric = evaluate.load(\"comet\")\n",
    "\n",
    "def get_quality_label(score):\n",
    "    if score < 0.2:\n",
    "        return \"Bad\"\n",
    "    elif score < 0.6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Good\"\n",
    "\n",
    "labeled_examples = []\n",
    "for ex in tqdm(data_for_labeling, desc=\"Generating & Labeling\"):\n",
    "    src_de = ex[\"translation\"][\"de\"]\n",
    "    ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "    prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).to(base_model.device)  \n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=4,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"English translation:\" in full_output_text:\n",
    "        pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "    else:\n",
    "        pred_en = full_output_text.strip()\n",
    "\n",
    "\n",
    "    comet_scores = comet_metric.compute(\n",
    "        predictions=[pred_en],\n",
    "        references=[ref_en],\n",
    "        sources=[src_de],\n",
    "        gpus=0,  \n",
    "        progress_bar=False  \n",
    "    )\n",
    "    score = comet_scores[\"scores\"][0]\n",
    "\n",
    "    label = get_quality_label(score)\n",
    "\n",
    "    labeled_examples.append({\n",
    "        \"source_de\": src_de,\n",
    "        \"reference_en\": ref_en,\n",
    "        \"baseline_translation\": pred_en,\n",
    "        \"comet_score\": float(score),\n",
    "        \"quality_label\": label\n",
    "    })\n",
    "\n",
    "output_file = \"./results/baseline_labeled_translations.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labeled_examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nDone. Saved {len(labeled_examples)} labeled examples to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c67e3-b718-42fd-83cb-b89dcfcc8131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722e995-2ef8-44e5-8b87-7f702ff10144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
