{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b624eade-f0ef-4678-b58b-ddb106cced58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# CELL 1\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from datasets import Dataset\n",
    "import csv\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73153aae-bf4c-4706-8b91-402d38f6bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38fc63e7fc54155859e9d6f4b99ec10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model and tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_save_path = \"./saved_models/base_llama\"\n",
    "\n",
    "def load_or_download_base_model():\n",
    "    if os.path.exists(base_model_save_path):\n",
    "        print(\"Loading saved base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_save_path)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_save_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Downloading base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        print(\"Saving base model...\")\n",
    "        tokenizer.save_pretrained(base_model_save_path)\n",
    "        base_model.save_pretrained(base_model_save_path)\n",
    "        print(f\"Base model saved to {base_model_save_path}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer, base_model\n",
    "\n",
    "tokenizer, base_model = load_or_download_base_model()\n",
    "print(\"Base model and tokenizer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68ef0e0f-4b22-47ad-955d-42da4211bd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing all datasets...\n",
      "\n",
      "Training Dataset (Europarl):\n",
      "Number of sentence pairs: 1,838,568\n",
      "File size: 558.89 MB\n",
      "\n",
      "Dev Dataset Size (newstest2018):\n",
      "Number of sentence pairs: 2,998\n",
      "\n",
      "Test Dataset Size (newstest2019):\n",
      "Number of sentence pairs: 2,000\n",
      "\n",
      "Summary:\n",
      "Training pairs: 1,838,568\n",
      "Dev pairs: 2,998\n",
      "Test pairs: 2,000\n"
     ]
    }
   ],
   "source": [
    "# CELL 3\n",
    "\n",
    "def analyze_sgm_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    segments = re.findall(r'<seg id=\\\"\\d+\\\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    return len(segments)\n",
    "\n",
    "def count_tsv_lines(file_path):\n",
    "    count = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(\"Analyzing all datasets...\")\n",
    "\n",
    "# Analyze training data\n",
    "train_file = \"wmt_dataset/train/europarl-v9.de-en.tsv\"\n",
    "if os.path.exists(train_file):\n",
    "    num_train = count_tsv_lines(train_file)\n",
    "    file_size_mb = os.path.getsize(train_file) / (1024 * 1024)\n",
    "    print(f\"\\nTraining Dataset (Europarl):\")\n",
    "    print(f\"Number of sentence pairs: {num_train:,}\")\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"\\nTraining file not found!\")\n",
    "    num_train = 0\n",
    "\n",
    "# Analyze dev data\n",
    "dev_de = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "dev_en = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "if os.path.exists(dev_de) and os.path.exists(dev_en):\n",
    "    num_dev = analyze_sgm_file(dev_de)\n",
    "    print(f\"\\nDev Dataset Size (newstest2018):\")\n",
    "    print(f\"Number of sentence pairs: {num_dev:,}\")\n",
    "else:\n",
    "    print(\"\\nDev files not found!\")\n",
    "    num_dev = 0\n",
    "\n",
    "# Analyze test data\n",
    "test_de = \"wmt_dataset/test/sgm/newstest2019-deen-src.de.sgm\"\n",
    "test_en = \"wmt_dataset/test/sgm/newstest2019-deen-ref.en.sgm\"\n",
    "if os.path.exists(test_de) and os.path.exists(test_en):\n",
    "    num_test = analyze_sgm_file(test_de)\n",
    "    print(f\"\\nTest Dataset Size (newstest2019):\")\n",
    "    print(f\"Number of sentence pairs: {num_test:,}\")\n",
    "else:\n",
    "    print(\"\\nTest files not found!\")\n",
    "    num_test = 0\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Training pairs: {num_train:,}\")\n",
    "print(f\"Dev pairs: {num_dev:,}\")\n",
    "print(f\"Test pairs: {num_test:,}\")\n",
    "\n",
    "if num_train > 0:\n",
    "    recommended_size = min(num_train, 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce09c1f-6e51-43e3-bde7-db4ab7af4ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CELL 4\n",
    "\n",
    "def parse_sgm_to_list(sgm_file):\n",
    "    with open(sgm_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    segments = re.findall(r'<seg id=\\\"\\d+\\\">(.*?)</seg>', content, flags=re.DOTALL)\n",
    "    segments = [seg.strip() for seg in segments]\n",
    "    return segments\n",
    "\n",
    "def load_sgm_parallel(de_sgm_file, en_sgm_file):\n",
    "    de_lines = parse_sgm_to_list(de_sgm_file)\n",
    "    en_lines = parse_sgm_to_list(en_sgm_file)\n",
    "    assert len(de_lines) == len(en_lines), (\n",
    "        f\"Mismatch in line counts: {len(de_lines)} vs {len(en_lines)}\"\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    for de_text, en_text in zip(de_lines, en_lines):\n",
    "        data.append({\n",
    "            \"translation\": {\n",
    "                \"de\": de_text,\n",
    "                \"en\": en_text\n",
    "            }\n",
    "        })\n",
    "    return data\n",
    "\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german_text}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    data_type=\"test\",\n",
    "    num_examples=100,\n",
    "    debug_print=5,\n",
    "    description=\"Model\"\n",
    "):\n",
    "    if data_type == \"test\":\n",
    "        de_path = \"wmt_dataset/test/sgm/newstest2019-deen-src.de.sgm\"\n",
    "        en_path = \"wmt_dataset/test/sgm/newstest2019-deen-ref.en.sgm\"\n",
    "    else:  # dev\n",
    "        de_path = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "        en_path = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "    \n",
    "    raw_data = load_sgm_parallel(de_path, en_path)\n",
    "    raw_dataset = Dataset.from_list(raw_data)\n",
    "    \n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = raw_dataset.select(range(min(num_examples, len(raw_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        if \"English translation:\" in full_output_text:\n",
    "            pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text.split(\"German text:\")[-1].strip()\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fc3bad-16bf-4eba-aa93-2753d996aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local WMT data (train)...\n",
      "Loading training data from wmt_dataset/train/europarl-v9.de-en.tsv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc6bfd785db4c6aa0d6b82a1402b899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building prompt + target text:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data prepared with prompt masking. Total examples: 5000\n"
     ]
    }
   ],
   "source": [
    "# CELL 5\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"English translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"English translation:\", 1)\n",
    "                prompt_part = prompt_part + \"English translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "def load_tsv_parallel(tsv_file):\n",
    "    data = []\n",
    "    with open(tsv_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                data.append({\n",
    "                    \"translation\": {\n",
    "                        \"de\": parts[0].strip(),\n",
    "                        \"en\": parts[1].strip()\n",
    "                    }\n",
    "                })\n",
    "    return data\n",
    "\n",
    "def load_local_wmt(data_type=\"train\", max_examples=None):\n",
    "    if data_type == \"dev\":\n",
    "        de_path = \"wmt_dataset/dev/newstest2018-deen-src.de.sgm\"\n",
    "        en_path = \"wmt_dataset/dev/newstest2018-deen-ref.en.sgm\"\n",
    "        data_list = load_sgm_parallel(de_path, en_path)\n",
    "    elif data_type == \"test\":\n",
    "        de_path = \"wmt_dataset/test/sgm/newstest2019-deen-src.de.sgm\"\n",
    "        en_path = \"wmt_dataset/test/sgm/newstest2019-deen-ref.en.sgm\"\n",
    "        data_list = load_sgm_parallel(de_path, en_path)\n",
    "    elif data_type == \"train\":\n",
    "        tsv_path = \"wmt_dataset/train/europarl-v9.de-en.tsv\"\n",
    "        print(f\"Loading training data from {tsv_path}...\")\n",
    "        data_list = load_tsv_parallel(tsv_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data_type: {data_type}\")\n",
    "    \n",
    "    if max_examples is not None:\n",
    "        data_list = data_list[:max_examples]\n",
    "    \n",
    "    return Dataset.from_list(data_list)\n",
    "\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(data_type=\"train\", max_examples=None): \n",
    "    print(f\"Loading local WMT data ({data_type})...\")\n",
    "    dataset = load_local_wmt(data_type, max_examples)\n",
    "    \n",
    "    if data_type == \"train\":\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        build_full_text,\n",
    "        desc=\"Building prompt + target text\",\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "training_data = load_and_format_wmt(data_type=\"train\", max_examples=5000)  \n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(f\"Training data prepared with prompt masking. Total examples: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9452c598-3393-4026-9938-bd2dd3eadfe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f53c02a62ee463486699dcc57a195c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] Baseline LLaMA (Test) on 100 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA (Test):   0%|                                                         | 0/100 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating Baseline LLaMA (Test):   1%|â–                                                | 1/100 [00:02<03:31,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "SchÃ¶ne MÃ¼nchnerin 2018: SchÃ¶ne MÃ¼nchnerin 2018 in Hvar: Neun Dates\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 4504, 29997, 484, 7957, 305, 1089, 262, 29871, 29906, 29900, 29896, 29947, 29901, 1102, 29997, 484, 7957, 305, 1089, 262, 29871, 29906, 29900, 29896, 29947, 297, 379, 1707, 29901, 2448, 348, 360, 1078, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nSchÃ¶ne MÃ¼nchnerin 2018: SchÃ¶ne MÃ¼nchnerin 2018 in Hvar: Neun Dates\\n\\nEnglish translation:\\nBeautiful Munich Girl 2018: Beautiful Munich Girl 2018 in Hvar: Nine Dates\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Beautiful Munich Girl 2018: Beautiful Munich Girl 2018 in Hvar: Nine Dates'\n",
      "--------------[ REFERENCE ]---------------\n",
      "The Beauty of Munich 2018: the Beauty of Munich 2018 in Hvar: Nine dates\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA (Test):   2%|â–‰                                                | 2/100 [00:04<03:23,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Von az, aktualisiert am 04.05.2018 um 11:11\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29963, 265, 2698, 29892, 11305, 950, 275, 3722, 626, 29871, 29900, 29946, 29889, 29900, 29945, 29889, 29906, 29900, 29896, 29947, 1922, 29871, 29896, 29896, 29901, 29896, 29896, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nVon az, aktualisiert am 04.05.2018 um 11:11\\n\\nEnglish translation:\\nBy az, updated on 04.05.2018 at 11:11\\n\\nGerman text:\\nVon az, aktualisiert am 04.05.2018 um 11:11\\n\\nEnglish translation:\\nBy az, updated on 04.05.2018 at 11:11'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'By az, updated on 04.05.2018 at 11:11'\n",
      "--------------[ REFERENCE ]---------------\n",
      "From A-Z, updated on 04/05/2018 at 11:11\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA (Test):   3%|â–ˆâ–                                               | 3/100 [00:04<01:59,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Ja, sie will...\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29967, 29874, 29892, 2686, 674, 856, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nJa, sie will...\\n\\nEnglish translation:\\nYes, she wants...\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Yes, she wants...'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Yes, she wants to...\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA (Test):   4%|â–ˆâ–‰                                               | 4/100 [00:05<02:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 3\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "\"SchÃ¶ne MÃ¼nchnerin\" 2018 werden!\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29908, 4504, 29997, 484, 7957, 305, 1089, 262, 29908, 29871, 29906, 29900, 29896, 29947, 3678, 29991, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\n\"SchÃ¶ne MÃ¼nchnerin\" 2018 werden!\\n\\nEnglish translation:\\n\"Beautiful Munich girl\" 2018!\\n\\nGerman text:\\n\"SchÃ¶ne MÃ¼nchnerin\" 2018 werden!\\n\\nEnglish translation:\\n\"Beautiful Munich girl\" 2018!\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'\"Beautiful Munich girl\" 2018!'\n",
      "--------------[ REFERENCE ]---------------\n",
      "to become \"The Beauty of Munich\" in 2018!\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA (Test):   5%|â–ˆâ–ˆâ–                                              | 5/100 [00:07<02:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 4\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Am Nachmittag wartet erneut eine Ãœberraschung auf unsere Kandidatinnen: sie werden das romantische Candlelight-Shooting vor der MY SOLARIS nicht alleine bestreiten, sondern an der Seite von Male-Model Fabian!\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 6833, 3975, 18344, 351, 281, 442, 300, 27818, 2128, 12093, 3417, 305, 686, 1622, 9644, 406, 476, 5380, 271, 10190, 29901, 2686, 3678, 1697, 6017, 424, 2010, 28433, 280, 4366, 29899, 29903, 1251, 11427, 3764, 589, 19519, 317, 5607, 1718, 3235, 3072, 4788, 457, 1900, 276, 3537, 29892, 17665, 385, 589, 16073, 1005, 27208, 29899, 3195, 10629, 713, 29991, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nAm Nachmittag wartet erneut eine Ãœberraschung auf unsere Kandidatinnen: sie werden das romantische Candlelight-Shooting vor der MY SOLARIS nicht alleine bestreiten, sondern an der Seite von Male-Model Fabian!\\n\\nEnglish translation:\\nOn the afternoon, again a surprise awaits our candidates: they will not have to face the romantic Candlelight-Shooting before the MY SOLARIS alone, but at the side of male-model Fabian!\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'On the afternoon, again a surprise awaits our candidates: they will not have to face the romantic Candlelight-Shooting before the MY SOLARIS alone, but at the side of male-model Fabian!'\n",
      "--------------[ REFERENCE ]---------------\n",
      "In the afternoon there is another surprise waiting for our contestants: they will be competing for the romantic candlelight photo shoot at MY SOLARIS not alone, but together with a male-model Fabian!\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA (Test): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:23<00:00,  1.43s/it]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline LLaMA (Test)] BLEU = 32.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline LLaMA (Test)] COMET = 0.828\n",
      "\n",
      "Baseline evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# CELL 6\n",
    "print(\"Running baseline evaluation...\")\n",
    "baseline_debug_results = debug_evaluate_model(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_type=\"test\",\n",
    "    num_examples=100,\n",
    "    debug_print=5,\n",
    "    description=\"Baseline LLaMA (Test)\"\n",
    ")\n",
    "\n",
    "baseline_results_path = \"./results/baseline_results.json\"\n",
    "with open(baseline_results_path, 'w') as f:\n",
    "    json.dump(baseline_debug_results, f)\n",
    "\n",
    "print(\"Baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eea2c22-9e8e-4dfa-88d7-c4920ba810e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA model...\n",
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n",
      "LoRA model is ready.\n"
     ]
    }
   ],
   "source": [
    "# CELL 7\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel \n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False \n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                   \n",
    "        lora_alpha=32,         \n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,   \n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \n",
    "            \"v_proj\", \n",
    "            \"k_proj\", \n",
    "            \"o_proj\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    for name, param in lora_model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "print(\"LoRA model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93afa863-4e0e-43bb-bb3a-a8e16e93c17c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up old results directory...\n",
      "Loading local WMT data (dev)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7b4666441a4b57b81864fc30f36082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building prompt + target text:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments set. Will train for 4 epochs on ~5000 examples.\n",
      "Starting LoRA fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1248' max='1248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1248/1248 54:56, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.059277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.058475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.060686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.060302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.060225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.062787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.063222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.062390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.066897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.066941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.066918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "Training metrics:\n",
      "TrainOutput(global_step=1248, training_loss=0.8171704912032837, metrics={'train_runtime': 3299.0926, 'train_samples_per_second': 6.062, 'train_steps_per_second': 0.378, 'total_flos': 4.058464231631094e+17, 'train_loss': 0.8171704912032837, 'epoch': 3.9888})\n",
      "Fine-tuning done. Model saved at ./my_results/lora_7b\n"
     ]
    }
   ],
   "source": [
    "# CELL 8\n",
    "import shutil\n",
    "if os.path.exists(\"./my_results\"):\n",
    "    print(\"Cleaning up old results directory...\")\n",
    "    shutil.rmtree(\"./my_results\")\n",
    "os.makedirs(\"./my_results\")\n",
    "\n",
    "monitor_dataset = load_and_format_wmt(data_type=\"dev\", max_examples=1000)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./my_results\",\n",
    "    num_train_epochs=4,          \n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    save_steps=100,              \n",
    "    logging_steps=20,            \n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=True,  \n",
    "    evaluation_strategy=\"steps\",  \n",
    "    eval_steps=100,              \n",
    "    save_total_limit=2,          \n",
    "    metric_for_best_model=\"loss\" \n",
    ")\n",
    "\n",
    "print(f\"Training arguments set. Will train for {train_args.num_train_epochs} epochs on ~{len(training_data)} examples.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=monitor_dataset,  \n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "trainer.save_model(\"./my_results/lora_7b\")\n",
    "print(\"Fine-tuning done. Model saved at ./my_results/lora_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7263710-7008-4c6e-9bac-80419f38d321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LoRA-Fine-Tuned Model on test data...\n",
      "Loading LoRA model from ./my_results/lora_7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f707acb7b624cd595f4d0d0e61debd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA weights with base model...\n",
      "\n",
      "Verifying model changes...\n",
      "Sample base params: tensor([ 1.2517e-06, -1.7881e-06, -4.3511e-06,  8.0466e-06,  1.9073e-06],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Sample LoRA params: tensor([ 1.2517e-06, -1.7881e-06, -4.3511e-06,  8.0466e-06,  1.9073e-06],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Parameters different: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e0c1aaf9b543158aa714bd7ce5e7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] LoRA Fine-Tuned (Test) on 100 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned (Test):   0%|                                                        | 0/100 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating LoRA Fine-Tuned (Test):   1%|â–                                               | 1/100 [00:00<01:03,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "SchÃ¶ne MÃ¼nchnerin 2018: SchÃ¶ne MÃ¼nchnerin 2018 in Hvar: Neun Dates\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 4504, 29997, 484, 7957, 305, 1089, 262, 29871, 29906, 29900, 29896, 29947, 29901, 1102, 29997, 484, 7957, 305, 1089, 262, 29871, 29906, 29900, 29896, 29947, 297, 379, 1707, 29901, 2448, 348, 360, 1078, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nSchÃ¶ne MÃ¼nchnerin 2018: SchÃ¶ne MÃ¼nchnerin 2018 in Hvar: Neun Dates\\n\\nEnglish translation: Beautiful Munich 2018: Beautiful Munich 2018 in Hvar: nine dates'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Beautiful Munich 2018: Beautiful Munich 2018 in Hvar: nine dates'\n",
      "--------------[ REFERENCE ]---------------\n",
      "The Beauty of Munich 2018: the Beauty of Munich 2018 in Hvar: Nine dates\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned (Test):   3%|â–ˆâ–                                              | 3/100 [00:01<00:38,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Von az, aktualisiert am 04.05.2018 um 11:11\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29963, 265, 2698, 29892, 11305, 950, 275, 3722, 626, 29871, 29900, 29946, 29889, 29900, 29945, 29889, 29906, 29900, 29896, 29947, 1922, 29871, 29896, 29896, 29901, 29896, 29896, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nVon az, aktualisiert am 04.05.2018 um 11:11\\n\\nEnglish translation: az, updated on 04/05/2018 at 11:11'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'az, updated on 04/05/2018 at 11:11'\n",
      "--------------[ REFERENCE ]---------------\n",
      "From A-Z, updated on 04/05/2018 at 11:11\n",
      "==========================================\n",
      "\n",
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Ja, sie will...\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29967, 29874, 29892, 2686, 674, 856, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nJa, sie will...\\n\\nEnglish translation: Yes, she wants...'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Yes, she wants...'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Yes, she wants to...\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned (Test):   4%|â–ˆâ–‰                                              | 4/100 [00:01<00:39,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 3\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "\"SchÃ¶ne MÃ¼nchnerin\" 2018 werden!\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29908, 4504, 29997, 484, 7957, 305, 1089, 262, 29908, 29871, 29906, 29900, 29896, 29947, 3678, 29991, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\n\"SchÃ¶ne MÃ¼nchnerin\" 2018 werden!\\n\\nEnglish translation: 2018 will be the year of the beautiful woman from Munich!'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'2018 will be the year of the beautiful woman from Munich!'\n",
      "--------------[ REFERENCE ]---------------\n",
      "to become \"The Beauty of Munich\" in 2018!\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned (Test):   5%|â–ˆâ–ˆâ–                                             | 5/100 [00:02<01:06,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 4\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Am Nachmittag wartet erneut eine Ãœberraschung auf unsere Kandidatinnen: sie werden das romantische Candlelight-Shooting vor der MY SOLARIS nicht alleine bestreiten, sondern an der Seite von Male-Model Fabian!\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 6833, 3975, 18344, 351, 281, 442, 300, 27818, 2128, 12093, 3417, 305, 686, 1622, 9644, 406, 476, 5380, 271, 10190, 29901, 2686, 3678, 1697, 6017, 424, 2010, 28433, 280, 4366, 29899, 29903, 1251, 11427, 3764, 589, 19519, 317, 5607, 1718, 3235, 3072, 4788, 457, 1900, 276, 3537, 29892, 17665, 385, 589, 16073, 1005, 27208, 29899, 3195, 10629, 713, 29991, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nAm Nachmittag wartet erneut eine Ãœberraschung auf unsere Kandidatinnen: sie werden das romantische Candlelight-Shooting vor der MY SOLARIS nicht alleine bestreiten, sondern an der Seite von Male-Model Fabian!\\n\\nEnglish translation: In the afternoon, there is another surprise in store for our candidates: they will not have to face the romantic Candlelight-Shooting on MY SOLARIS on their own, but will be accompanied by male model Fabian!'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'In the afternoon, there is another surprise in store for our candidates: they will not have to face the romantic Candlelight-Shooting on MY SOLARIS on their own, but will be accompanied by male model Fabian!'\n",
      "--------------[ REFERENCE ]---------------\n",
      "In the afternoon there is another surprise waiting for our contestants: they will be competing for the romantic candlelight photo shoot at MY SOLARIS not alone, but together with a male-model Fabian!\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LoRA Fine-Tuned (Test): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:00<00:00,  1.66it/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned (Test)] BLEU = 44.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA Fine-Tuned (Test)] COMET = 0.850\n",
      "\n",
      "\n",
      "Final comparison on test set:\n",
      "Baseline -> BLEU = 32.56, COMET = 0.828\n",
      "LoRA     -> BLEU = 44.83, COMET = 0.850\n",
      "LoRA test results saved to ./results/lora_test_results.json\n"
     ]
    }
   ],
   "source": [
    "# CELL 9\n",
    "from peft import PeftModel, PeftConfig\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sacrebleu import corpus_bleu\n",
    "import torch\n",
    "\n",
    "def load_lora_model(checkpoint_path=\"./my_results/lora_7b\"):\n",
    "    print(f\"Loading LoRA model from {checkpoint_path}...\")\n",
    "    \n",
    "    # Load base model with FP16\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_save_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=False\n",
    "    )\n",
    "    \n",
    "    print(\"Loading LoRA weights...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        checkpoint_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"Merging LoRA weights with base model...\")\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\nVerifying model changes...\")\n",
    "    sample_base_params = next(base.parameters()).detach().flatten()[:5]\n",
    "    sample_lora_params = next(model.parameters()).detach().flatten()[:5]\n",
    "    print(f\"Sample base params: {sample_base_params}\")\n",
    "    print(f\"Sample LoRA params: {sample_lora_params}\")\n",
    "    print(f\"Parameters different: {not torch.allclose(sample_base_params, sample_lora_params)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"\\nEvaluating LoRA-Fine-Tuned Model on test data...\")\n",
    "merged_model = load_lora_model(\"./my_results/lora_7b\")\n",
    "\n",
    "lora_test_results = debug_evaluate_model(\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_type=\"test\",\n",
    "    num_examples=100,\n",
    "    debug_print=5,\n",
    "    description=\"LoRA Fine-Tuned (Test)\"\n",
    ")\n",
    "\n",
    "print(\"\\nFinal comparison on test set:\")\n",
    "print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "print(f\"LoRA     -> BLEU = {lora_test_results['bleu']:.2f}, COMET = {lora_test_results['comet']:.3f}\")\n",
    "\n",
    "lora_results_path = \"./results/lora_test_results.json\"\n",
    "with open(lora_results_path, 'w') as f:\n",
    "    json.dump(lora_test_results, f)\n",
    "print(f\"LoRA test results saved to {lora_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba704a9-af91-4fe7-87e2-e8563dd04290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data for labeling (50 examples)...\n",
      "Loaded 50 examples for labeling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b602e210a6fc4560898a16c4efeb1dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "Generating & Labeling:   0%|                                                                     | 0/50 [00:00<?, ?it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:   2%|â–ˆâ–                                                           | 1/50 [00:02<02:26,  2.99s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:   4%|â–ˆâ–ˆâ–                                                          | 2/50 [00:05<02:02,  2.54s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:   6%|â–ˆâ–ˆâ–ˆâ–‹                                                         | 3/50 [00:06<01:41,  2.16s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                        | 4/50 [00:08<01:37,  2.12s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                       | 5/50 [00:12<01:57,  2.60s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                     | 6/50 [00:14<01:51,  2.53s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                    | 7/50 [00:16<01:39,  2.31s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                   | 8/50 [00:19<01:47,  2.55s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                  | 9/50 [00:21<01:36,  2.36s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                | 10/50 [00:23<01:31,  2.28s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 11/50 [00:26<01:32,  2.38s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 12/50 [00:30<01:52,  2.95s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                            | 13/50 [00:33<01:52,  3.05s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                           | 14/50 [00:35<01:38,  2.73s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 15/50 [00:38<01:30,  2.59s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                        | 16/50 [00:41<01:32,  2.73s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                       | 17/50 [00:44<01:31,  2.78s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 18/50 [00:47<01:31,  2.87s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 19/50 [00:49<01:22,  2.65s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                    | 20/50 [00:51<01:18,  2.62s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 21/50 [00:54<01:13,  2.52s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 22/50 [00:56<01:09,  2.47s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 23/50 [00:58<01:05,  2.43s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 24/50 [01:01<01:02,  2.39s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 25/50 [01:02<00:55,  2.20s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 26/50 [01:05<00:53,  2.24s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 27/50 [01:07<00:52,  2.29s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 28/50 [01:10<00:50,  2.31s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 29/50 [01:12<00:48,  2.33s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 30/50 [01:14<00:46,  2.31s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 31/50 [01:16<00:42,  2.22s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 32/50 [01:19<00:44,  2.46s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 33/50 [01:21<00:40,  2.40s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 34/50 [01:24<00:37,  2.33s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 35/50 [01:26<00:37,  2.49s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 36/50 [01:29<00:34,  2.45s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 37/50 [01:31<00:29,  2.30s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 38/50 [01:33<00:26,  2.18s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 39/50 [01:35<00:25,  2.32s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 40/50 [01:37<00:21,  2.19s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 41/50 [01:40<00:19,  2.22s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 42/50 [01:42<00:17,  2.22s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 43/50 [01:44<00:15,  2.28s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 44/50 [01:46<00:13,  2.25s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 45/50 [01:48<00:11,  2.21s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 46/50 [01:51<00:09,  2.26s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 47/50 [01:54<00:07,  2.40s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/50 [01:56<00:04,  2.26s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 49/50 [01:58<00:02,  2.25s/it]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating & Labeling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:01<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Saved 50 labeled examples to ./results/baseline_labeled_translations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 10\n",
    "num_samples = 50\n",
    "print(f\"Loading test data for labeling ({num_samples} examples)...\")\n",
    "\n",
    "test_de = \"wmt_dataset/test/sgm/newstest2019-deen-src.de.sgm\"\n",
    "test_en = \"wmt_dataset/test/sgm/newstest2019-deen-ref.en.sgm\"\n",
    "raw_data = load_sgm_parallel(test_de, test_en)\n",
    "data_for_labeling = Dataset.from_list(raw_data[:num_samples])\n",
    "\n",
    "print(f\"Loaded {len(data_for_labeling)} examples for labeling.\")\n",
    "\n",
    "comet_metric = evaluate.load(\"comet\")\n",
    "\n",
    "def get_quality_label(score):\n",
    "    if score < 0.2:\n",
    "        return \"Bad\"\n",
    "    elif score < 0.6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Good\"\n",
    "\n",
    "labeled_examples = []\n",
    "for ex in tqdm(data_for_labeling, desc=\"Generating & Labeling\"):\n",
    "    src_de = ex[\"translation\"][\"de\"]\n",
    "    ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "    prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).to(base_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            num_beams=4,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"English translation:\" in full_output_text:\n",
    "        pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "    else:\n",
    "        pred_en = full_output_text.strip()\n",
    "\n",
    "    comet_scores = comet_metric.compute(\n",
    "        predictions=[pred_en],\n",
    "        references=[ref_en],\n",
    "        sources=[src_de]\n",
    "    )\n",
    "    score = comet_scores[\"scores\"][0]\n",
    "\n",
    "    label = get_quality_label(score)\n",
    "\n",
    "    labeled_examples.append({\n",
    "        \"source_de\": src_de,\n",
    "        \"reference_en\": ref_en,\n",
    "        \"baseline_translation\": pred_en,\n",
    "        \"comet_score\": float(score),\n",
    "        \"quality_label\": label\n",
    "    })\n",
    "\n",
    "output_file = \"./results/baseline_labeled_translations.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labeled_examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nDone. Saved {len(labeled_examples)} labeled examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd27454-0b63-48ed-9635-7df72b34a2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
