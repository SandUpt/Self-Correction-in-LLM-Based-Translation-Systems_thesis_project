{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b624eade-f0ef-4678-b58b-ddb106cced58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.25.2\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 1: Initial imports and environment setup\n",
    "# --------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./my_results\", exist_ok=True)\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73153aae-bf4c-4706-8b91-402d38f6bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5c01722ab84c18bf71fc3adb71d22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving base model...\n",
      "Base model saved to ./saved_models/base_llama\n",
      "Base model and tokenizer ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Base Model & Tokenizer (Baseline)\n",
    "# --------------------------------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_save_path = \"./saved_models/base_llama\"\n",
    "\n",
    "def load_or_download_base_model():\n",
    "    if os.path.exists(base_model_save_path):\n",
    "        print(\"Loading saved base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_save_path)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_save_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Downloading base model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # Save the model and tokenizer\n",
    "        print(\"Saving base model...\")\n",
    "        tokenizer.save_pretrained(base_model_save_path)\n",
    "        base_model.save_pretrained(base_model_save_path)\n",
    "        print(f\"Base model saved to {base_model_save_path}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer, base_model\n",
    "\n",
    "# Load/download the model\n",
    "tokenizer, base_model = load_or_download_base_model()\n",
    "print(\"Base model and tokenizer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce09c1f-6e51-43e3-bde7-db4ab7af4ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT19 (de-en) validation data with 50 examples...\n",
      "Evaluating baseline model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3cf872e5764b2cac07001b32802cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/f49d328952c3470eff6bb6f545d62bfdb6e66304/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG EVAL] Baseline LLaMA on 20 examples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:   0%|                                                                 | 0/20 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Evaluating Baseline LLaMA:   5%|â–ˆâ–ˆâ–Š                                                      | 1/20 [00:02<00:52,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 0\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "MÃ¼nchen 1856: Vier Karten, die Ihren Blick auf die Stadt verÃ¤ndern\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29924, 3346, 2724, 29871, 29896, 29947, 29945, 29953, 29901, 23650, 476, 8109, 29892, 762, 306, 13608, 350, 1406, 1622, 762, 5587, 1147, 3140, 824, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nMÃ¼nchen 1856: Vier Karten, die Ihren Blick auf die Stadt verÃ¤ndern\\n\\nEnglish translation:\\nMunich 1856: Four cards that change your view of the city\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'Munich 1856: Four cards that change your view of the city'\n",
      "--------------[ REFERENCE ]---------------\n",
      "Munich 1856: Four maps that will change your view of the city\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                   | 2/20 [00:03<00:30,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 1\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Eine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 6600, 1267, 29899, 2744, 303, 1997, 29892, 8879, 2160, 12843, 19472, 4545, 1812, 387, 4566, 899, 2435, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "\"You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nEine Irren-Anstalt, wo sich heute Jugendliche begegnen sollen.\\n\\nEnglish translation:\\nAn insane asylum, where today's youth should meet.\\n\"\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "\"An insane asylum, where today's youth should meet.\"\n",
      "--------------[ REFERENCE ]---------------\n",
      "A mental asylum, where today young people are said to meet.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                | 3/20 [00:05<00:26,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "Example 2\n",
      "---------------[ PROMPT ]-----------------\n",
      "You are an expert German-English translator with deep knowledge of both languages.\n",
      "\n",
      "Instructions:\n",
      "- Translate the German text into natural, fluent English\n",
      "- Maintain the original meaning and tone\n",
      "- Use appropriate idioms and expressions\n",
      "- Ensure cultural nuances are properly conveyed\n",
      "\n",
      "German text:\n",
      "Eine Gruftkapelle, wo nun fÃ¼r den S-Bahn-Tunnel gegraben wird.\n",
      "\n",
      "English translation:\n",
      "--------------[ TOKENIZED ]---------------\n",
      "Input IDs: [1, 887, 526, 385, 17924, 5332, 29899, 24636, 5578, 1061, 411, 6483, 7134, 310, 1716, 10276, 29889, 13, 13, 3379, 582, 1953, 29901, 13, 29899, 4103, 9632, 278, 5332, 1426, 964, 5613, 29892, 1652, 8122, 4223, 13, 29899, 341, 2365, 475, 278, 2441, 6593, 322, 16225, 13, 29899, 4803, 8210, 1178, 29875, 4835, 322, 12241, 13, 29899, 22521, 545, 16375, 4948, 2925, 526, 6284, 27769, 287, 13, 13, 29954, 3504, 1426, 29901, 13, 29923, 457, 5430, 615, 21474, 1808, 29892, 8879, 11923, 1865, 972, 317, 29899, 29933, 5422, 29899, 29911, 16163, 21598, 336, 1785, 4296, 29889, 13, 13, 24636, 13962, 29901]\n",
      "-----------[ FULL MODEL OUTPUT ]----------\n",
      "'You are an expert German-English translator with deep knowledge of both languages.\\n\\nInstructions:\\n- Translate the German text into natural, fluent English\\n- Maintain the original meaning and tone\\n- Use appropriate idioms and expressions\\n- Ensure cultural nuances are properly conveyed\\n\\nGerman text:\\nEine Gruftkapelle, wo nun fÃ¼r den S-Bahn-Tunnel gegraben wird.\\n\\nEnglish translation:\\nA graveyard chapel, where now a S-Bahn tunnel is being dug.\\n'\n",
      "-------------[ EXTRACTED EN ]-------------\n",
      "'A graveyard chapel, where now a S-Bahn tunnel is being dug.'\n",
      "--------------[ REFERENCE ]---------------\n",
      "A crypt chapel, where they are now digging tunnels for the S-Bahn.\n",
      "==========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline LLaMA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:50<00:00,  2.52s/it]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline LLaMA] BLEU = 64.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline LLaMA] COMET = 0.786\n",
      "\n",
      "Baseline evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Evaluation Data & Get Baseline Scores\n",
    "# --------------------------------------------\n",
    "def build_prompt_for_translation(german_text: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german_text}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def debug_evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"Model\"\n",
    "):\n",
    "    comet_metric = evaluate.load(\"comet\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    subset = eval_dataset.select(range(min(num_examples, len(eval_dataset))))\n",
    "    print(f\"\\n[DEBUG EVAL] {description} on {num_examples} examples...\\n\")\n",
    "\n",
    "    for i, ex in enumerate(tqdm(subset, desc=f\"Evaluating {description}\")):\n",
    "        src_de = ex[\"translation\"][\"de\"]\n",
    "        ref_en = ex[\"translation\"][\"en\"]\n",
    "\n",
    "        prompt_text = build_prompt_for_translation(src_de)\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **tokenized_input,\n",
    "                max_new_tokens=256,\n",
    "                num_beams=4,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        full_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Fixed extraction logic\n",
    "        if \"English translation:\" in full_output_text:\n",
    "            pred_en = full_output_text.split(\"English translation:\")[-1].strip()\n",
    "        else:\n",
    "            pred_en = full_output_text.split(\"German text:\")[-1].strip()\n",
    "\n",
    "        predictions.append(pred_en)\n",
    "        references.append([ref_en])\n",
    "        sources.append(src_de)\n",
    "\n",
    "        if i < debug_print:\n",
    "            print(\"\\n==========================================\")\n",
    "            print(f\"Example {i}\")\n",
    "            print(\"---------------[ PROMPT ]-----------------\")\n",
    "            print(prompt_text)\n",
    "            print(\"--------------[ TOKENIZED ]---------------\")\n",
    "            print(f\"Input IDs: {tokenized_input['input_ids'][0].tolist()}\")\n",
    "            print(\"-----------[ FULL MODEL OUTPUT ]----------\")\n",
    "            print(repr(full_output_text))\n",
    "            print(\"-------------[ EXTRACTED EN ]-------------\")\n",
    "            print(repr(pred_en))\n",
    "            print(\"--------------[ REFERENCE ]---------------\")\n",
    "            print(ref_en)\n",
    "            print(\"==========================================\\n\")\n",
    "\n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    print(f\"[{description}] BLEU = {bleu.score:.2f}\")\n",
    "\n",
    "    comet_results = comet_metric.compute(\n",
    "        predictions=predictions,\n",
    "        references=[r[0] for r in references],\n",
    "        sources=sources\n",
    "    )\n",
    "    print(f\"[{description}] COMET = {comet_results['mean_score']:.3f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"bleu\": bleu.score,\n",
    "        \"comet\": comet_results[\"mean_score\"]\n",
    "    }\n",
    "\n",
    "def load_eval_data(num_examples=50):\n",
    "    print(f\"Loading WMT19 (de-en) validation data with {num_examples} examples...\")\n",
    "    eval_data = load_dataset(\"wmt19\", \"de-en\", split=\"validation\")\n",
    "    eval_data = eval_data.select(range(min(num_examples, len(eval_data))))\n",
    "    return eval_data\n",
    "\n",
    "# Get baseline scores immediately\n",
    "eval_dataset = load_eval_data(num_examples=50)\n",
    "baseline_results_path = \"./results/baseline_results.json\"\n",
    "\n",
    "if os.path.exists(baseline_results_path):\n",
    "    print(\"Loading saved baseline results...\")\n",
    "    with open(baseline_results_path, 'r') as f:\n",
    "        baseline_debug_results = json.load(f)\n",
    "    print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "else:\n",
    "    print(\"Evaluating baseline model...\")\n",
    "    baseline_debug_results = debug_evaluate_model(\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        eval_dataset=eval_dataset,\n",
    "        num_examples=20,\n",
    "        debug_print=3,\n",
    "        description=\"Baseline LLaMA\"\n",
    "    )\n",
    "    # Save results\n",
    "    with open(baseline_results_path, 'w') as f:\n",
    "        json.dump(baseline_debug_results, f)\n",
    "\n",
    "print(\"Baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9452c598-3393-4026-9938-bd2dd3eadfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading WMT19 (de-en) train data with 5000 examples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248d84d14daa42cb8e663a894c5f8985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building prompt + target text:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13ee3d82a2b4d99b9f0e8e2e3f3729d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./saved_datasets/wmt19_train_5000\n",
      "Training data prepared with prompt masking.\n"
     ]
    }
   ],
   "source": [
    " #--------------------------------------------\n",
    "# Cell 4: Prepare Training Data with Improved Prompt\n",
    "# --------------------------------------------\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "def build_full_text(example):\n",
    "    german = example[\"translation\"][\"de\"]\n",
    "    english = example[\"translation\"][\"en\"]\n",
    "    prompt = (\n",
    "        \"You are an expert German-English translator with deep knowledge of both languages.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Translate the German text into natural, fluent English\\n\"\n",
    "        \"- Maintain the original meaning and tone\\n\"\n",
    "        \"- Use appropriate idioms and expressions\\n\"\n",
    "        \"- Ensure cultural nuances are properly conveyed\\n\\n\"\n",
    "        f\"German text:\\n{german}\\n\\n\"\n",
    "        \"English translation:\"\n",
    "    )\n",
    "    full_text = prompt + \" \" + english\n",
    "    return {\"full_text\": full_text}\n",
    "\n",
    "def load_and_format_wmt(num_examples=5000):\n",
    "    dataset_save_path = f\"./saved_datasets/wmt19_train_{num_examples}\"\n",
    "    \n",
    "    if os.path.exists(dataset_save_path):\n",
    "        print(f\"Loading saved WMT19 dataset from {dataset_save_path}...\")\n",
    "        dataset = datasets.load_from_disk(dataset_save_path)\n",
    "    else:\n",
    "        print(f\"Downloading WMT19 (de-en) train data with {num_examples} examples...\")\n",
    "        dataset = load_dataset(\"wmt19\", \"de-en\", split=\"train\")\n",
    "        dataset = dataset.shuffle(seed=42).select(range(num_examples))\n",
    "        \n",
    "        dataset = dataset.map(\n",
    "            build_full_text,\n",
    "            desc=\"Building prompt + target text\",\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        os.makedirs(\"./saved_datasets\", exist_ok=True)\n",
    "        dataset.save_to_disk(dataset_save_path)\n",
    "        print(f\"Dataset saved to {dataset_save_path}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "class PromptMaskCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, examples):\n",
    "        texts = [ex[\"full_text\"] for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            if \"English translation:\" in text:\n",
    "                prompt_part, _ = text.split(\"English translation:\", 1)\n",
    "                prompt_part = prompt_part + \"English translation:\"\n",
    "            else:\n",
    "                prompt_part = text\n",
    "            \n",
    "            prompt_ids = self.tokenizer(\n",
    "                prompt_part,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                add_special_tokens=True\n",
    "            )[\"input_ids\"]\n",
    "            \n",
    "            prompt_len = len(prompt_ids)\n",
    "            if prompt_len > labels.size(1):\n",
    "                prompt_len = labels.size(1)\n",
    "            \n",
    "            labels[i, :prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Load training data\n",
    "training_data = load_and_format_wmt(num_examples=5000)\n",
    "data_collator = PromptMaskCollator(tokenizer, max_length=512)\n",
    "\n",
    "print(\"Training data prepared with prompt masking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93afa863-4e0e-43bb-bb3a-a8e16e93c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LoRA model...\n",
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484\n",
      "LoRA model is ready.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 5: Updated LoRA Configuration\n",
    "# --------------------------------------------\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel \n",
    "\n",
    "def setup_lora_model():\n",
    "    print(\"Setting up LoRA model...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False \n",
    "        \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,                    \n",
    "        lora_alpha=32,          \n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,    \n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \n",
    "            \"v_proj\", \n",
    "            \"k_proj\", \n",
    "            \"o_proj\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    for name, param in lora_model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "    return lora_model\n",
    "\n",
    "model_for_training = setup_lora_model()\n",
    "print(\"LoRA model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7263710-7008-4c6e-9bac-80419f38d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved WMT19 dataset from ./saved_datasets/wmt19_train_1000...\n",
      "Training arguments set. Will train for 4 epochs on ~5000 examples.\n",
      "trainable params: 256 with total size: 16777216\n",
      "all params: 6755192832, trainable%: 0.2484%\n",
      "Starting LoRA fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [624/624 2:11:35, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished.\n",
      "Training metrics:\n",
      "TrainOutput(global_step=624, training_loss=1.8458855952589939, metrics={'train_runtime': 7908.0716, 'train_samples_per_second': 2.529, 'train_steps_per_second': 0.079, 'total_flos': 4.04869656801706e+17, 'train_loss': 1.8458855952589939, 'epoch': 3.9792})\n",
      "Fine-tuning done. Model saved at ./my_results/lora_7b\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "eval_data_training = load_and_format_wmt(num_examples=1000)\n",
    "\n",
    "model_for_training.train()\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./my_results\",\n",
    "    num_train_epochs=4,           \n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,  \n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,\n",
    "    save_steps=1000,\n",
    "    logging_steps=50,            \n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,  \n",
    "    evaluation_strategy=\"steps\",   \n",
    "    eval_steps=100,              \n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    optim=\"adamw_torch\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    report_to=\"tensorboard\",     \n",
    "    logging_dir=\"./logs\",        \n",
    "    logging_first_step=True      \n",
    ")\n",
    "\n",
    "print(f\"Training arguments set. Will train for {train_args.num_train_epochs} epochs on ~{len(training_data)} examples.\")\n",
    "\n",
    "trainable_params = []\n",
    "all_param_size = 0\n",
    "trainable_param_size = 0\n",
    "\n",
    "for name, param in model_for_training.named_parameters():\n",
    "    all_param_size += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params.append(name)\n",
    "        trainable_param_size += param.numel()\n",
    "print(f\"trainable params: {len(trainable_params)} with total size: {trainable_param_size}\")\n",
    "print(f\"all params: {all_param_size}, trainable%: {100 * trainable_param_size / all_param_size:.4f}%\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_training,\n",
    "    args=train_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=eval_data_training,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "train_output = trainer.train()\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "print(train_output)\n",
    "\n",
    "trainer.save_model(\"./my_results/lora_7b\")\n",
    "print(\"Fine-tuning done. Model saved at ./my_results/lora_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b31f58-871c-438e-9c82-67539e2d8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model_save_path = \"./saved_models/base_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba704a9-af91-4fe7-87e2-e8563dd04290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/utils/modeling.py:1593: UserWarning: Current model requires 4224 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 666910848 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LoRA-Fine-Tuned Model...\n",
      "Loading LoRA model from ./my_results/lora_7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a860c484d7214899ad76cd2de02510a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded. Now loading and merging LoRA weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/utils/modeling.py:1593: UserWarning: Current model requires 8448 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 669008128 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA weights loaded. Merging weights...\n",
      "Weights merged successfully.\n",
      "\n",
      "Verifying models are different:\n",
      "Base Model ID: 140174717987088\n",
      "LoRA Model ID: 140159898659792\n",
      "\n",
      "Comparing translations between base and fine-tuned models:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Cell 7: Verify and Evaluate Fine-tuned Model\n",
    "# --------------------------------------------\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from sacrebleu import corpus_bleu\n",
    "#import gc\n",
    "#torch.cuda.empty_cache()\n",
    "#gc.collect()\n",
    "\n",
    "def load_lora_model(checkpoint_path=\"./my_results/lora_7b\"):\n",
    "    print(f\"Loading LoRA model from {checkpoint_path}...\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_save_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    print(\"Base model loaded. Now loading and merging LoRA weights...\")\n",
    "    \n",
    "    lora_model_loaded = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    print(\"LoRA weights loaded. Merging weights...\")\n",
    "    \n",
    "    lora_model_loaded = lora_model_loaded.merge_and_unload()\n",
    "    print(\"Weights merged successfully.\")\n",
    "    \n",
    "    # Verify the models are different\n",
    "    print(\"\\nVerifying models are different:\")\n",
    "    print(f\"Base Model ID: {id(base_model)}\")\n",
    "    print(f\"LoRA Model ID: {id(lora_model_loaded)}\")\n",
    "    \n",
    "    return lora_model_loaded\n",
    "\n",
    "def compare_translations(model1, model2, tokenizer, text, name1=\"Base\", name2=\"LoRA\"):\n",
    "    \"\"\"Compare translations from two models\"\"\"\n",
    "    prompt = build_prompt_for_translation(text)\n",
    "    \n",
    "    def get_translation(model, prompt):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"English translation:\" in translation:\n",
    "            translation = translation.split(\"English translation:\")[-1].strip()\n",
    "        return translation\n",
    "    \n",
    "    trans1 = get_translation(model1, prompt)\n",
    "    trans2 = get_translation(model2, prompt)\n",
    "    \n",
    "    print(f\"\\nGerman: {text}\")\n",
    "    print(f\"{name1}: {trans1}\")\n",
    "    print(f\"{name2}: {trans2}\")\n",
    "    print(\"Different?\" if trans1 != trans2 else \"Same\")\n",
    "    return trans1 != trans2\n",
    "\n",
    "print(\"\\nEvaluating LoRA-Fine-Tuned Model...\")\n",
    "merged_model = load_lora_model(\"./my_results/lora_7b\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"Die Sonne scheint heute besonders hell.\",\n",
    "    \"KÃ¼nstliche Intelligenz verÃ¤ndert die Welt.\",\n",
    "    \"Der kleine Hund spielt im Garten.\",\n",
    "    \"MÃ¼nchen ist eine wunderschÃ¶ne Stadt.\"\n",
    "]\n",
    "\n",
    "print(\"\\nComparing translations between base and fine-tuned models:\")\n",
    "differences_found = 0\n",
    "for text in test_sentences:\n",
    "    if compare_translations(base_model, merged_model, tokenizer, text):\n",
    "        differences_found += 1\n",
    "\n",
    "print(f\"\\nFound differences in {differences_found}/{len(test_sentences)} translations\")\n",
    "\n",
    "print(\"\\nRunning full evaluation...\")\n",
    "lora_debug_results = debug_evaluate_model(\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_examples=20,\n",
    "    debug_print=3,\n",
    "    description=\"LoRA Fine-Tuned\"\n",
    ")\n",
    "\n",
    "print(\"\\nFinal comparison:\")\n",
    "print(f\"Baseline -> BLEU = {baseline_debug_results['bleu']:.2f}, COMET = {baseline_debug_results['comet']:.3f}\")\n",
    "print(f\"LoRA     -> BLEU = {lora_debug_results['bleu']:.2f}, COMET = {lora_debug_results['comet']:.3f}\")\n",
    "\n",
    "lora_results_path = \"./results/lora_results.json\"\n",
    "with open(lora_results_path, 'w') as f:\n",
    "    json.dump(lora_debug_results, f)\n",
    "print(f\"LoRA results saved to {lora_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87d621-fd30-4b2a-ac69-e071c8963312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c67e3-b718-42fd-83cb-b89dcfcc8131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722e995-2ef8-44e5-8b87-7f702ff10144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
