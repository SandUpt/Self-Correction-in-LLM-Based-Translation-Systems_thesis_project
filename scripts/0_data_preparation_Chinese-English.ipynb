{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838a503-6139-4538-b21b-79e90ac4eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking raw data structure:\n",
      "\n",
      "Raw directory exists: True\n",
      "\n",
      "Contents of raw/wmt_zh_en:\n",
      "  newscommentary/news-commentary-v15.en-zh.tsv.gz\n",
      "  newstest/newstest2017-zhen-ref.en.sgm\n",
      "  newstest/newstest2017-zhen-src.zh.sgm\n",
      "  newstest/newstest2018-zhen-ref.en.sgm\n",
      "  newstest/newstest2018-zhen-src.zh.sgm\n",
      "  newstest/newstest2019-zhen-ref.en.sgm\n",
      "  newstest/newstest2019-zhen-src.zh.sgm\n",
      "  newstest/newstestB2020-zhen-ref.en.sgm\n",
      "  newstest/newstestB2020-zhen-src.zh.sgm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "# paths\n",
    "base_dir = Path(\"../data\")\n",
    "raw_dir = base_dir / \"raw\" / \"wmt_zh_en\"\n",
    "\n",
    "print(\"Checking raw data structure:\")\n",
    "print(f\"\\nRaw directory exists: {raw_dir.exists()}\")\n",
    "\n",
    "if raw_dir.exists():\n",
    "    print(\"\\nContents of raw/wmt_zh_en:\")\n",
    "    for item in raw_dir.rglob(\"*\"):\n",
    "        if item.is_file():\n",
    "            print(f\"  {item.relative_to(raw_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21e2c9a-a334-4ac1-a65d-f65f1af08622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder structure created\n"
     ]
    }
   ],
   "source": [
    "processed_dir = base_dir / \"processed\" / \"zh_en\"\n",
    "eval_dir = base_dir / \"evaluation_sets\" / \"zh_en\"\n",
    "error_dir = base_dir / \"error_dataset\" / \"zh_en\"\n",
    "reserve_dir = base_dir / \"reserve\" / \"zh_en\"\n",
    "\n",
    "for dir_path in [processed_dir, eval_dir, error_dir, reserve_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Folder structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a88c2c4-1da3-4e70-8b52-e80a8f106d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample lines from training data:\n",
      "Line 1: 2 fields\n",
      "  Field 1 (first 80 chars): 1929 or 1989?\n",
      "  Field 2 (first 80 chars): 1929年还是1989年?\n",
      "Line 2: 2 fields\n",
      "  Field 1 (first 80 chars): PARIS – As the economic crisis deepens and widens, the world has been searching \n",
      "  Field 2 (first 80 chars): 巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
      "Line 3: 2 fields\n",
      "  Field 1 (first 80 chars): At the start of the crisis, many people likened it to 1982 or 1973, which was re\n",
      "  Field 2 (first 80 chars): 一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。\n",
      "Line 4: 2 fields\n",
      "  Field 1 (first 80 chars): Today, the mood is much grimmer, with references to 1929 and 1931 beginning to a\n",
      "  Field 2 (first 80 chars): 如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。\n",
      "Line 5: 2 fields\n",
      "  Field 1 (first 80 chars): The tendency is either excessive restraint (Europe) or a diffusion of the effort\n",
      "  Field 2 (first 80 chars): 目前的趋势是，要么是过度的克制（欧洲 ） ， 要么是努力的扩展（美国 ） 。\n"
     ]
    }
   ],
   "source": [
    "train_file = raw_dir / \"newscommentary\" / \"news-commentary-v15.en-zh.tsv.gz\"\n",
    "\n",
    "with gzip.open(train_file, 'rt', encoding='utf-8') as f:\n",
    "    first_lines = [next(f) for _ in range(5)]\n",
    "    \n",
    "print(\"Sample lines from training data:\")\n",
    "for i, line in enumerate(first_lines, 1):\n",
    "    fields = line.strip().split('\\t')\n",
    "    print(f\"Line {i}: {len(fields)} fields\")\n",
    "    if len(fields) >= 2:\n",
    "        print(f\"  Field 1 (first 80 chars): {fields[0][:80]}\")\n",
    "        print(f\"  Field 2 (first 80 chars): {fields[1][:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a3f953-1a62-4e71-bf90-4d4ad4ab8b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 320,713\n",
      "Valid pairs: 312,268\n",
      "Problematic lines: 498\n"
     ]
    }
   ],
   "source": [
    "train_file = raw_dir / \"newscommentary\" / \"news-commentary-v15.en-zh.tsv.gz\"\n",
    "\n",
    "source_texts = []\n",
    "target_texts = []\n",
    "problematic_lines = 0\n",
    "total_lines = 0\n",
    "\n",
    "with gzip.open(train_file, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        total_lines += 1\n",
    "        if not line.strip():\n",
    "            continue\n",
    "            \n",
    "        fields = [f.strip() for f in line.strip().split('\\t') if f.strip()]\n",
    "        \n",
    "        if len(fields) == 2:\n",
    "            source_texts.append(fields[1])  # Chinese\n",
    "            target_texts.append(fields[0])  # English\n",
    "        else:\n",
    "            problematic_lines += 1\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'source_zh': source_texts,\n",
    "    'target_en': target_texts\n",
    "})\n",
    "\n",
    "print(f\"Total lines: {total_lines:,}\")\n",
    "print(f\"Valid pairs: {len(train_df):,}\")\n",
    "print(f\"Problematic lines: {problematic_lines:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de673684-562c-40a6-b5c4-8de7897db0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After word length filtering (10-40): 1,610\n",
      "After deduplication: 1,604\n"
     ]
    }
   ],
   "source": [
    "train_df['source_words'] = train_df['source_zh'].str.split().str.len()\n",
    "train_df['target_words'] = train_df['target_en'].str.split().str.len()\n",
    "\n",
    "train_filtered = train_df[\n",
    "    (train_df['source_words'] >= 10) & \n",
    "    (train_df['source_words'] <= 40) &\n",
    "    (train_df['target_words'] >= 10) & \n",
    "    (train_df['target_words'] <= 40)\n",
    "].copy()\n",
    "\n",
    "train_filtered = train_filtered.drop(['source_words', 'target_words'], axis=1)\n",
    "train_clean = train_filtered.drop_duplicates(subset=['source_zh', 'target_en'])\n",
    "\n",
    "print(f\"After word length filtering (10-40): {len(train_filtered):,}\")\n",
    "print(f\"After deduplication: {len(train_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117ee840-d040-46fa-a598-e232b7615a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newstest2019: 2000 pairs\n",
      "newstest2018: 3981 pairs\n",
      "newstest2017: 2001 pairs\n",
      "newstestB2020: 2000 pairs\n",
      "\n",
      "Total test pairs: 9,982\n"
     ]
    }
   ],
   "source": [
    "def read_sgm_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    soup = BeautifulSoup(content, 'xml')\n",
    "    return [seg.text.strip() for seg in soup.find_all('seg')]\n",
    "\n",
    "test_files = [\n",
    "    (\"newstest2019-zhen-src.zh.sgm\", \"newstest2019-zhen-ref.en.sgm\"),\n",
    "    (\"newstest2018-zhen-src.zh.sgm\", \"newstest2018-zhen-ref.en.sgm\"),\n",
    "    (\"newstest2017-zhen-src.zh.sgm\", \"newstest2017-zhen-ref.en.sgm\"),\n",
    "    (\"newstestB2020-zhen-src.zh.sgm\", \"newstestB2020-zhen-ref.en.sgm\"),\n",
    "]\n",
    "\n",
    "all_test_data = []\n",
    "test_dir = raw_dir / \"newstest\"\n",
    "\n",
    "for zh_file, en_file in test_files:\n",
    "    zh_path = test_dir / zh_file\n",
    "    en_path = test_dir / en_file\n",
    "    \n",
    "    if zh_path.exists() and en_path.exists():\n",
    "        zh_segments = read_sgm_file(zh_path)\n",
    "        en_segments = read_sgm_file(en_path)\n",
    "        \n",
    "        print(f\"{zh_file.split('-')[0]}: {len(zh_segments)} pairs\")\n",
    "        \n",
    "        for zh, en in zip(zh_segments, en_segments):\n",
    "            all_test_data.append({'source_zh': zh, 'target_en': en})\n",
    "\n",
    "test_df = pd.DataFrame(all_test_data)\n",
    "print(f\"\\nTotal test pairs: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed810a5a-11fe-482d-8ce0-b312e40d0555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After word length filtering (10-40): 21\n",
      "After deduplication: 21\n"
     ]
    }
   ],
   "source": [
    "test_df['source_words'] = test_df['source_zh'].str.split().str.len()\n",
    "test_df['target_words'] = test_df['target_en'].str.split().str.len()\n",
    "\n",
    "test_filtered = test_df[\n",
    "    (test_df['source_words'] >= 10) & \n",
    "    (test_df['source_words'] <= 40) &\n",
    "    (test_df['target_words'] >= 10) & \n",
    "    (test_df['target_words'] <= 40)\n",
    "].copy()\n",
    "\n",
    "test_filtered = test_filtered.drop(['source_words', 'target_words'], axis=1)\n",
    "test_clean = test_filtered.drop_duplicates(subset=['source_zh', 'target_en'])\n",
    "\n",
    "print(f\"After word length filtering (10-40): {len(test_filtered):,}\")\n",
    "print(f\"After deduplication: {len(test_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c8566ee-938a-4a86-9350-74156c657ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data word count statistics:\n",
      "        source_words   target_words\n",
      "count  312268.000000  312268.000000\n",
      "mean        1.794904      22.057806\n",
      "std         1.955388      10.796307\n",
      "min         1.000000       1.000000\n",
      "25%         1.000000      14.000000\n",
      "50%         1.000000      21.000000\n",
      "75%         1.000000      29.000000\n",
      "max        70.000000     153.000000\n",
      "\n",
      "Test data word count statistics:\n",
      "       source_words  target_words\n",
      "count   9982.000000   9982.000000\n",
      "mean       1.575235     28.473552\n",
      "std        1.598798     17.519407\n",
      "min        1.000000      1.000000\n",
      "25%        1.000000     17.000000\n",
      "50%        1.000000     25.000000\n",
      "75%        1.000000     36.000000\n",
      "max       30.000000    217.000000\n",
      "\n",
      "Train - samples by word count ranges:\n",
      "  5-30 words: 11,272\n",
      "  5-40 words: 18,701\n",
      "  5-50 words: 22,430\n",
      "  10-40 words: 1,610\n",
      "  10-50 words: 2,201\n",
      "\n",
      "Test - samples by word count ranges:\n",
      "  5-30 words: 210\n",
      "  5-40 words: 329\n",
      "  5-50 words: 402\n",
      "  10-40 words: 21\n",
      "  10-50 words: 33\n"
     ]
    }
   ],
   "source": [
    "# Reset to original data\n",
    "train_df_orig = pd.DataFrame({\n",
    "    'source_zh': source_texts,\n",
    "    'target_en': target_texts\n",
    "})\n",
    "\n",
    "test_df_orig = pd.DataFrame(all_test_data)\n",
    "\n",
    "# Calculate word counts\n",
    "train_df_orig['source_words'] = train_df_orig['source_zh'].str.split().str.len()\n",
    "train_df_orig['target_words'] = train_df_orig['target_en'].str.split().str.len()\n",
    "\n",
    "test_df_orig['source_words'] = test_df_orig['source_zh'].str.split().str.len()\n",
    "test_df_orig['target_words'] = test_df_orig['target_en'].str.split().str.len()\n",
    "\n",
    "print(\"Training data word count statistics:\")\n",
    "print(train_df_orig[['source_words', 'target_words']].describe())\n",
    "\n",
    "print(\"\\nTest data word count statistics:\")\n",
    "print(test_df_orig[['source_words', 'target_words']].describe())\n",
    "\n",
    "print(\"\\nTrain - samples by word count ranges:\")\n",
    "for min_w, max_w in [(5, 30), (5, 40), (5, 50), (10, 40), (10, 50)]:\n",
    "    count = len(train_df_orig[\n",
    "        (train_df_orig['source_words'] >= min_w) & \n",
    "        (train_df_orig['source_words'] <= max_w) &\n",
    "        (train_df_orig['target_words'] >= min_w) & \n",
    "        (train_df_orig['target_words'] <= max_w)\n",
    "    ])\n",
    "    print(f\"  {min_w}-{max_w} words: {count:,}\")\n",
    "\n",
    "print(\"\\nTest - samples by word count ranges:\")\n",
    "for min_w, max_w in [(5, 30), (5, 40), (5, 50), (10, 40), (10, 50)]:\n",
    "    count = len(test_df_orig[\n",
    "        (test_df_orig['source_words'] >= min_w) & \n",
    "        (test_df_orig['source_words'] <= max_w) &\n",
    "        (test_df_orig['target_words'] >= min_w) & \n",
    "        (test_df_orig['target_words'] <= max_w)\n",
    "    ])\n",
    "    print(f\"  {min_w}-{max_w} words: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9fbdf0-929d-4350-9563-22a4d7305884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data character count statistics:\n",
      "        source_chars   target_chars\n",
      "count  312268.000000  312268.000000\n",
      "mean       42.643838     138.316712\n",
      "std        22.706085      68.917720\n",
      "min         2.000000       2.000000\n",
      "25%        26.000000      88.000000\n",
      "50%        39.000000     131.000000\n",
      "75%        55.000000     181.000000\n",
      "max       518.000000    1022.000000\n",
      "\n",
      "Test data character count statistics:\n",
      "       source_chars  target_chars\n",
      "count   9982.000000   9982.000000\n",
      "mean      45.761571    177.135143\n",
      "std       26.966578    114.148919\n",
      "min        1.000000      1.000000\n",
      "25%       28.000000    102.000000\n",
      "50%       41.000000    155.000000\n",
      "75%       57.000000    222.000000\n",
      "max      446.000000   1427.000000\n",
      "\n",
      "Train - samples by character count ranges:\n",
      "  50-200 chars: 47,210\n",
      "  50-250 chars: 80,892\n",
      "  100-250 chars: 1,498\n",
      "  100-300 chars: 3,182\n",
      "\n",
      "Test - samples by character count ranges:\n",
      "  50-200 chars: 915\n",
      "  50-250 chars: 1,783\n",
      "  100-250 chars: 26\n",
      "  100-300 chars: 46\n"
     ]
    }
   ],
   "source": [
    "train_df_orig['source_chars'] = train_df_orig['source_zh'].str.len()\n",
    "train_df_orig['target_chars'] = train_df_orig['target_en'].str.len()\n",
    "\n",
    "test_df_orig['source_chars'] = test_df_orig['source_zh'].str.len()\n",
    "test_df_orig['target_chars'] = test_df_orig['target_en'].str.len()\n",
    "\n",
    "print(\"Training data character count statistics:\")\n",
    "print(train_df_orig[['source_chars', 'target_chars']].describe())\n",
    "\n",
    "print(\"\\nTest data character count statistics:\")\n",
    "print(test_df_orig[['source_chars', 'target_chars']].describe())\n",
    "\n",
    "print(\"\\nTrain - samples by character count ranges:\")\n",
    "for min_c, max_c in [(50, 200), (50, 250), (100, 250), (100, 300)]:\n",
    "    count = len(train_df_orig[\n",
    "        (train_df_orig['source_chars'] >= min_c) & \n",
    "        (train_df_orig['source_chars'] <= max_c) &\n",
    "        (train_df_orig['target_chars'] >= min_c) & \n",
    "        (train_df_orig['target_chars'] <= max_c)\n",
    "    ])\n",
    "    print(f\"  {min_c}-{max_c} chars: {count:,}\")\n",
    "\n",
    "print(\"\\nTest - samples by character count ranges:\")\n",
    "for min_c, max_c in [(50, 200), (50, 250), (100, 250), (100, 300)]:\n",
    "    count = len(test_df_orig[\n",
    "        (test_df_orig['source_chars'] >= min_c) & \n",
    "        (test_df_orig['source_chars'] <= max_c) &\n",
    "        (test_df_orig['target_chars'] >= min_c) & \n",
    "        (test_df_orig['target_chars'] <= max_c)\n",
    "    ])\n",
    "    print(f\"  {min_c}-{max_c} chars: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cca42f0-cdbb-4227-a228-60b5f421fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "  After filtering (50-250 chars): 80,892\n",
      "  After deduplication: 80,834\n",
      "\n",
      "Test data:\n",
      "  After filtering (50-250 chars): 1,783\n",
      "  After deduplication: 1,783\n"
     ]
    }
   ],
   "source": [
    "train_filtered = train_df_orig[\n",
    "    (train_df_orig['source_chars'] >= 50) & \n",
    "    (train_df_orig['source_chars'] <= 250) &\n",
    "    (train_df_orig['target_chars'] >= 50) & \n",
    "    (train_df_orig['target_chars'] <= 250)\n",
    "].copy()\n",
    "\n",
    "train_filtered = train_filtered[['source_zh', 'target_en']]\n",
    "train_clean = train_filtered.drop_duplicates(subset=['source_zh', 'target_en'])\n",
    "\n",
    "test_filtered = test_df_orig[\n",
    "    (test_df_orig['source_chars'] >= 50) & \n",
    "    (test_df_orig['source_chars'] <= 250) &\n",
    "    (test_df_orig['target_chars'] >= 50) & \n",
    "    (test_df_orig['target_chars'] <= 250)\n",
    "].copy()\n",
    "\n",
    "test_filtered = test_filtered[['source_zh', 'target_en']]\n",
    "test_clean = test_filtered.drop_duplicates(subset=['source_zh', 'target_en'])\n",
    "\n",
    "print(f\"Training data:\")\n",
    "print(f\"  After filtering (50-250 chars): {len(train_filtered):,}\")\n",
    "print(f\"  After deduplication: {len(train_clean):,}\")\n",
    "\n",
    "print(f\"\\nTest data:\")\n",
    "print(f\"  After filtering (50-250 chars): {len(test_filtered):,}\")\n",
    "print(f\"  After deduplication: {len(test_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc2623-ce8e-43d1-a47e-b39d57a3dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No existing contamination sources found\n"
     ]
    }
   ],
   "source": [
    "# if there are any zh_en contamination files\n",
    "contamination_paths = [\n",
    "    base_dir / \"isolated_clean\" / \"zh_en\",\n",
    "    base_dir / \"error_dataset\" / \"zh_en\",\n",
    "    base_dir / \"processed\" / \"zh_en\",\n",
    "]\n",
    "\n",
    "existing_files = []\n",
    "for path in contamination_paths:\n",
    "    if path.exists():\n",
    "        for file in path.glob(\"*.tsv\"):\n",
    "            existing_files.append(file)\n",
    "            print(f\"  Found: {file}\")\n",
    "        for file in path.glob(\"*.xlsx\"):\n",
    "            existing_files.append(file)\n",
    "            print(f\"  Found: {file}\")\n",
    "\n",
    "if not existing_files:\n",
    "    print(\"  No existing contamination sources found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad62287a-a888-4b6a-a851-27b82f5989c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split from training:\n",
      "  Training pool: 50,000\n",
      "  Test pool: 5,000\n",
      "  Dev pool: 2,000\n",
      "  Reserve pool: 23,834\n",
      "  Total: 80,834\n"
     ]
    }
   ],
   "source": [
    "train_shuffled = train_clean.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_pool = train_shuffled.iloc[:50000].copy()\n",
    "test_pool = train_shuffled.iloc[50000:55000].copy()\n",
    "dev_pool = train_shuffled.iloc[55000:57000].copy()\n",
    "reserve_pool = train_shuffled.iloc[57000:].copy()\n",
    "\n",
    "print(f\"Data split from training:\")\n",
    "print(f\"  Training pool: {len(train_pool):,}\")\n",
    "print(f\"  Test pool: {len(test_pool):,}\")\n",
    "print(f\"  Dev pool: {len(dev_pool):,}\")\n",
    "print(f\"  Reserve pool: {len(reserve_pool):,}\")\n",
    "print(f\"  Total: {len(train_pool) + len(test_pool) + len(dev_pool) + len(reserve_pool):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e921894-6e38-48a0-b399-64da371233c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train_1000.tsv\n",
      "Saved train_5000.tsv\n",
      "Saved train_10000.tsv\n",
      "Saved train_15000.tsv\n",
      "Saved train_20000.tsv\n",
      "Saved train_30000.tsv\n",
      "Saved train_40000.tsv\n",
      "Saved train_50000.tsv\n",
      "Saved train_full.tsv: 50,000 samples\n"
     ]
    }
   ],
   "source": [
    "train_sizes = [1000, 5000, 10000, 15000, 20000, 30000, 40000, 50000]\n",
    "\n",
    "for size in train_sizes:\n",
    "    if size <= len(train_pool):\n",
    "        train_subset = train_pool.sample(n=size, random_state=42)\n",
    "        output_path = processed_dir / f\"train_{size}.tsv\"\n",
    "        train_subset.to_csv(output_path, sep='\\t', index=False)\n",
    "        print(f\"Saved train_{size}.tsv\")\n",
    "\n",
    "output_path = processed_dir / \"train_full.tsv\"\n",
    "train_pool.to_csv(output_path, sep='\\t', index=False)\n",
    "print(f\"Saved train_full.tsv: {len(train_pool):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3219756-5ea5-424c-a96c-1df3b3524dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test_500_clean.tsv\n",
      "Saved test_1000_clean.tsv\n",
      "Saved test_2000_clean.tsv\n",
      "Saved test_5000_clean.tsv\n",
      "Saved dev_2000.tsv: 2,000 samples\n"
     ]
    }
   ],
   "source": [
    "test_sizes = [500, 1000, 2000, 5000]\n",
    "\n",
    "for size in test_sizes:\n",
    "    if size <= len(test_pool):\n",
    "        test_subset = test_pool.sample(n=size, random_state=42)\n",
    "        output_path = eval_dir / f\"test_{size}_clean.tsv\"\n",
    "        test_subset.to_csv(output_path, sep='\\t', index=False)\n",
    "        print(f\"Saved test_{size}_clean.tsv\")\n",
    "\n",
    "dev_output = processed_dir / \"dev_2000.tsv\"\n",
    "dev_pool.to_csv(dev_output, sep='\\t', index=False)\n",
    "print(f\"Saved dev_2000.tsv: {len(dev_pool):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c529c78-e095-4d39-90e9-8c6120cbd8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reserve_pool.tsv: 23,834 samples\n"
     ]
    }
   ],
   "source": [
    "reserve_output = reserve_dir / \"reserve_pool.tsv\"\n",
    "reserve_pool.to_csv(reserve_output, sep='\\t', index=False)\n",
    "print(f\"Saved reserve_pool.tsv: {len(reserve_pool):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "424cd0ed-8a18-40f3-b7e5-6f853bd991f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved error_dataset_pool_10000.tsv: 10,000 samples\n",
      "Saved remaining_reserve.tsv: 13,834 samples\n",
      "Removed old reserve_pool.tsv\n"
     ]
    }
   ],
   "source": [
    "reserve_shuffled = reserve_pool.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "error_dataset_pool = reserve_shuffled.iloc[:10000].copy()\n",
    "remaining_reserve = reserve_shuffled.iloc[10000:].copy()\n",
    "\n",
    "error_output = reserve_dir / \"error_dataset_pool_10000.tsv\"\n",
    "error_dataset_pool.to_csv(error_output, sep='\\t', index=False)\n",
    "print(f\"Saved error_dataset_pool_10000.tsv: {len(error_dataset_pool):,} samples\")\n",
    "\n",
    "reserve_output = reserve_dir / \"remaining_reserve.tsv\"\n",
    "remaining_reserve.to_csv(reserve_output, sep='\\t', index=False)\n",
    "print(f\"Saved remaining_reserve.tsv: {len(remaining_reserve):,} samples\")\n",
    "\n",
    "# Remove the old reserve_pool.tsv\n",
    "old_reserve = reserve_dir / \"reserve_pool.tsv\"\n",
    "if old_reserve.exists():\n",
    "    old_reserve.unlink()\n",
    "    print(\"Removed old reserve_pool.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b4198d-922b-4dd4-bfec-e23166b1e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification - checking for overlaps:\n",
      "Train vs Test: 0 overlaps\n",
      "Train vs Dev: 0 overlaps\n",
      "Train vs Error pool: 0 overlaps\n",
      "Train vs Reserve: 0 overlaps\n",
      "Test vs Dev: 0 overlaps\n",
      "Test vs Error pool: 0 overlaps\n",
      "Test vs Reserve: 0 overlaps\n",
      "Dev vs Error pool: 0 overlaps\n",
      "Dev vs Reserve: 0 overlaps\n",
      "Error pool vs Reserve: 0 overlaps\n",
      "\n",
      "No contamination detected\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification - checking for overlaps:\")\n",
    "\n",
    "train_sample = pd.read_csv(processed_dir / \"train_5000.tsv\", delimiter='\\t')\n",
    "test_sample = pd.read_csv(eval_dir / \"test_2000_clean.tsv\", delimiter='\\t')\n",
    "dev_sample = pd.read_csv(processed_dir / \"dev_2000.tsv\", delimiter='\\t')\n",
    "error_sample = pd.read_csv(reserve_dir / \"error_dataset_pool_10000.tsv\", delimiter='\\t')\n",
    "reserve_sample = pd.read_csv(reserve_dir / \"remaining_reserve.tsv\", delimiter='\\t')\n",
    "\n",
    "train_norm = set(train_sample['source_zh'].str.strip().str.lower())\n",
    "test_norm = set(test_sample['source_zh'].str.strip().str.lower())\n",
    "dev_norm = set(dev_sample['source_zh'].str.strip().str.lower())\n",
    "error_norm = set(error_sample['source_zh'].str.strip().str.lower())\n",
    "reserve_norm = set(reserve_sample['source_zh'].str.strip().str.lower())\n",
    "\n",
    "print(f\"Train vs Test: {len(train_norm & test_norm)} overlaps\")\n",
    "print(f\"Train vs Dev: {len(train_norm & dev_norm)} overlaps\")\n",
    "print(f\"Train vs Error pool: {len(train_norm & error_norm)} overlaps\")\n",
    "print(f\"Train vs Reserve: {len(train_norm & reserve_norm)} overlaps\")\n",
    "print(f\"Test vs Dev: {len(test_norm & dev_norm)} overlaps\")\n",
    "print(f\"Test vs Error pool: {len(test_norm & error_norm)} overlaps\")\n",
    "print(f\"Test vs Reserve: {len(test_norm & reserve_norm)} overlaps\")\n",
    "print(f\"Dev vs Error pool: {len(dev_norm & error_norm)} overlaps\")\n",
    "print(f\"Dev vs Reserve: {len(dev_norm & reserve_norm)} overlaps\")\n",
    "print(f\"Error pool vs Reserve: {len(error_norm & reserve_norm)} overlaps\")\n",
    "\n",
    "total_overlaps = (\n",
    "    len(train_norm & test_norm) + len(train_norm & dev_norm) + \n",
    "    len(train_norm & error_norm) + len(train_norm & reserve_norm) +\n",
    "    len(test_norm & dev_norm) + len(test_norm & error_norm) + \n",
    "    len(test_norm & reserve_norm) + len(dev_norm & error_norm) +\n",
    "    len(dev_norm & reserve_norm) + len(error_norm & reserve_norm)\n",
    ")\n",
    "\n",
    "if total_overlaps == 0:\n",
    "    print(\"\\nNo contamination detected\")\n",
    "else:\n",
    "    print(f\"\\nTotal overlaps detected: {total_overlaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49d99ddd-6cbb-4b3d-839e-da400bd02bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed directory (training sets):\n",
      "  dev_2000.tsv: 2,000 samples\n",
      "  train_1000.tsv: 1,000 samples\n",
      "  train_10000.tsv: 10,000 samples\n",
      "  train_15000.tsv: 15,000 samples\n",
      "  train_20000.tsv: 20,000 samples\n",
      "  train_30000.tsv: 30,000 samples\n",
      "  train_40000.tsv: 40,000 samples\n",
      "  train_5000.tsv: 5,000 samples\n",
      "  train_50000.tsv: 50,000 samples\n",
      "  train_full.tsv: 50,000 samples\n",
      "\n",
      "Evaluation directory (test sets):\n",
      "  test_1000_clean.tsv: 1,000 samples\n",
      "  test_2000_clean.tsv: 2,000 samples\n",
      "  test_5000_clean.tsv: 5,000 samples\n",
      "  test_500_clean.tsv: 500 samples\n",
      "\n",
      "Reserve directory:\n",
      "  error_dataset_pool_10000.tsv: 10,000 samples\n",
      "  remaining_reserve.tsv: 13,834 samples\n",
      "\n",
      "Total samples used: 80,834\n",
      "Character range: 50-250 characters (both source and target)\n",
      "Data source: WMT News Commentary v15\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessed directory (training sets):\")\n",
    "for file in sorted(processed_dir.glob(\"*.tsv\")):\n",
    "    size = len(pd.read_csv(file, delimiter='\\t'))\n",
    "    print(f\"  {file.name}: {size:,} samples\")\n",
    "\n",
    "print(\"\\nEvaluation directory (test sets):\")\n",
    "for file in sorted(eval_dir.glob(\"*.tsv\")):\n",
    "    size = len(pd.read_csv(file, delimiter='\\t'))\n",
    "    print(f\"  {file.name}: {size:,} samples\")\n",
    "\n",
    "print(\"\\nReserve directory:\")\n",
    "for file in sorted(reserve_dir.glob(\"*.tsv\")):\n",
    "    size = len(pd.read_csv(file, delimiter='\\t'))\n",
    "    print(f\"  {file.name}: {size:,} samples\")\n",
    "\n",
    "print(\"\\nTotal samples used: 80,834\")\n",
    "print(\"Character range: 50-250 characters (both source and target)\")\n",
    "print(\"Data source: WMT News Commentary v15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1053f-df5f-4982-b081-7fe1bf237f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (User + Packages)",
   "language": "python",
   "name": "user-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
