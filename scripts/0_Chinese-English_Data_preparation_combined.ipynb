{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940288d7-036b-4f28-88ec-7336a16cf4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270b490-bd8f-4c13-8f19-6eee93079c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "  Train News: ../../data/processed/zh_en/train_50000.tsv\n",
      "  Train UN:   ../../data/processed/zh_en/train_un_version_one_15000.tsv\n",
      "  Dev News:   ../../data/processed/zh_en/dev_2000.tsv\n",
      "  Dev UN:     ../../data/processed/zh_en/dev_un_version_one_2000.tsv\n",
      "\n",
      "Output files:\n",
      "  Train Combined: ../../data/processed/zh_en/train_news_un_balanced_30000.tsv\n",
      "  Dev Combined:   ../../data/processed/zh_en/mix2k_dev.tsv\n"
     ]
    }
   ],
   "source": [
    "# base directory\n",
    "DATA_DIR = Path(\"../../data/processed/zh_en/\")\n",
    "\n",
    "TRAIN_NEWS = DATA_DIR / \"train_50000.tsv\"\n",
    "TRAIN_UN = DATA_DIR / \"train_un_version_one_15000.tsv\"\n",
    "DEV_NEWS = DATA_DIR / \"dev_2000.tsv\"\n",
    "DEV_UN = DATA_DIR / \"dev_un_version_one_2000.tsv\"\n",
    "\n",
    "TRAIN_COMBINED = DATA_DIR / \"train_news_un_balanced_30000.tsv\"\n",
    "DEV_COMBINED = DATA_DIR / \"mix2k_dev.tsv\"\n",
    "\n",
    "print(\"Input files:\")\n",
    "print(f\"  Train News: {TRAIN_NEWS}\")\n",
    "print(f\"  Train UN:   {TRAIN_UN}\")\n",
    "print(f\"  Dev News:   {DEV_NEWS}\")\n",
    "print(f\"  Dev UN:     {DEV_UN}\")\n",
    "print(\"\\nOutput files:\")\n",
    "print(f\"  Train Combined: {TRAIN_COMBINED}\")\n",
    "print(f\"  Dev Combined:   {DEV_COMBINED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faa0c9-3b90-4381-a268-52af7d3f7089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics\n",
      "\n",
      "Train NewsCommentary: 50,000 samples\n",
      "Columns: ['source_zh', 'target_en']\n",
      "\n",
      "Train UN Parallel:    15,000 samples\n",
      "Columns: ['source_zh', 'target_en']\n",
      "\n",
      "Dev NewsCommentary:   2,000 samples\n",
      "Columns: ['source_zh', 'target_en']\n",
      "\n",
      "Dev UN Parallel:      2,000 samples\n",
      "Columns: ['source_zh', 'target_en']\n"
     ]
    }
   ],
   "source": [
    "# load all datasets\n",
    "train_news_df = pd.read_csv(TRAIN_NEWS, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
    "train_un_df = pd.read_csv(TRAIN_UN, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
    "dev_news_df = pd.read_csv(DEV_NEWS, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
    "dev_un_df = pd.read_csv(DEV_UN, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
    "\n",
    "print(\"Dataset Statistics\")\n",
    "print(f\"\\nTrain NewsCommentary: {len(train_news_df):,} samples\")\n",
    "print(f\"Columns: {train_news_df.columns.tolist()}\")\n",
    "print(f\"\\nTrain UN Parallel:    {len(train_un_df):,} samples\")\n",
    "print(f\"Columns: {train_un_df.columns.tolist()}\")\n",
    "print(f\"\\nDev NewsCommentary:   {len(dev_news_df):,} samples\")\n",
    "print(f\"Columns: {dev_news_df.columns.tolist()}\")\n",
    "print(f\"\\nDev UN Parallel:      {len(dev_un_df):,} samples\")\n",
    "print(f\"Columns: {dev_un_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfa4be4-1039-4510-8705-da1b7aa4cd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data\n",
      "\n",
      "[Train NewsCommentary - First Sample]\n",
      "Chinese: 最鲜为人知的或许是欧洲大型强子对撞机中用于加速粒子的巨型磁体，而该设备则被利用来探索物质的基本原理。...\n",
      "English: Perhaps the most exotic are the huge magnets used to accelerate particles in the Large Hadron Collid...\n",
      "\n",
      "[Train UN Parallel - First Sample]\n",
      "Chinese: 1. Value and Process of National Dialogue, al-Falaq e-journal, January 2010....\n",
      "English: 1. Value and process of national dialogue, al-Falaq e-journal, January 2010...\n",
      "\n",
      "[Dev NewsCommentary - First Sample]\n",
      "Chinese: 但是，另一个事实是强化技术训练并不能提供解决更抽象但极其重要的问题的充分基础。 这些问题最终将指导全球政策和决策。...\n",
      "English: It is also true, however, that such training does not provide an adequate foundation for addressing ...\n",
      "\n",
      "[Dev UN Parallel - First Sample]\n",
      "Chinese: 登记发言请求应发给NancyBeteta女士(传真:1(212)963-5935;电子邮件:beteta@un.org)。...\n",
      "English: Requests for inscription should be sent to Ms. Nancy Beteta (fax 1 (212) 963-5935; e-mail beteta@un....\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Data\")\n",
    "\n",
    "print(\"\\n[Train NewsCommentary - First Sample]\")\n",
    "print(f\"Chinese: {train_news_df['source_zh'].iloc[0][:100]}...\")\n",
    "print(f\"English: {train_news_df['target_en'].iloc[0][:100]}...\")\n",
    "\n",
    "print(\"\\n[Train UN Parallel - First Sample]\")\n",
    "print(f\"Chinese: {train_un_df['source_zh'].iloc[0][:100]}...\")\n",
    "print(f\"English: {train_un_df['target_en'].iloc[0][:100]}...\")\n",
    "\n",
    "print(\"\\n[Dev NewsCommentary - First Sample]\")\n",
    "print(f\"Chinese: {dev_news_df['source_zh'].iloc[0][:100]}...\")\n",
    "print(f\"English: {dev_news_df['target_en'].iloc[0][:100]}...\")\n",
    "\n",
    "print(\"\\n[Dev UN Parallel - First Sample]\")\n",
    "print(f\"Chinese: {dev_un_df['source_zh'].iloc[0][:100]}...\")\n",
    "print(f\"English: {dev_un_df['target_en'].iloc[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2095b44-0759-42a0-85c4-bb971d883550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Training Set 30k\n",
      "Sampled 15,000 from NewsCommentary\n",
      "Sampled 15,000 from UN Parallel\n",
      "\n",
      " Combined training set: 30,000 samples\n",
      "\n",
      "Source distribution:\n",
      "source_corpus\n",
      "newscommentary    15000\n",
      "un_parallel       15000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "samples_per_source = 15000\n",
    "print(\"Combined Training Set 30k\")\n",
    "\n",
    "if len(train_news_df) >= samples_per_source:\n",
    "    train_news_sampled = train_news_df.sample(n=samples_per_source, random_state=42)\n",
    "    print(f\"Sampled {samples_per_source:,} from NewsCommentary\")\n",
    "else:\n",
    "    train_news_sampled = train_news_df\n",
    "    print(f\"NewsCommentary has only {len(train_news_df):,} samples (wanted {samples_per_source:,})\")\n",
    "\n",
    "# Sample from UN Parallel\n",
    "if len(train_un_df) >= samples_per_source:\n",
    "    train_un_sampled = train_un_df.sample(n=samples_per_source, random_state=42)\n",
    "    print(f\"Sampled {samples_per_source:,} from UN Parallel\")\n",
    "else:\n",
    "    train_un_sampled = train_un_df\n",
    "    print(f\"UN Parallel has only {len(train_un_df):,} samples (wanted {samples_per_source:,})\")\n",
    "\n",
    "# source labels for tracking\n",
    "train_news_sampled = train_news_sampled.copy()\n",
    "train_news_sampled['source_corpus'] = 'newscommentary'\n",
    "\n",
    "train_un_sampled = train_un_sampled.copy()\n",
    "train_un_sampled['source_corpus'] = 'un_parallel'\n",
    "\n",
    "# Combine\n",
    "train_combined = pd.concat([train_news_sampled, train_un_sampled], ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "train_combined = train_combined.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n Combined training set: {len(train_combined):,} samples\")\n",
    "print(\"\\nSource distribution:\")\n",
    "print(train_combined['source_corpus'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0955dd7-585c-4ff9-843f-807364b73491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Dev Set mix2k\n",
      " Sampled 1,000 from NewsCommentary dev\n",
      " Sampled 1,000 from UN Parallel dev\n",
      "\n",
      " Combined dev set: 2,000 samples\n",
      "\n",
      "Source distribution:\n",
      "source_corpus\n",
      "un_parallel       1000\n",
      "newscommentary    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dev_samples_per_source = 1000\n",
    "print(\"Combined Dev Set mix2k\")\n",
    "\n",
    "# Sample from NewsCommentary dev\n",
    "if len(dev_news_df) >= dev_samples_per_source:\n",
    "    dev_news_sampled = dev_news_df.sample(n=dev_samples_per_source, random_state=42)\n",
    "    print(f\" Sampled {dev_samples_per_source:,} from NewsCommentary dev\")\n",
    "else:\n",
    "    dev_news_sampled = dev_news_df\n",
    "    print(f\" NewsCommentary dev has only {len(dev_news_df):,} samples (wanted {dev_samples_per_source:,})\")\n",
    "\n",
    "# Sample from UN Parallel dev\n",
    "if len(dev_un_df) >= dev_samples_per_source:\n",
    "    dev_un_sampled = dev_un_df.sample(n=dev_samples_per_source, random_state=42)\n",
    "    print(f\" Sampled {dev_samples_per_source:,} from UN Parallel dev\")\n",
    "else:\n",
    "    dev_un_sampled = dev_un_df\n",
    "    print(f\" UN Parallel dev has only {len(dev_un_df):,} samples (wanted {dev_samples_per_source:,})\")\n",
    "\n",
    "dev_news_sampled = dev_news_sampled.copy()\n",
    "dev_news_sampled['source_corpus'] = 'newscommentary'\n",
    "\n",
    "dev_un_sampled = dev_un_sampled.copy()\n",
    "dev_un_sampled['source_corpus'] = 'un_parallel'\n",
    "\n",
    "dev_combined = pd.concat([dev_news_sampled, dev_un_sampled], ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "dev_combined = dev_combined.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n Combined dev set: {len(dev_combined):,} samples\")\n",
    "print(\"\\nSource distribution:\")\n",
    "print(dev_combined['source_corpus'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab9352-594d-4426-809c-2d3b7c48ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved training set: ../../data/processed/zh_en/train_news_un_balanced_30000.tsv\n",
      "  Size: 30,000 samples\n",
      " Saved dev set: ../../data/processed/zh_en/mix2k_dev.tsv\n",
      "  Size: 2,000 samples\n"
     ]
    }
   ],
   "source": [
    "# training set (without source_corpus column)\n",
    "train_output = train_combined[['source_zh', 'target_en']]\n",
    "train_output.to_csv(TRAIN_COMBINED, sep=\"\\t\", index=False)\n",
    "print(f\" Saved training set: {TRAIN_COMBINED}\")\n",
    "print(f\"  Size: {len(train_output):,} samples\")\n",
    "\n",
    "# dev set (without source_corpus column)\n",
    "dev_output = dev_combined[['source_zh', 'target_en']]\n",
    "dev_output.to_csv(DEV_COMBINED, sep=\"\\t\", index=False)\n",
    "print(f\" Saved dev set: {DEV_COMBINED}\")\n",
    "print(f\"  Size: {len(dev_output):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1abe5509-a4d6-4966-8195-acd82ecfd11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train file loaded: 30,000 samples\n",
      "  Columns: ['source_zh', 'target_en']\n",
      "  First row Chinese: 当一国年轻人身体健康、受到良好教育时，他们就能找到高薪工作，赢得尊严并成功地调整渡过全球劳动力市场的波动期。...\n",
      "  First row English: When its young people are healthy and well educated, they can find gainful emplo...\n",
      "\n",
      " Dev file loaded: 2,000 samples\n",
      "  Columns: ['source_zh', 'target_en']\n",
      "  First row Chinese: 126. 截止2013年10月1日,哈萨克斯坦居民总人数为17,098,546名,其中8,845,067人为妇女(占51.8%)。...\n",
      "  First row English: 126. As of 1 October 2013, women numbered 8,845,067 (51.8 per cent) in the total...\n",
      "datasets created\n",
      "train_tsv ../../data/processed/zh_en/train_news_un_balanced_30000.tsv\n",
      "dev_tsv ../../data/processed/zh_en/mix2k_dev.tsv\n"
     ]
    }
   ],
   "source": [
    "train_verify = pd.read_csv(TRAIN_COMBINED, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
    "dev_verify = pd.read_csv(DEV_COMBINED, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
    "\n",
    "print(f\"\\n Train file loaded: {len(train_verify):,} samples\")\n",
    "print(f\"  Columns: {train_verify.columns.tolist()}\")\n",
    "print(f\"  First row Chinese: {train_verify['source_zh'].iloc[0][:80]}...\")\n",
    "print(f\"  First row English: {train_verify['target_en'].iloc[0][:80]}...\")\n",
    "\n",
    "print(f\"\\n Dev file loaded: {len(dev_verify):,} samples\")\n",
    "print(f\"  Columns: {dev_verify.columns.tolist()}\")\n",
    "print(f\"  First row Chinese: {dev_verify['source_zh'].iloc[0][:80]}...\")\n",
    "print(f\"  First row English: {dev_verify['target_en'].iloc[0][:80]}...\")\n",
    "\n",
    "print(\"datasets created\")\n",
    "\n",
    "print(f\"train_tsv {TRAIN_COMBINED}\")\n",
    "print(f\"dev_tsv {DEV_COMBINED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9892517-22e1-43f2-8363-e784e0e5c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "Training Set: ../../data/processed/zh_en/train_news_un_balanced_30000.tsv\n",
      "Total samples: 30,000\n",
      "NewsCommentary: 15,000\n",
      "UN Parallel: 15,000\n",
      "\n",
      "Dev Set: ../../data/processed/zh_en/mix2k_dev.tsv\n",
      "Total samples: 2,000\n",
      "NewsCommentary: 1,000\n",
      "UN Parallel: 1,000\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary\")\n",
    "print(f\"Training Set: {TRAIN_COMBINED}\")\n",
    "print(f\"Total samples: {len(train_combined):,}\")\n",
    "print(f\"NewsCommentary: {(train_combined['source_corpus'] == 'newscommentary').sum():,}\")\n",
    "print(f\"UN Parallel: {(train_combined['source_corpus'] == 'un_parallel').sum():,}\")\n",
    "\n",
    "print(f\"\\nDev Set: {DEV_COMBINED}\")\n",
    "print(f\"Total samples: {len(dev_combined):,}\")\n",
    "print(f\"NewsCommentary: {(dev_combined['source_corpus'] == 'newscommentary').sum():,}\")\n",
    "print(f\"UN Parallel: {(dev_combined['source_corpus'] == 'un_parallel').sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d022e0-dd98-4f65-bd8c-26f5ed9c79b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (User + Packages)",
   "language": "python",
   "name": "user-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
