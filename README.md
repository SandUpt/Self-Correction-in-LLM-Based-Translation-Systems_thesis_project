# Self-Correction in LLM-Based Translation Systems

Large language models are commonly used for machine translation and can generate fluent translations, yet their output still contains recurring problems such as omitting information, inappropriate lexical choices, and meaning shifts. Previous work has attempted to address these issues by prompting models to revise their own translations or by performing multiple generation steps at inference time.

This thesis evaluates a training-based approach to self-correction in LLM-based translation systems. Models are fine-tuned on examples of their own translation errors instead of relying on inference-time prompting. The training data includes two types of examples. For translations with errors, each example contains the source, the modelâ€™s output, an analysis explaining the problem, and the reference as the correction. To balance the dataset, correct sentence pairs from the parallel corpus are also included where the reference translation serves as both the initial and corrected translation, with the analysis simply stating that the translation accurately captures the meaning. Training is carried out in two stages. Models are first fine-tuned on standard parallel data to learn basic translation behaviour and are then further fine-tuned on the self-correction dataset using parameter-efficient LoRA adapters. Experiments focus primarily on German-English and Chinese-English translation using models such as LLaMA-2, LLaMA-3, Qwen 2.5, and Mistral.

Self-correction fine-tuning improved translation quality for some models. Qwen 2.5 on Chinese-English, for instance, went from 32.04 to 33.72 BLEU and from 0.842 to 0.862 COMET after training. Other models improved less, and in some cases got worse. Manual evaluation showed that corrected translations were generally better when the model had correctly identified an error in its initial output. Overall, the findings indicate that at least part of self-correction behaviour can be learned during training rather than relying on prompting at inference time. 
