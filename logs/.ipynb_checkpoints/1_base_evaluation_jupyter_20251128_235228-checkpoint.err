`torch_dtype` is deprecated! Use `dtype` instead!
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|                                                              | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|███████████████████████████                           | 1/2 [00:15<00:15, 15.84s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 2/2 [00:22<00:00, 10.41s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 2/2 [00:22<00:00, 11.22s/it]
Generating:   0%|                                                                            | 0/11 [00:00<?, ?it/s]Generating:   9%|██████▏                                                             | 1/11 [00:49<08:10, 49.04s/it]Generating:  18%|████████████▎                                                       | 2/11 [01:36<07:10, 47.88s/it]Generating:  27%|██████████████████▌                                                 | 3/11 [02:24<06:24, 48.12s/it]Generating:  36%|████████████████████████▋                                           | 4/11 [03:09<05:28, 46.86s/it]Generating:  45%|██████████████████████████████▉                                     | 5/11 [03:53<04:35, 45.96s/it]Generating:  55%|█████████████████████████████████████                               | 6/11 [04:40<03:51, 46.29s/it]Generating:  64%|███████████████████████████████████████████▎                        | 7/11 [05:24<03:01, 45.31s/it]Generating:  73%|█████████████████████████████████████████████████▍                  | 8/11 [06:10<02:16, 45.58s/it]Generating:  82%|███████████████████████████████████████████████████████▋            | 9/11 [06:55<01:31, 45.56s/it]Generating:  91%|████████████████████████████████████████████████████████████▉      | 10/11 [07:45<00:46, 46.95s/it]Generating: 100%|███████████████████████████████████████████████████████████████████| 11/11 [08:07<00:00, 39.10s/it]Generating: 100%|███████████████████████████████████████████████████████████████████| 11/11 [08:07<00:00, 44.28s/it]
Fetching 5 files:   0%|                                                                       | 0/5 [00:00<?, ?it/s]Fetching 5 files: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 8285.86it/s]
Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`
Encoder model frozen.
/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']
/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 1_base_evaluation.py ...
Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
