Using GPU inside JupyterLab session
stdout log: ../logs/6_self_correction_training_20251129_185220.log
stderr log: ../logs/6_self_correction_training_20251129_185220.err
Start Time: Sat Nov 29 18:52:20 CET 2025
Node: gx29
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sat Nov 29 18:52:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A40                     Off |   00000000:81:00.0 Off |                    0 |
|  0%   58C    P0            120W /  300W |     301MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         2111969      C   /usr/bin/python3                        292MiB |
+-----------------------------------------------------------------------------------------+
Self-Correction Fine-tuning
Model: llama3
Language Pair: zh_en
Using END token: False

Loading training data
  Train: ../../data/processed/zh_en/self_correction_zh_en/train.tsv
  Val: ../../data/processed/zh_en/self_correction_zh_en/val.tsv
  Train samples: 1400
  Val samples: 200

Loading WMT model: ../models/wmt_merged/llama3_zh_en

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.0628, 'grad_norm': 0.7322410941123962, 'learning_rate': 9.524135262330098e-05, 'epoch': 0.57}
{'loss': 0.8075, 'grad_norm': 0.6929689049720764, 'learning_rate': 7.408768370508576e-05, 'epoch': 1.14}
{'loss': 0.7081, 'grad_norm': 0.8255481719970703, 'learning_rate': 4.373333832178478e-05, 'epoch': 1.71}
{'loss': 0.6424, 'grad_norm': 0.9829105734825134, 'learning_rate': 1.5772644703565565e-05, 'epoch': 2.27}
{'eval_loss': 0.7792813777923584, 'eval_runtime': 13.0845, 'eval_samples_per_second': 15.285, 'eval_steps_per_second': 7.643, 'epoch': 2.27}
{'loss': 0.56, 'grad_norm': 1.1113131046295166, 'learning_rate': 8.856374635655695e-07, 'epoch': 2.85}
{'train_runtime': 980.0872, 'train_samples_per_second': 4.285, 'train_steps_per_second': 0.269, 'train_loss': 0.7455122904344038, 'epoch': 3.0}

Saving adapter to ../models/self_correction/llama3_zh_en

Training complete.
Adapter saved to: ../models/self_correction/llama3_zh_en
Done at: Sat Nov 29 19:09:35 CET 2025
