+ echo 'Job ID: 1458060'
+ echo 'Job Name: 6_self_correction_training'
+ echo 'Node: gx01'
++ date
+ echo 'Start Time: Sun Nov 30 12:15:50 CET 2025'
++ pwd
+ echo 'Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files'
+ nvidia-smi
+ cd /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/scripts/
+ export TOKENIZERS_PARALLELISM=false
+ TOKENIZERS_PARALLELISM=false
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
+ echo 6_self_correction_training
+ python3 6_self_correction_training.py
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.34s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.85s/it]
Tokenizing:   0%|          | 0/1288 [00:00<?, ? examples/s]Tokenizing:  78%|███████▊  | 1000/1288 [00:00<00:00, 1998.97 examples/s]Tokenizing: 100%|██████████| 1288/1288 [00:00<00:00, 1948.39 examples/s]Tokenizing: 100%|██████████| 1288/1288 [00:00<00:00, 1944.73 examples/s]
Tokenizing:   0%|          | 0/184 [00:00<?, ? examples/s]Tokenizing: 100%|██████████| 184/184 [00:00<00:00, 1799.28 examples/s]Tokenizing: 100%|██████████| 184/184 [00:00<00:00, 1774.55 examples/s]
The model is already on multiple devices. Skipping the move to device specified in `args`.
  0%|          | 0/243 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|          | 1/243 [00:02<10:37,  2.63s/it]  1%|          | 2/243 [00:04<08:46,  2.19s/it]  1%|          | 3/243 [00:06<08:09,  2.04s/it]  2%|▏         | 4/243 [00:08<07:50,  1.97s/it]  2%|▏         | 5/243 [00:10<07:38,  1.93s/it]  2%|▏         | 6/243 [00:11<07:30,  1.90s/it]  3%|▎         | 7/243 [00:13<07:25,  1.89s/it]  3%|▎         | 8/243 [00:15<07:22,  1.88s/it]  4%|▎         | 9/243 [00:17<07:22,  1.89s/it]  4%|▍         | 10/243 [00:19<07:21,  1.90s/it]  5%|▍         | 11/243 [00:21<07:17,  1.89s/it]  5%|▍         | 12/243 [00:23<07:14,  1.88s/it]  5%|▌         | 13/243 [00:25<07:14,  1.89s/it]  6%|▌         | 14/243 [00:26<07:12,  1.89s/it]  6%|▌         | 15/243 [00:28<07:12,  1.90s/it]  7%|▋         | 16/243 [00:30<07:13,  1.91s/it]  7%|▋         | 17/243 [00:32<07:12,  1.91s/it]  7%|▋         | 18/243 [00:34<07:09,  1.91s/it]  8%|▊         | 19/243 [00:36<07:07,  1.91s/it]  8%|▊         | 20/243 [00:38<07:07,  1.92s/it]  9%|▊         | 21/243 [00:40<07:01,  1.90s/it]  9%|▉         | 22/243 [00:42<06:58,  1.90s/it]  9%|▉         | 23/243 [00:44<06:58,  1.90s/it] 10%|▉         | 24/243 [00:46<06:55,  1.90s/it] 10%|█         | 25/243 [00:47<06:50,  1.88s/it] 11%|█         | 26/243 [00:49<06:47,  1.88s/it]