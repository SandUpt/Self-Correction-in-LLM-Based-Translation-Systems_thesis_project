Using GPU inside JupyterLab session
stdout log: ../logs/6_self_correction_training_20251129_144024.log
stderr log: ../logs/6_self_correction_training_20251129_144024.err
Start Time: Sat Nov 29 14:40:24 CET 2025
Node: gx04
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sat Nov 29 14:40:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:25:00.0 Off |                    0 |
| N/A   32C    P0             37W /  250W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Self-Correction Fine-tuning
Model: qwen
Language Pair: zh_en
Using END token: True

Loading training data
  Train: ../../data/processed/zh_en/self_correction_zh_en_end/train.tsv
  Val: ../../data/processed/zh_en/self_correction_zh_en_end/val.tsv
  Train samples: 1400
  Val samples: 200

Loading WMT model: ../models/wmt_merged/qwen_zh_en

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 10,092,544 || all params: 7,625,709,056 || trainable%: 0.1323

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
