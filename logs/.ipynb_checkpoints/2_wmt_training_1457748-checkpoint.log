Job ID: 1457748
Job Name: 2_wmt_training.py
Node: gx01
Start Time: Sun Nov 30 00:42:14 CET 2025
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sun Nov 30 00:42:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:0F:00.0 Off |                    0 |
| N/A   32C    P0             61W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
2_wmt_training.py
============================================================
WMT Fine-tuning
Model: llama3
Language Pair: zh_en
============================================================

Loading training data
  Train: ../../data/processed/zh_en/train_news_un_balanced_30000.tsv
  Val: ../../data/processed/zh_en/mix2k_dev.tsv
  Train samples: 30000
  Val samples: 2000

Loading model: meta-llama/Meta-Llama-3-8B

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.2732, 'grad_norm': 1.852638840675354, 'learning_rate': 1.7375886524822697e-05, 'epoch': 0.03}
{'loss': 0.7151, 'grad_norm': 0.6504985094070435, 'learning_rate': 3.5106382978723407e-05, 'epoch': 0.05}
{'loss': 0.6754, 'grad_norm': 0.8005422353744507, 'learning_rate': 5.283687943262412e-05, 'epoch': 0.08}
{'loss': 0.6244, 'grad_norm': 1.0431138277053833, 'learning_rate': 7.056737588652482e-05, 'epoch': 0.11}
{'loss': 0.6204, 'grad_norm': 0.8043088912963867, 'learning_rate': 8.829787234042553e-05, 'epoch': 0.13}
{'loss': 0.6368, 'grad_norm': 1.191440224647522, 'learning_rate': 9.999750216565724e-05, 'epoch': 0.16}
{'loss': 0.6369, 'grad_norm': 0.7853909134864807, 'learning_rate': 9.996120615348041e-05, 'epoch': 0.19}
{'loss': 0.6485, 'grad_norm': 1.0352429151535034, 'learning_rate': 9.988173129447252e-05, 'epoch': 0.21}
{'loss': 0.6325, 'grad_norm': 0.9102717638015747, 'learning_rate': 9.975914627458066e-05, 'epoch': 0.24}
{'loss': 0.6118, 'grad_norm': 0.8238329291343689, 'learning_rate': 9.959355703760014e-05, 'epoch': 0.27}
{'eval_loss': 0.643545389175415, 'eval_runtime': 61.0787, 'eval_samples_per_second': 32.745, 'eval_steps_per_second': 16.372, 'epoch': 0.27}
{'loss': 0.6334, 'grad_norm': 1.057283639907837, 'learning_rate': 9.93851066936128e-05, 'epoch': 0.29}
{'loss': 0.5951, 'grad_norm': 0.5759139657020569, 'learning_rate': 9.913397539530444e-05, 'epoch': 0.32}
{'loss': 0.6015, 'grad_norm': 0.8455507755279541, 'learning_rate': 9.884038018226838e-05, 'epoch': 0.35}
{'loss': 0.58, 'grad_norm': 0.8799837827682495, 'learning_rate': 9.850457479342942e-05, 'epoch': 0.37}
{'loss': 0.6269, 'grad_norm': 3.6032838821411133, 'learning_rate': 9.812684944775082e-05, 'epoch': 0.4}
{'loss': 0.6215, 'grad_norm': 0.6766691207885742, 'learning_rate': 9.770753059341306e-05, 'epoch': 0.43}
{'loss': 0.5965, 'grad_norm': 0.877606213092804, 'learning_rate': 9.724698062568196e-05, 'epoch': 0.45}
{'loss': 0.591, 'grad_norm': 0.773507833480835, 'learning_rate': 9.674559757370947e-05, 'epoch': 0.48}
{'loss': 0.6072, 'grad_norm': 0.8200927972793579, 'learning_rate': 9.620381475653791e-05, 'epoch': 0.51}
{'loss': 0.6153, 'grad_norm': 0.7408896088600159, 'learning_rate': 9.562210040860518e-05, 'epoch': 0.53}
{'eval_loss': 0.6234452128410339, 'eval_runtime': 61.459, 'eval_samples_per_second': 32.542, 'eval_steps_per_second': 16.271, 'epoch': 0.53}
{'loss': 0.5873, 'grad_norm': 0.9633743166923523, 'learning_rate': 9.50009572750742e-05, 'epoch': 0.56}
{'loss': 0.6041, 'grad_norm': 0.6662806868553162, 'learning_rate': 9.434092217733677e-05, 'epoch': 0.59}
{'loss': 0.5756, 'grad_norm': 0.7343398928642273, 'learning_rate': 9.364256554906699e-05, 'epoch': 0.61}
{'loss': 0.5771, 'grad_norm': 0.9305400848388672, 'learning_rate': 9.290649094322538e-05, 'epoch': 0.64}
{'loss': 0.596, 'grad_norm': 0.9429876208305359, 'learning_rate': 9.213333451043981e-05, 'epoch': 0.67}
{'loss': 0.5899, 'grad_norm': 0.7689723372459412, 'learning_rate': 9.132376444921379e-05, 'epoch': 0.69}
{'loss': 0.5682, 'grad_norm': 0.8368207216262817, 'learning_rate': 9.047848042843774e-05, 'epoch': 0.72}
{'loss': 0.5821, 'grad_norm': 0.9328218698501587, 'learning_rate': 8.959821298270183e-05, 'epoch': 0.75}
{'loss': 0.593, 'grad_norm': 0.8141748905181885, 'learning_rate': 8.868372288093334e-05, 'epoch': 0.77}
{'loss': 0.5929, 'grad_norm': 1.0393892526626587, 'learning_rate': 8.773580046890396e-05, 'epoch': 0.8}
