Using GPU inside JupyterLab session
stdout log: ../logs/2_wmt_training_20251129_005932.log
stderr log: ../logs/2_wmt_training_20251129_005932.err
Start Time: Sat Nov 29 00:59:32 CET 2025
Node: gx05
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sat Nov 29 00:59:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:25:00.0 Off |                    0 |
| N/A   33C    P0             40W /  250W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
============================================================
WMT Fine-tuning
Model: mistral
Language Pair: de_en
============================================================

Loading training data
  Train: ../../data/processed/de_en/train_europarl_newstest_balanced_26000.tsv
  Val: ../../data/processed/de_en/mix2k_dev.tsv
  Train samples: 26000
  Val samples: 1999

Loading model: mistralai/Mistral-7B-v0.1

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.3652, 'grad_norm': 2.138632297515869, 'learning_rate': 2.0081967213114755e-05, 'epoch': 0.03}
{'loss': 0.8268, 'grad_norm': 1.9109511375427246, 'learning_rate': 4.057377049180328e-05, 'epoch': 0.06}
{'loss': 0.8136, 'grad_norm': 2.0933263301849365, 'learning_rate': 6.10655737704918e-05, 'epoch': 0.09}
{'loss': 0.7892, 'grad_norm': 1.6283283233642578, 'learning_rate': 8.155737704918032e-05, 'epoch': 0.12}
{'loss': 0.8022, 'grad_norm': 1.538051724433899, 'learning_rate': 9.999971237291203e-05, 'epoch': 0.15}
{'loss': 0.8172, 'grad_norm': 1.6719844341278076, 'learning_rate': 9.996520112627602e-05, 'epoch': 0.18}
{'loss': 0.782, 'grad_norm': 1.504905343055725, 'learning_rate': 9.987320995457625e-05, 'epoch': 0.22}
{'loss': 0.7798, 'grad_norm': 1.7100046873092651, 'learning_rate': 9.972384468437874e-05, 'epoch': 0.25}
{'loss': 0.7762, 'grad_norm': 1.283862590789795, 'learning_rate': 9.951727714536386e-05, 'epoch': 0.28}
{'loss': 0.8191, 'grad_norm': 1.3452632427215576, 'learning_rate': 9.925374497265355e-05, 'epoch': 0.31}
{'eval_loss': 0.764509916305542, 'eval_runtime': 49.4858, 'eval_samples_per_second': 40.395, 'eval_steps_per_second': 20.208, 'epoch': 0.31}
{'loss': 0.7895, 'grad_norm': 1.3792506456375122, 'learning_rate': 9.893355133343611e-05, 'epoch': 0.34}
{'loss': 0.7838, 'grad_norm': 1.2790637016296387, 'learning_rate': 9.8557064578203e-05, 'epoch': 0.37}
{'loss': 0.7732, 'grad_norm': 1.1945741176605225, 'learning_rate': 9.812471781699857e-05, 'epoch': 0.4}
{'loss': 0.7576, 'grad_norm': 1.3813693523406982, 'learning_rate': 9.76370084211708e-05, 'epoch': 0.43}
{'loss': 0.7886, 'grad_norm': 1.160966157913208, 'learning_rate': 9.709449745119565e-05, 'epoch': 0.46}
{'loss': 0.8091, 'grad_norm': 1.3944369554519653, 'learning_rate': 9.649780901123357e-05, 'epoch': 0.49}
{'loss': 0.7615, 'grad_norm': 1.3407959938049316, 'learning_rate': 9.58476295311606e-05, 'epoch': 0.52}
{'loss': 0.7792, 'grad_norm': 1.5014795064926147, 'learning_rate': 9.51447069769e-05, 'epoch': 0.55}
{'loss': 0.7758, 'grad_norm': 1.4899169206619263, 'learning_rate': 9.438984998996298e-05, 'epoch': 0.58}
{'loss': 0.7833, 'grad_norm': 1.1726652383804321, 'learning_rate': 9.358392695718805e-05, 'epoch': 0.62}
{'eval_loss': 0.7498368620872498, 'eval_runtime': 49.6282, 'eval_samples_per_second': 40.279, 'eval_steps_per_second': 20.15, 'epoch': 0.62}
{'loss': 0.7658, 'grad_norm': 1.3138904571533203, 'learning_rate': 9.272786501174964e-05, 'epoch': 0.65}
{'loss': 0.7691, 'grad_norm': 1.2001889944076538, 'learning_rate': 9.182264896658486e-05, 'epoch': 0.68}
{'loss': 0.7701, 'grad_norm': 1.4988367557525635, 'learning_rate': 9.08693201814655e-05, 'epoch': 0.71}
{'loss': 0.7882, 'grad_norm': 1.3874839544296265, 'learning_rate': 8.986897536501864e-05, 'epoch': 0.74}
{'loss': 0.7745, 'grad_norm': 1.5348221063613892, 'learning_rate': 8.8822765313074e-05, 'epoch': 0.77}
{'loss': 0.7547, 'grad_norm': 1.3890423774719238, 'learning_rate': 8.77318935847894e-05, 'epoch': 0.8}
{'loss': 0.7725, 'grad_norm': 1.4703409671783447, 'learning_rate': 8.659761511807727e-05, 'epoch': 0.83}
{'loss': 0.7637, 'grad_norm': 1.5221478939056396, 'learning_rate': 8.542123478592518e-05, 'epoch': 0.86}
{'loss': 0.7879, 'grad_norm': 1.5979576110839844, 'learning_rate': 8.420410589527104e-05, 'epoch': 0.89}
{'loss': 0.7689, 'grad_norm': 1.3734478950500488, 'learning_rate': 8.294762863015995e-05, 'epoch': 0.92}
{'eval_loss': 0.7445321679115295, 'eval_runtime': 50.0938, 'eval_samples_per_second': 39.905, 'eval_steps_per_second': 19.963, 'epoch': 0.92}
{'loss': 0.7585, 'grad_norm': 1.2478384971618652, 'learning_rate': 8.165324844097368e-05, 'epoch': 0.95}
{'loss': 0.772, 'grad_norm': 1.3627431392669678, 'learning_rate': 8.032245438158576e-05, 'epoch': 0.98}
{'loss': 0.7089, 'grad_norm': 1.424381971359253, 'learning_rate': 7.895677739635517e-05, 'epoch': 1.02}
{'loss': 0.6522, 'grad_norm': 1.377589225769043, 'learning_rate': 7.755778855892922e-05, 'epoch': 1.05}
{'loss': 0.659, 'grad_norm': 1.458264946937561, 'learning_rate': 7.612709726488175e-05, 'epoch': 1.08}
{'loss': 0.642, 'grad_norm': 1.7896661758422852, 'learning_rate': 7.466634938026594e-05, 'epoch': 1.11}
{'loss': 0.6529, 'grad_norm': 1.513883352279663, 'learning_rate': 7.317722534821117e-05, 'epoch': 1.14}
{'loss': 0.6623, 'grad_norm': 1.377991795539856, 'learning_rate': 7.166143825574297e-05, 'epoch': 1.17}
{'loss': 0.6595, 'grad_norm': 2.0939159393310547, 'learning_rate': 7.012073186304886e-05, 'epoch': 1.2}
{'loss': 0.6317, 'grad_norm': 1.487076997756958, 'learning_rate': 6.855687859745827e-05, 'epoch': 1.23}
{'eval_loss': 0.7554022669792175, 'eval_runtime': 49.7428, 'eval_samples_per_second': 40.187, 'eval_steps_per_second': 20.103, 'epoch': 1.23}
{'loss': 0.6673, 'grad_norm': 1.6925668716430664, 'learning_rate': 6.697167751444366e-05, 'epoch': 1.26}
{'loss': 0.6613, 'grad_norm': 1.6328177452087402, 'learning_rate': 6.536695222798851e-05, 'epoch': 1.29}
{'loss': 0.6558, 'grad_norm': 1.6626451015472412, 'learning_rate': 6.374454881270344e-05, 'epoch': 1.32}
{'loss': 0.6687, 'grad_norm': 1.8707650899887085, 'learning_rate': 6.210633368010352e-05, 'epoch': 1.35}
{'loss': 0.6366, 'grad_norm': 1.66096830368042, 'learning_rate': 6.045419143148997e-05, 'epoch': 1.38}
{'loss': 0.6515, 'grad_norm': 1.657007098197937, 'learning_rate': 5.879002268990653e-05, 'epoch': 1.42}
{'loss': 0.6505, 'grad_norm': 1.502670407295227, 'learning_rate': 5.7115741913664264e-05, 'epoch': 1.45}
{'loss': 0.6155, 'grad_norm': 1.6571617126464844, 'learning_rate': 5.5433275193950326e-05, 'epoch': 1.48}
{'loss': 0.6587, 'grad_norm': 1.6473076343536377, 'learning_rate': 5.3744558039054296e-05, 'epoch': 1.51}
{'loss': 0.6256, 'grad_norm': 1.6998180150985718, 'learning_rate': 5.2051533147761155e-05, 'epoch': 1.54}
{'eval_loss': 0.7597089409828186, 'eval_runtime': 50.1707, 'eval_samples_per_second': 39.844, 'eval_steps_per_second': 19.932, 'epoch': 1.54}
{'loss': 0.6503, 'grad_norm': 1.4779521226882935, 'learning_rate': 5.035614817447212e-05, 'epoch': 1.57}
{'loss': 0.6525, 'grad_norm': 1.5036944150924683, 'learning_rate': 4.866035348862476e-05, 'epoch': 1.6}
{'loss': 0.6382, 'grad_norm': 1.6078945398330688, 'learning_rate': 4.696609993098965e-05, 'epoch': 1.63}
{'loss': 0.6357, 'grad_norm': 1.4591727256774902, 'learning_rate': 4.527533656942472e-05, 'epoch': 1.66}
{'loss': 0.6609, 'grad_norm': 1.6430994272232056, 'learning_rate': 4.359000845666936e-05, 'epoch': 1.69}
{'loss': 0.6264, 'grad_norm': 1.639954686164856, 'learning_rate': 4.191205439275729e-05, 'epoch': 1.72}
{'loss': 0.6692, 'grad_norm': 1.6904551982879639, 'learning_rate': 4.02434046946227e-05, 'epoch': 1.75}
{'loss': 0.6357, 'grad_norm': 1.6302158832550049, 'learning_rate': 3.858597897546526e-05, 'epoch': 1.78}
{'loss': 0.6377, 'grad_norm': 1.5501660108566284, 'learning_rate': 3.6941683936428716e-05, 'epoch': 1.82}
{'loss': 0.6326, 'grad_norm': 1.3634992837905884, 'learning_rate': 3.531241117313359e-05, 'epoch': 1.85}
{'eval_loss': 0.7501770257949829, 'eval_runtime': 50.0695, 'eval_samples_per_second': 39.925, 'eval_steps_per_second': 19.972, 'epoch': 1.85}
{'loss': 0.6418, 'grad_norm': 1.6643519401550293, 'learning_rate': 3.370003499958703e-05, 'epoch': 1.88}
{'loss': 0.6447, 'grad_norm': 1.7777657508850098, 'learning_rate': 3.210641029197368e-05, 'epoch': 1.91}
{'loss': 0.6442, 'grad_norm': 1.740062952041626, 'learning_rate': 3.053337035480765e-05, 'epoch': 1.94}
{'loss': 0.6293, 'grad_norm': 1.6735066175460815, 'learning_rate': 2.8982724811900564e-05, 'epoch': 1.97}
{'loss': 0.6422, 'grad_norm': 1.463382601737976, 'learning_rate': 2.7456257524571888e-05, 'epoch': 2.0}
{'loss': 0.5129, 'grad_norm': 1.8629955053329468, 'learning_rate': 2.5955724539496262e-05, 'epoch': 2.03}
{'loss': 0.4971, 'grad_norm': 2.361457586288452, 'learning_rate': 2.4482852068549046e-05, 'epoch': 2.06}
{'loss': 0.4902, 'grad_norm': 1.8005520105361938, 'learning_rate': 2.3039334502973542e-05, 'epoch': 2.09}
{'loss': 0.5079, 'grad_norm': 1.9203077554702759, 'learning_rate': 2.1626832464154785e-05, 'epoch': 2.12}
{'loss': 0.4907, 'grad_norm': 2.389726161956787, 'learning_rate': 2.024697089324208e-05, 'epoch': 2.15}
{'eval_loss': 0.8122403025627136, 'eval_runtime': 49.6375, 'eval_samples_per_second': 40.272, 'eval_steps_per_second': 20.146, 'epoch': 2.15}
{'loss': 0.4859, 'grad_norm': 2.3360352516174316, 'learning_rate': 1.89013371818181e-05, 'epoch': 2.18}
{'loss': 0.5008, 'grad_norm': 2.201178550720215, 'learning_rate': 1.7591479345764973e-05, 'epoch': 2.22}
{'loss': 0.4978, 'grad_norm': 2.211751699447632, 'learning_rate': 1.6318904244428028e-05, 'epoch': 2.25}
{'loss': 0.4967, 'grad_norm': 1.9863096475601196, 'learning_rate': 1.5085075847126213e-05, 'epoch': 2.28}
{'loss': 0.5062, 'grad_norm': 2.1070199012756348, 'learning_rate': 1.389141354900294e-05, 'epoch': 2.31}
{'loss': 0.4827, 'grad_norm': 2.0305335521698, 'learning_rate': 1.2739290538155147e-05, 'epoch': 2.34}
{'loss': 0.495, 'grad_norm': 2.2170002460479736, 'learning_rate': 1.1630032215918862e-05, 'epoch': 2.37}
{'loss': 0.4861, 'grad_norm': 2.2961232662200928, 'learning_rate': 1.0564914672128639e-05, 'epoch': 2.4}
{'loss': 0.4936, 'grad_norm': 1.8771047592163086, 'learning_rate': 9.54516321710488e-06, 'epoch': 2.43}
{'loss': 0.5124, 'grad_norm': 2.348444938659668, 'learning_rate': 8.57195097205789e-06, 'epoch': 2.46}
{'eval_loss': 0.8106608390808105, 'eval_runtime': 50.0543, 'eval_samples_per_second': 39.937, 'eval_steps_per_second': 19.978, 'epoch': 2.46}
{'loss': 0.4945, 'grad_norm': 1.7964375019073486, 'learning_rate': 7.64639751953023e-06, 'epoch': 2.49}
{'loss': 0.4712, 'grad_norm': 2.1704602241516113, 'learning_rate': 6.769567615429912e-06, 'epoch': 2.52}
{'loss': 0.4877, 'grad_norm': 2.310544490814209, 'learning_rate': 5.942469964136055e-06, 'epoch': 2.55}
{'loss': 0.47, 'grad_norm': 2.2089083194732666, 'learning_rate': 5.166056058086349e-06, 'epoch': 2.58}
{'loss': 0.494, 'grad_norm': 2.119028091430664, 'learning_rate': 4.441219083180786e-06, 'epoch': 2.62}
{'loss': 0.5167, 'grad_norm': 2.3287951946258545, 'learning_rate': 3.768792891261497e-06, 'epoch': 2.65}
{'loss': 0.4909, 'grad_norm': 2.4623911380767822, 'learning_rate': 3.1495510408502404e-06, 'epoch': 2.68}
{'loss': 0.4848, 'grad_norm': 2.177983283996582, 'learning_rate': 2.584205907247339e-06, 'epoch': 2.71}
{'loss': 0.4683, 'grad_norm': 2.1369569301605225, 'learning_rate': 2.0734078630157304e-06, 'epoch': 2.74}
{'loss': 0.4846, 'grad_norm': 2.362022638320923, 'learning_rate': 1.6177445297929527e-06, 'epoch': 2.77}
{'eval_loss': 0.8155132532119751, 'eval_runtime': 49.6053, 'eval_samples_per_second': 40.298, 'eval_steps_per_second': 20.159, 'epoch': 2.77}
{'loss': 0.4902, 'grad_norm': 2.0470807552337646, 'learning_rate': 1.2177401022916756e-06, 'epoch': 2.8}
{'loss': 0.486, 'grad_norm': 2.3759663105010986, 'learning_rate': 8.738547452665446e-07, 'epoch': 2.83}
{'loss': 0.4708, 'grad_norm': 2.1990249156951904, 'learning_rate': 5.864840641410907e-07, 'epoch': 2.86}
{'loss': 0.5122, 'grad_norm': 2.5302319526672363, 'learning_rate': 3.5595864990352056e-07, 'epoch': 2.89}
{'loss': 0.4994, 'grad_norm': 2.293771982192993, 'learning_rate': 1.825436987951512e-07, 'epoch': 2.92}
{'loss': 0.4783, 'grad_norm': 2.324763774871826, 'learning_rate': 6.643870722889411e-08, 'epoch': 2.95}
{'loss': 0.5024, 'grad_norm': 2.407323122024536, 'learning_rate': 7.777242288725672e-09, 'epoch': 2.98}
{'train_runtime': 7963.2415, 'train_samples_per_second': 9.795, 'train_steps_per_second': 0.612, 'train_loss': 0.6463658928504357, 'epoch': 3.0}

Saving adapter to ../models/wmt_adapters/mistral_de_en

Training complete.
Adapter saved to: ../models/wmt_adapters/mistral_de_en
Done at: Sat Nov 29 03:12:42 CET 2025
