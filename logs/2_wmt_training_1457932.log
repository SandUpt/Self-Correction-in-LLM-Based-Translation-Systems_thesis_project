Job ID: 1457932
Job Name: 2_wmt_training
Node: gx25
Start Time: Sun Nov 30 09:48:45 CET 2025
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sun Nov 30 09:48:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:07:00.0 Off |                    0 |
| N/A   39C    P0             57W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
2_wmt_training
============================================================
WMT Fine-tuning
Model: llama3
Language Pair: de_en
============================================================

Loading training data
  Train: ../../data/processed/de_en/train_europarl_newstest_balanced_26000.tsv
  Val: ../../data/processed/de_en/mix2k_dev.tsv
  Train samples: 26000
  Val samples: 1999

Loading model: meta-llama/Meta-Llama-3-8B

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.6249, 'grad_norm': 1.6810745000839233, 'learning_rate': 2.0081967213114755e-05, 'epoch': 0.03}
{'loss': 0.8743, 'grad_norm': 0.8592216968536377, 'learning_rate': 4.057377049180328e-05, 'epoch': 0.06}
{'loss': 0.855, 'grad_norm': 1.0450719594955444, 'learning_rate': 6.10655737704918e-05, 'epoch': 0.09}
{'loss': 0.8301, 'grad_norm': 0.9997121691703796, 'learning_rate': 8.155737704918032e-05, 'epoch': 0.12}
{'loss': 0.8441, 'grad_norm': 0.9454529285430908, 'learning_rate': 9.999971237291203e-05, 'epoch': 0.15}
{'loss': 0.8623, 'grad_norm': 0.8938636183738708, 'learning_rate': 9.996520112627602e-05, 'epoch': 0.18}
{'loss': 0.82, 'grad_norm': 0.7658501267433167, 'learning_rate': 9.987320995457625e-05, 'epoch': 0.22}
{'loss': 0.8194, 'grad_norm': 0.86100172996521, 'learning_rate': 9.972384468437874e-05, 'epoch': 0.25}
{'loss': 0.8178, 'grad_norm': 0.7001951932907104, 'learning_rate': 9.951727714536386e-05, 'epoch': 0.28}
{'loss': 0.8658, 'grad_norm': 0.8045905828475952, 'learning_rate': 9.925374497265355e-05, 'epoch': 0.31}
{'eval_loss': 0.8177748322486877, 'eval_runtime': 51.711, 'eval_samples_per_second': 38.657, 'eval_steps_per_second': 19.338, 'epoch': 0.31}
{'loss': 0.8284, 'grad_norm': 0.7525880932807922, 'learning_rate': 9.893355133343611e-05, 'epoch': 0.34}
{'loss': 0.827, 'grad_norm': 0.7700954675674438, 'learning_rate': 9.8557064578203e-05, 'epoch': 0.37}
{'loss': 0.8167, 'grad_norm': 0.6175194978713989, 'learning_rate': 9.812471781699857e-05, 'epoch': 0.4}
{'loss': 0.7997, 'grad_norm': 0.7809846997261047, 'learning_rate': 9.76370084211708e-05, 'epoch': 0.43}
{'loss': 0.8326, 'grad_norm': 0.6656531095504761, 'learning_rate': 9.709449745119565e-05, 'epoch': 0.46}
{'loss': 0.8518, 'grad_norm': 0.8237669467926025, 'learning_rate': 9.649780901123357e-05, 'epoch': 0.49}
{'loss': 0.8061, 'grad_norm': 0.720057487487793, 'learning_rate': 9.58476295311606e-05, 'epoch': 0.52}
{'loss': 0.8162, 'grad_norm': 0.6748267412185669, 'learning_rate': 9.51447069769e-05, 'epoch': 0.55}
{'loss': 0.8116, 'grad_norm': 0.7162894010543823, 'learning_rate': 9.438984998996298e-05, 'epoch': 0.58}
{'loss': 0.8309, 'grad_norm': 0.6132400035858154, 'learning_rate': 9.358392695718805e-05, 'epoch': 0.62}
{'eval_loss': 0.8034767508506775, 'eval_runtime': 51.6231, 'eval_samples_per_second': 38.723, 'eval_steps_per_second': 19.371, 'epoch': 0.62}
{'loss': 0.8071, 'grad_norm': 0.7044355869293213, 'learning_rate': 9.272786501174964e-05, 'epoch': 0.65}
{'loss': 0.8071, 'grad_norm': 0.6400267481803894, 'learning_rate': 9.182264896658486e-05, 'epoch': 0.68}
{'loss': 0.8206, 'grad_norm': 0.8086464405059814, 'learning_rate': 9.08693201814655e-05, 'epoch': 0.71}
{'loss': 0.8335, 'grad_norm': 0.6630319952964783, 'learning_rate': 8.986897536501864e-05, 'epoch': 0.74}
{'loss': 0.8175, 'grad_norm': 0.8517874479293823, 'learning_rate': 8.8822765313074e-05, 'epoch': 0.77}
{'loss': 0.8025, 'grad_norm': 0.6094303727149963, 'learning_rate': 8.77318935847894e-05, 'epoch': 0.8}
{'loss': 0.8176, 'grad_norm': 0.800285816192627, 'learning_rate': 8.659761511807727e-05, 'epoch': 0.83}
{'loss': 0.8113, 'grad_norm': 0.7532534003257751, 'learning_rate': 8.542123478592518e-05, 'epoch': 0.86}
{'loss': 0.8312, 'grad_norm': 0.8331425786018372, 'learning_rate': 8.420410589527104e-05, 'epoch': 0.89}
{'loss': 0.8123, 'grad_norm': 0.8362042307853699, 'learning_rate': 8.294762863015995e-05, 'epoch': 0.92}
{'eval_loss': 0.7965719699859619, 'eval_runtime': 52.2939, 'eval_samples_per_second': 38.226, 'eval_steps_per_second': 19.123, 'epoch': 0.92}
{'loss': 0.8045, 'grad_norm': 0.6409282684326172, 'learning_rate': 8.165324844097368e-05, 'epoch': 0.95}
{'loss': 0.8128, 'grad_norm': 0.7629387974739075, 'learning_rate': 8.032245438158576e-05, 'epoch': 0.98}
{'loss': 0.7751, 'grad_norm': 0.7413039207458496, 'learning_rate': 7.895677739635517e-05, 'epoch': 1.02}
{'loss': 0.7525, 'grad_norm': 0.7759178876876831, 'learning_rate': 7.755778855892922e-05, 'epoch': 1.05}
{'loss': 0.7517, 'grad_norm': 0.7587421536445618, 'learning_rate': 7.612709726488175e-05, 'epoch': 1.08}
{'loss': 0.7372, 'grad_norm': 1.283016562461853, 'learning_rate': 7.466634938026594e-05, 'epoch': 1.11}
{'loss': 0.7504, 'grad_norm': 0.7746458649635315, 'learning_rate': 7.317722534821117e-05, 'epoch': 1.14}
{'loss': 0.7603, 'grad_norm': 0.747231662273407, 'learning_rate': 7.166143825574297e-05, 'epoch': 1.17}
{'loss': 0.7554, 'grad_norm': 0.9708666205406189, 'learning_rate': 7.012073186304886e-05, 'epoch': 1.2}
{'loss': 0.7231, 'grad_norm': 0.8702391386032104, 'learning_rate': 6.855687859745827e-05, 'epoch': 1.23}
{'eval_loss': 0.8012210726737976, 'eval_runtime': 51.5873, 'eval_samples_per_second': 38.75, 'eval_steps_per_second': 19.385, 'epoch': 1.23}
{'loss': 0.7599, 'grad_norm': 0.8565815091133118, 'learning_rate': 6.697167751444366e-05, 'epoch': 1.26}
{'loss': 0.7535, 'grad_norm': 0.9988126754760742, 'learning_rate': 6.536695222798851e-05, 'epoch': 1.29}
{'loss': 0.7465, 'grad_norm': 0.9444641470909119, 'learning_rate': 6.374454881270344e-05, 'epoch': 1.32}
{'loss': 0.7671, 'grad_norm': 0.9462023377418518, 'learning_rate': 6.210633368010352e-05, 'epoch': 1.35}
{'loss': 0.7268, 'grad_norm': 0.9108491539955139, 'learning_rate': 6.045419143148997e-05, 'epoch': 1.38}
{'loss': 0.741, 'grad_norm': 0.9542595148086548, 'learning_rate': 5.879002268990653e-05, 'epoch': 1.42}
{'loss': 0.7398, 'grad_norm': 0.855075478553772, 'learning_rate': 5.7115741913664264e-05, 'epoch': 1.45}
{'loss': 0.7113, 'grad_norm': 0.8506426811218262, 'learning_rate': 5.5433275193950326e-05, 'epoch': 1.48}
{'loss': 0.7544, 'grad_norm': 0.942410945892334, 'learning_rate': 5.3744558039054296e-05, 'epoch': 1.51}
{'loss': 0.716, 'grad_norm': 0.9400450587272644, 'learning_rate': 5.2051533147761155e-05, 'epoch': 1.54}
{'eval_loss': 0.8003994822502136, 'eval_runtime': 51.5617, 'eval_samples_per_second': 38.769, 'eval_steps_per_second': 19.394, 'epoch': 1.54}
{'loss': 0.7436, 'grad_norm': 0.7259465456008911, 'learning_rate': 5.035614817447212e-05, 'epoch': 1.57}
{'loss': 0.749, 'grad_norm': 0.8621475696563721, 'learning_rate': 4.866035348862476e-05, 'epoch': 1.6}
{'loss': 0.7358, 'grad_norm': 0.9340049028396606, 'learning_rate': 4.696609993098965e-05, 'epoch': 1.63}
{'loss': 0.7251, 'grad_norm': 0.7908816337585449, 'learning_rate': 4.527533656942472e-05, 'epoch': 1.66}
{'loss': 0.7513, 'grad_norm': 0.8784936666488647, 'learning_rate': 4.359000845666936e-05, 'epoch': 1.69}
{'loss': 0.7199, 'grad_norm': 0.8620195388793945, 'learning_rate': 4.191205439275729e-05, 'epoch': 1.72}
{'loss': 0.7603, 'grad_norm': 1.031882405281067, 'learning_rate': 4.02434046946227e-05, 'epoch': 1.75}
{'loss': 0.7282, 'grad_norm': 0.9480578899383545, 'learning_rate': 3.858597897546526e-05, 'epoch': 1.78}
{'loss': 0.7317, 'grad_norm': 0.8190841674804688, 'learning_rate': 3.6941683936428716e-05, 'epoch': 1.82}
{'loss': 0.7256, 'grad_norm': 0.8454771041870117, 'learning_rate': 3.531241117313359e-05, 'epoch': 1.85}
{'eval_loss': 0.7961196899414062, 'eval_runtime': 51.5317, 'eval_samples_per_second': 38.792, 'eval_steps_per_second': 19.406, 'epoch': 1.85}
{'loss': 0.739, 'grad_norm': 0.8341857194900513, 'learning_rate': 3.370003499958703e-05, 'epoch': 1.88}
{'loss': 0.7399, 'grad_norm': 0.9708234071731567, 'learning_rate': 3.210641029197368e-05, 'epoch': 1.91}
{'loss': 0.7358, 'grad_norm': 0.9074041247367859, 'learning_rate': 3.053337035480765e-05, 'epoch': 1.94}
{'loss': 0.721, 'grad_norm': 0.8442946076393127, 'learning_rate': 2.8982724811900564e-05, 'epoch': 1.97}
{'loss': 0.7361, 'grad_norm': 0.8907608389854431, 'learning_rate': 2.7456257524571888e-05, 'epoch': 2.0}
{'loss': 0.6634, 'grad_norm': 1.0360002517700195, 'learning_rate': 2.5955724539496262e-05, 'epoch': 2.03}
{'loss': 0.6522, 'grad_norm': 1.0899755954742432, 'learning_rate': 2.4482852068549046e-05, 'epoch': 2.06}
{'loss': 0.6413, 'grad_norm': 0.889306902885437, 'learning_rate': 2.3039334502973542e-05, 'epoch': 2.09}
{'loss': 0.6659, 'grad_norm': 0.9668008089065552, 'learning_rate': 2.1626832464154785e-05, 'epoch': 2.12}
{'loss': 0.646, 'grad_norm': 1.1616026163101196, 'learning_rate': 2.024697089324208e-05, 'epoch': 2.15}
{'eval_loss': 0.8183122873306274, 'eval_runtime': 52.6405, 'eval_samples_per_second': 37.975, 'eval_steps_per_second': 18.997, 'epoch': 2.15}
{'loss': 0.6322, 'grad_norm': 1.2477490901947021, 'learning_rate': 1.89013371818181e-05, 'epoch': 2.18}
{'loss': 0.651, 'grad_norm': 1.2241730690002441, 'learning_rate': 1.7591479345764973e-05, 'epoch': 2.22}
{'loss': 0.6496, 'grad_norm': 1.4115947484970093, 'learning_rate': 1.6318904244428028e-05, 'epoch': 2.25}
{'loss': 0.6443, 'grad_norm': 1.1805522441864014, 'learning_rate': 1.5085075847126213e-05, 'epoch': 2.28}
{'loss': 0.6618, 'grad_norm': 1.0667388439178467, 'learning_rate': 1.389141354900294e-05, 'epoch': 2.31}
{'loss': 0.6282, 'grad_norm': 1.1256260871887207, 'learning_rate': 1.2739290538155147e-05, 'epoch': 2.34}
{'loss': 0.6391, 'grad_norm': 1.3426458835601807, 'learning_rate': 1.1630032215918862e-05, 'epoch': 2.37}
{'loss': 0.6397, 'grad_norm': 1.1579753160476685, 'learning_rate': 1.0564914672128639e-05, 'epoch': 2.4}
{'loss': 0.6473, 'grad_norm': 1.0071372985839844, 'learning_rate': 9.54516321710488e-06, 'epoch': 2.43}
{'loss': 0.6683, 'grad_norm': 1.2889809608459473, 'learning_rate': 8.57195097205789e-06, 'epoch': 2.46}
{'eval_loss': 0.8226931691169739, 'eval_runtime': 51.4814, 'eval_samples_per_second': 38.83, 'eval_steps_per_second': 19.425, 'epoch': 2.46}
{'loss': 0.6423, 'grad_norm': 0.9710009098052979, 'learning_rate': 7.64639751953023e-06, 'epoch': 2.49}
{'loss': 0.618, 'grad_norm': 1.11573326587677, 'learning_rate': 6.769567615429912e-06, 'epoch': 2.52}
{'loss': 0.6424, 'grad_norm': 1.2885675430297852, 'learning_rate': 5.942469964136055e-06, 'epoch': 2.55}
{'loss': 0.6148, 'grad_norm': 1.1138019561767578, 'learning_rate': 5.166056058086349e-06, 'epoch': 2.58}
{'loss': 0.6512, 'grad_norm': 1.0717957019805908, 'learning_rate': 4.441219083180786e-06, 'epoch': 2.62}
{'loss': 0.6719, 'grad_norm': 1.1540080308914185, 'learning_rate': 3.768792891261497e-06, 'epoch': 2.65}
{'loss': 0.6518, 'grad_norm': 1.342812418937683, 'learning_rate': 3.1495510408502404e-06, 'epoch': 2.68}
{'loss': 0.6302, 'grad_norm': 1.135015606880188, 'learning_rate': 2.584205907247339e-06, 'epoch': 2.71}
{'loss': 0.6175, 'grad_norm': 1.1895473003387451, 'learning_rate': 2.0734078630157304e-06, 'epoch': 2.74}
{'loss': 0.637, 'grad_norm': 1.2939307689666748, 'learning_rate': 1.6177445297929527e-06, 'epoch': 2.77}
{'eval_loss': 0.8231144547462463, 'eval_runtime': 51.115, 'eval_samples_per_second': 39.108, 'eval_steps_per_second': 19.564, 'epoch': 2.77}
{'loss': 0.6439, 'grad_norm': 1.1397448778152466, 'learning_rate': 1.2177401022916756e-06, 'epoch': 2.8}
{'loss': 0.6348, 'grad_norm': 1.2995637655258179, 'learning_rate': 8.738547452665446e-07, 'epoch': 2.83}
{'loss': 0.6237, 'grad_norm': 1.2207449674606323, 'learning_rate': 5.864840641410907e-07, 'epoch': 2.86}
{'loss': 0.6641, 'grad_norm': 1.5906251668930054, 'learning_rate': 3.5595864990352056e-07, 'epoch': 2.89}
{'loss': 0.6545, 'grad_norm': 1.1212024688720703, 'learning_rate': 1.825436987951512e-07, 'epoch': 2.92}
{'loss': 0.637, 'grad_norm': 1.1625034809112549, 'learning_rate': 6.643870722889411e-08, 'epoch': 2.95}
{'loss': 0.6569, 'grad_norm': 1.3353736400604248, 'learning_rate': 7.777242288725672e-09, 'epoch': 2.98}
{'train_runtime': 8285.9636, 'train_samples_per_second': 9.414, 'train_steps_per_second': 0.588, 'train_loss': 0.7447842978453025, 'epoch': 3.0}

Saving adapter to ../models/wmt_adapters/llama3_de_en_4M_Lora

Training complete.
Adapter saved to: ../models/wmt_adapters/llama3_de_en_4M_Lora
Done at: Sun Nov 30 12:07:17 CET 2025
