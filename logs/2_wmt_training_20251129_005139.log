Using GPU inside JupyterLab session
stdout log: ../logs/2_wmt_training_20251129_005139.log
stderr log: ../logs/2_wmt_training_20251129_005139.err
Start Time: Sat Nov 29 00:51:39 CET 2025
Node: gx05
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sat Nov 29 00:51:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:81:00.0 Off |                    0 |
| N/A   32C    P0             38W /  250W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
============================================================
WMT Fine-tuning
Model: llama3
Language Pair: zh_en
============================================================

Loading training data
  Train: ../../data/processed/zh_en/train_news_un_balanced_30000.tsv
  Val: ../../data/processed/zh_en/mix2k_dev.tsv
  Train samples: 30000
  Val samples: 2000

Loading model: meta-llama/Meta-Llama-3-8B

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.1431, 'grad_norm': 1.3139426708221436, 'learning_rate': 1.7375886524822697e-05, 'epoch': 0.03}
{'loss': 0.6849, 'grad_norm': 0.7505747675895691, 'learning_rate': 3.5106382978723407e-05, 'epoch': 0.05}
{'loss': 0.6594, 'grad_norm': 0.8461370468139648, 'learning_rate': 5.283687943262412e-05, 'epoch': 0.08}
{'loss': 0.6135, 'grad_norm': 1.0175708532333374, 'learning_rate': 7.056737588652482e-05, 'epoch': 0.11}
{'loss': 0.6089, 'grad_norm': 0.8076003789901733, 'learning_rate': 8.829787234042553e-05, 'epoch': 0.13}
{'loss': 0.6242, 'grad_norm': 0.9504638910293579, 'learning_rate': 9.999750216565724e-05, 'epoch': 0.16}
{'loss': 0.6234, 'grad_norm': 0.7675459980964661, 'learning_rate': 9.996120615348041e-05, 'epoch': 0.19}
{'loss': 0.6374, 'grad_norm': 1.08387291431427, 'learning_rate': 9.988173129447252e-05, 'epoch': 0.21}
{'loss': 0.6223, 'grad_norm': 0.9254703521728516, 'learning_rate': 9.975914627458066e-05, 'epoch': 0.24}
{'loss': 0.6003, 'grad_norm': 0.7504329085350037, 'learning_rate': 9.959355703760014e-05, 'epoch': 0.27}
{'eval_loss': 0.629638671875, 'eval_runtime': 63.7339, 'eval_samples_per_second': 31.38, 'eval_steps_per_second': 15.69, 'epoch': 0.27}
{'loss': 0.6228, 'grad_norm': 1.054520845413208, 'learning_rate': 9.93851066936128e-05, 'epoch': 0.29}
{'loss': 0.5834, 'grad_norm': 0.6434516906738281, 'learning_rate': 9.913397539530444e-05, 'epoch': 0.32}
{'loss': 0.5878, 'grad_norm': 0.8252302408218384, 'learning_rate': 9.884038018226838e-05, 'epoch': 0.35}
{'loss': 0.5655, 'grad_norm': 0.927117109298706, 'learning_rate': 9.850457479342942e-05, 'epoch': 0.37}
{'loss': 0.6136, 'grad_norm': 0.8971065282821655, 'learning_rate': 9.812684944775082e-05, 'epoch': 0.4}
{'loss': 0.6093, 'grad_norm': 0.8759095072746277, 'learning_rate': 9.770753059341306e-05, 'epoch': 0.43}
{'loss': 0.5822, 'grad_norm': 0.850034773349762, 'learning_rate': 9.724698062568196e-05, 'epoch': 0.45}
{'loss': 0.5766, 'grad_norm': 0.7560808658599854, 'learning_rate': 9.674559757370947e-05, 'epoch': 0.48}
{'loss': 0.5943, 'grad_norm': 0.8520886898040771, 'learning_rate': 9.620381475653791e-05, 'epoch': 0.51}
{'loss': 0.6018, 'grad_norm': 0.7439383268356323, 'learning_rate': 9.562210040860518e-05, 'epoch': 0.53}
{'eval_loss': 0.6106086373329163, 'eval_runtime': 64.698, 'eval_samples_per_second': 30.913, 'eval_steps_per_second': 15.456, 'epoch': 0.53}
{'loss': 0.5732, 'grad_norm': 0.8835819363594055, 'learning_rate': 9.50009572750742e-05, 'epoch': 0.56}
{'loss': 0.59, 'grad_norm': 0.7685063481330872, 'learning_rate': 9.434092217733677e-05, 'epoch': 0.59}
{'loss': 0.5623, 'grad_norm': 0.7697482705116272, 'learning_rate': 9.364256554906699e-05, 'epoch': 0.61}
{'loss': 0.5644, 'grad_norm': 0.9625360369682312, 'learning_rate': 9.290649094322538e-05, 'epoch': 0.64}
{'loss': 0.5835, 'grad_norm': 0.9282980561256409, 'learning_rate': 9.213333451043981e-05, 'epoch': 0.67}
{'loss': 0.576, 'grad_norm': 0.9263642430305481, 'learning_rate': 9.132376444921379e-05, 'epoch': 0.69}
{'loss': 0.5541, 'grad_norm': 0.9133768081665039, 'learning_rate': 9.047848042843774e-05, 'epoch': 0.72}
{'loss': 0.5688, 'grad_norm': 0.9947856664657593, 'learning_rate': 8.959821298270183e-05, 'epoch': 0.75}
{'loss': 0.5793, 'grad_norm': 0.760326087474823, 'learning_rate': 8.868372288093334e-05, 'epoch': 0.77}
{'loss': 0.582, 'grad_norm': 1.1728054285049438, 'learning_rate': 8.773580046890396e-05, 'epoch': 0.8}
{'eval_loss': 0.5974187254905701, 'eval_runtime': 63.8307, 'eval_samples_per_second': 31.333, 'eval_steps_per_second': 15.666, 'epoch': 0.8}
{'loss': 0.5611, 'grad_norm': 0.8910515308380127, 'learning_rate': 8.675526498617548e-05, 'epoch': 0.83}
{'loss': 0.6013, 'grad_norm': 0.7000088095664978, 'learning_rate': 8.57429638580741e-05, 'epoch': 0.85}
{'loss': 0.5754, 'grad_norm': 0.9446976780891418, 'learning_rate': 8.469977196330519e-05, 'epoch': 0.88}
{'loss': 0.5909, 'grad_norm': 0.9197479486465454, 'learning_rate': 8.362659087784153e-05, 'epoch': 0.91}
{'loss': 0.6016, 'grad_norm': 0.8572443723678589, 'learning_rate': 8.252434809573857e-05, 'epoch': 0.93}
{'loss': 0.5616, 'grad_norm': 0.909412682056427, 'learning_rate': 8.139399622755006e-05, 'epoch': 0.96}
{'loss': 0.5685, 'grad_norm': 0.6872982382774353, 'learning_rate': 8.023651217703671e-05, 'epoch': 0.99}
{'loss': 0.5339, 'grad_norm': 0.7506179809570312, 'learning_rate': 7.905289629687964e-05, 'epoch': 1.01}
{'loss': 0.4236, 'grad_norm': 0.8671579360961914, 'learning_rate': 7.784417152412801e-05, 'epoch': 1.04}
{'loss': 0.4259, 'grad_norm': 0.9795401692390442, 'learning_rate': 7.661138249612833e-05, 'epoch': 1.07}
{'eval_loss': 0.6065726280212402, 'eval_runtime': 63.8192, 'eval_samples_per_second': 31.339, 'eval_steps_per_second': 15.669, 'epoch': 1.07}
{'loss': 0.4264, 'grad_norm': 0.7154147028923035, 'learning_rate': 7.535559464769916e-05, 'epoch': 1.09}
{'loss': 0.4296, 'grad_norm': 0.8473572731018066, 'learning_rate': 7.407789329033188e-05, 'epoch': 1.12}
{'loss': 0.4187, 'grad_norm': 0.9381014704704285, 'learning_rate': 7.277938267421285e-05, 'epoch': 1.15}
{'loss': 0.4133, 'grad_norm': 1.0336949825286865, 'learning_rate': 7.146118503387795e-05, 'epoch': 1.17}
{'loss': 0.4079, 'grad_norm': 1.0814639329910278, 'learning_rate': 7.012443961832434e-05, 'epoch': 1.2}
{'loss': 0.4144, 'grad_norm': 1.2211148738861084, 'learning_rate': 6.877030170641722e-05, 'epoch': 1.23}
{'loss': 0.4472, 'grad_norm': 1.1146531105041504, 'learning_rate': 6.73999416084431e-05, 'epoch': 1.25}
{'loss': 0.4275, 'grad_norm': 1.011775255203247, 'learning_rate': 6.601454365467196e-05, 'epoch': 1.28}
{'loss': 0.4163, 'grad_norm': 0.8833091259002686, 'learning_rate': 6.46153051718029e-05, 'epoch': 1.31}
{'loss': 0.4154, 'grad_norm': 0.9966570734977722, 'learning_rate': 6.320343544817749e-05, 'epoch': 1.33}
{'eval_loss': 0.6168606281280518, 'eval_runtime': 63.7722, 'eval_samples_per_second': 31.362, 'eval_steps_per_second': 15.681, 'epoch': 1.33}
{'loss': 0.4142, 'grad_norm': 1.0581287145614624, 'learning_rate': 6.178015468865534e-05, 'epoch': 1.36}
{'loss': 0.4311, 'grad_norm': 0.9209120869636536, 'learning_rate': 6.034669296005522e-05, 'epoch': 1.39}
{'loss': 0.4409, 'grad_norm': 1.0481834411621094, 'learning_rate': 5.8904289128072745e-05, 'epoch': 1.41}
{'loss': 0.416, 'grad_norm': 1.3413686752319336, 'learning_rate': 5.745418978659398e-05, 'epoch': 1.44}
{'loss': 0.4014, 'grad_norm': 1.1424832344055176, 'learning_rate': 5.599764818032969e-05, 'epoch': 1.47}
{'loss': 0.4551, 'grad_norm': 1.0541508197784424, 'learning_rate': 5.453592312170179e-05, 'epoch': 1.49}
{'loss': 0.4454, 'grad_norm': 0.8064592480659485, 'learning_rate': 5.307027790291787e-05, 'epoch': 1.52}
{'loss': 0.4378, 'grad_norm': 0.9705061912536621, 'learning_rate': 5.160197920417409e-05, 'epoch': 1.55}
{'loss': 0.4276, 'grad_norm': 0.8168502449989319, 'learning_rate': 5.013229599892998e-05, 'epoch': 1.57}
{'loss': 0.4134, 'grad_norm': 0.9497534036636353, 'learning_rate': 4.866249845720133e-05, 'epoch': 1.6}
{'eval_loss': 0.6058246493339539, 'eval_runtime': 63.7961, 'eval_samples_per_second': 31.35, 'eval_steps_per_second': 15.675, 'epoch': 1.6}
{'loss': 0.4296, 'grad_norm': 1.2204418182373047, 'learning_rate': 4.7193856847818996e-05, 'epoch': 1.63}
{'loss': 0.4365, 'grad_norm': 1.0548479557037354, 'learning_rate': 4.5727640440602174e-05, 'epoch': 1.65}
{'loss': 0.4222, 'grad_norm': 1.0801821947097778, 'learning_rate': 4.426511640939515e-05, 'epoch': 1.68}
{'loss': 0.4215, 'grad_norm': 1.255296230316162, 'learning_rate': 4.2807548736915565e-05, 'epoch': 1.71}
{'loss': 0.4186, 'grad_norm': 0.6883515119552612, 'learning_rate': 4.13561971223605e-05, 'epoch': 1.73}
{'loss': 0.3888, 'grad_norm': 0.7510970830917358, 'learning_rate': 3.991231589271458e-05, 'epoch': 1.76}
{'loss': 0.4308, 'grad_norm': 1.1309173107147217, 'learning_rate': 3.8477152918701056e-05, 'epoch': 1.79}
{'loss': 0.4144, 'grad_norm': 1.087898850440979, 'learning_rate': 3.7051948536312654e-05, 'epoch': 1.81}
{'loss': 0.4094, 'grad_norm': 0.9059281945228577, 'learning_rate': 3.5637934474854334e-05, 'epoch': 1.84}
{'loss': 0.4167, 'grad_norm': 1.00819730758667, 'learning_rate': 3.423633279242433e-05, 'epoch': 1.87}
{'eval_loss': 0.6066486239433289, 'eval_runtime': 63.8008, 'eval_samples_per_second': 31.348, 'eval_steps_per_second': 15.674, 'epoch': 1.87}
{'loss': 0.4258, 'grad_norm': 1.053638219833374, 'learning_rate': 3.2848354819753455e-05, 'epoch': 1.89}
{'loss': 0.3966, 'grad_norm': 0.951492428779602, 'learning_rate': 3.147520011331566e-05, 'epoch': 1.92}
{'loss': 0.4086, 'grad_norm': 1.5090389251708984, 'learning_rate': 3.01180554186143e-05, 'epoch': 1.95}
{'loss': 0.4162, 'grad_norm': 1.0676186084747314, 'learning_rate': 2.877809364454032e-05, 'epoch': 1.97}
{'loss': 0.4158, 'grad_norm': 1.173796534538269, 'learning_rate': 2.7456472849688708e-05, 'epoch': 2.0}
{'loss': 0.2585, 'grad_norm': 1.0890604257583618, 'learning_rate': 2.6154335241509287e-05, 'epoch': 2.03}
{'loss': 0.2388, 'grad_norm': 1.2758758068084717, 'learning_rate': 2.4872806189156745e-05, 'epoch': 2.05}
{'loss': 0.2393, 'grad_norm': 1.1837645769119263, 'learning_rate': 2.3612993250893185e-05, 'epoch': 2.08}
{'loss': 0.2635, 'grad_norm': 1.1503537893295288, 'learning_rate': 2.2375985216883755e-05, 'epoch': 2.11}
{'loss': 0.2307, 'grad_norm': 1.1300123929977417, 'learning_rate': 2.1162851168212354e-05, 'epoch': 2.13}
{'eval_loss': 0.6909651160240173, 'eval_runtime': 63.7594, 'eval_samples_per_second': 31.368, 'eval_steps_per_second': 15.684, 'epoch': 2.13}
{'loss': 0.2338, 'grad_norm': 1.2402619123458862, 'learning_rate': 1.9974639552931145e-05, 'epoch': 2.16}
{'loss': 0.2303, 'grad_norm': 1.0491981506347656, 'learning_rate': 1.881237727994181e-05, 'epoch': 2.19}
{'loss': 0.2254, 'grad_norm': 1.0761886835098267, 'learning_rate': 1.7677068831492223e-05, 'epoch': 2.21}
{'loss': 0.2365, 'grad_norm': 1.655961036682129, 'learning_rate': 1.6569695395055107e-05, 'epoch': 2.24}
{'loss': 0.2445, 'grad_norm': 1.7969510555267334, 'learning_rate': 1.549121401533935e-05, 'epoch': 2.27}
{'loss': 0.2497, 'grad_norm': 1.3006178140640259, 'learning_rate': 1.444255676716637e-05, 'epoch': 2.29}
{'loss': 0.2443, 'grad_norm': 1.5468615293502808, 'learning_rate': 1.3424629949926931e-05, 'epoch': 2.32}
{'loss': 0.2443, 'grad_norm': 1.4694010019302368, 'learning_rate': 1.2438313304314048e-05, 'epoch': 2.35}
{'loss': 0.2359, 'grad_norm': 1.9000188112258911, 'learning_rate': 1.1484459252009421e-05, 'epoch': 2.37}
{'loss': 0.2306, 'grad_norm': 1.1587960720062256, 'learning_rate': 1.0563892158980033e-05, 'epoch': 2.4}
{'eval_loss': 0.696696400642395, 'eval_runtime': 63.7202, 'eval_samples_per_second': 31.387, 'eval_steps_per_second': 15.694, 'epoch': 2.4}
{'loss': 0.2377, 'grad_norm': 1.3701975345611572, 'learning_rate': 9.677407623022039e-06, 'epoch': 2.43}
{'loss': 0.2384, 'grad_norm': 1.228967547416687, 'learning_rate': 8.825771786167269e-06, 'epoch': 2.45}
{'loss': 0.2355, 'grad_norm': 1.4954782724380493, 'learning_rate': 8.009720672547e-06, 'epoch': 2.48}
{'loss': 0.2417, 'grad_norm': 1.3830848932266235, 'learning_rate': 7.229959552284849e-06, 'epoch': 2.51}
{'loss': 0.2274, 'grad_norm': 1.3268649578094482, 'learning_rate': 6.487162331968943e-06, 'epoch': 2.53}
{'loss': 0.243, 'grad_norm': 1.55472993850708, 'learning_rate': 5.781970972229766e-06, 'epoch': 2.56}
{'loss': 0.2308, 'grad_norm': 1.3227390050888062, 'learning_rate': 5.114994932927353e-06, 'epoch': 2.59}
{'loss': 0.2538, 'grad_norm': 1.56516695022583, 'learning_rate': 4.486810646427092e-06, 'epoch': 2.61}
{'loss': 0.2273, 'grad_norm': 1.4812021255493164, 'learning_rate': 3.897961019419516e-06, 'epoch': 2.64}
{'loss': 0.2386, 'grad_norm': 1.4796626567840576, 'learning_rate': 3.3489549637145958e-06, 'epoch': 2.67}
{'eval_loss': 0.6984347105026245, 'eval_runtime': 64.6047, 'eval_samples_per_second': 30.958, 'eval_steps_per_second': 15.479, 'epoch': 2.67}
{'loss': 0.2271, 'grad_norm': 1.474327802658081, 'learning_rate': 2.8402669564159323e-06, 'epoch': 2.69}
{'loss': 0.2488, 'grad_norm': 1.1426438093185425, 'learning_rate': 2.3723366298551652e-06, 'epoch': 2.72}
{'loss': 0.2386, 'grad_norm': 1.4361913204193115, 'learning_rate': 1.945568391640773e-06, 'epoch': 2.75}
{'loss': 0.2321, 'grad_norm': 1.3673456907272339, 'learning_rate': 1.560331075149879e-06, 'epoch': 2.77}
{'loss': 0.2341, 'grad_norm': 1.764587163925171, 'learning_rate': 1.2169576207648857e-06, 'epoch': 2.8}
{'loss': 0.2299, 'grad_norm': 1.2947217226028442, 'learning_rate': 9.15744788130618e-07, 'epoch': 2.83}
{'loss': 0.2319, 'grad_norm': 1.3406822681427002, 'learning_rate': 6.56952899680513e-07, 'epoch': 2.85}
{'loss': 0.2372, 'grad_norm': 1.0670300722122192, 'learning_rate': 4.408056156536555e-07, 'epoch': 2.88}
{'loss': 0.219, 'grad_norm': 1.198966383934021, 'learning_rate': 2.6748974079692235e-07, 'epoch': 2.91}
{'loss': 0.2252, 'grad_norm': 1.0984560251235962, 'learning_rate': 1.3715506291951395e-07, 'epoch': 2.93}
{'eval_loss': 0.6987260580062866, 'eval_runtime': 63.8864, 'eval_samples_per_second': 31.306, 'eval_steps_per_second': 15.653, 'epoch': 2.93}
{'loss': 0.2485, 'grad_norm': 1.4352986812591553, 'learning_rate': 4.991422343914587e-08, 'epoch': 2.96}
{'loss': 0.229, 'grad_norm': 1.8563799858093262, 'learning_rate': 5.842620032053825e-09, 'epoch': 2.99}
{'train_runtime': 12168.9481, 'train_samples_per_second': 7.396, 'train_steps_per_second': 0.462, 'train_loss': 0.4230565515306261, 'epoch': 3.0}

Saving adapter to ../models/wmt_adapters/llama3_zh_en

Training complete.
Adapter saved to: ../models/wmt_adapters/llama3_zh_en
Done at: Sat Nov 29 04:15:11 CET 2025
