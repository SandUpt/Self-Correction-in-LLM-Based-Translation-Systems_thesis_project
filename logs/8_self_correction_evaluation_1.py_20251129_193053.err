`torch_dtype` is deprecated! Use `dtype` instead!
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|                                                                                               | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|█████████████████████████████                                                          | 1/3 [00:06<00:13,  6.57s/it]Loading checkpoint shards:  67%|██████████████████████████████████████████████████████████                             | 2/3 [00:12<00:06,  6.04s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.54s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:17<00:00,  5.73s/it]
Generating:   0%|                                                                                                             | 0/16 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Generating:   0%|                                                                                                             | 0/16 [01:06<?, ?it/s]
Traceback (most recent call last):
  File "/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/scripts/8_self_correction_evaluation_1.py", line 514, in <module>
    main()
  File "/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/scripts/8_self_correction_evaluation_1.py", line 346, in main
    raw_outputs = generate_outputs(
                  ^^^^^^^^^^^^^^^^^
  File "/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/scripts/8_self_correction_evaluation_1.py", line 238, in generate_outputs
    outputs = model.generate(**inputs, **gen_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
             ^^^^^^^^^^^^^^^^
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/transformers/generation/utils.py", line 3377, in _beam_search
    model_kwargs["past_key_values"].reorder_cache(beam_idx)
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/transformers/cache_utils.py", line 832, in reorder_cache
    self.layers[layer_idx].reorder_cache(beam_idx)
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/transformers/cache_utils.py", line 80, in reorder_cache
    self.keys = self.keys.index_select(0, beam_idx.to(self.keys.device))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 410.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 399.56 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 38.22 GiB is allocated by PyTorch, and 389.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
