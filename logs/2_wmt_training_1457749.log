Job ID: 1457749
Job Name: 2_wmt_training.py
Node: gx01
Start Time: Sun Nov 30 00:44:52 CET 2025
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sun Nov 30 00:44:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:47:00.0 Off |                    0 |
| N/A   31C    P0             54W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
2_wmt_training.py
============================================================
WMT Fine-tuning
Model: llama3
Language Pair: zh_en
============================================================

Loading training data
  Train: ../../data/processed/zh_en/train_news_un_balanced_30000.tsv
  Val: ../../data/processed/zh_en/mix2k_dev.tsv
  Train samples: 30000
  Val samples: 2000

Loading model: meta-llama/Meta-Llama-3-8B

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.2732, 'grad_norm': 1.852638840675354, 'learning_rate': 1.7375886524822697e-05, 'epoch': 0.03}
{'loss': 0.7151, 'grad_norm': 0.6504985094070435, 'learning_rate': 3.5106382978723407e-05, 'epoch': 0.05}
{'loss': 0.6754, 'grad_norm': 0.8005422353744507, 'learning_rate': 5.283687943262412e-05, 'epoch': 0.08}
{'loss': 0.6244, 'grad_norm': 1.0431138277053833, 'learning_rate': 7.056737588652482e-05, 'epoch': 0.11}
{'loss': 0.6204, 'grad_norm': 0.8043088912963867, 'learning_rate': 8.829787234042553e-05, 'epoch': 0.13}
{'loss': 0.6368, 'grad_norm': 1.191440224647522, 'learning_rate': 9.999750216565724e-05, 'epoch': 0.16}
{'loss': 0.6369, 'grad_norm': 0.7853909134864807, 'learning_rate': 9.996120615348041e-05, 'epoch': 0.19}
{'loss': 0.6485, 'grad_norm': 1.0352429151535034, 'learning_rate': 9.988173129447252e-05, 'epoch': 0.21}
{'loss': 0.6325, 'grad_norm': 0.9102717638015747, 'learning_rate': 9.975914627458066e-05, 'epoch': 0.24}
{'loss': 0.6118, 'grad_norm': 0.8238329291343689, 'learning_rate': 9.959355703760014e-05, 'epoch': 0.27}
{'eval_loss': 0.643545389175415, 'eval_runtime': 61.6629, 'eval_samples_per_second': 32.434, 'eval_steps_per_second': 16.217, 'epoch': 0.27}
{'loss': 0.6334, 'grad_norm': 1.057283639907837, 'learning_rate': 9.93851066936128e-05, 'epoch': 0.29}
{'loss': 0.5951, 'grad_norm': 0.5759139657020569, 'learning_rate': 9.913397539530444e-05, 'epoch': 0.32}
{'loss': 0.6015, 'grad_norm': 0.8455507755279541, 'learning_rate': 9.884038018226838e-05, 'epoch': 0.35}
{'loss': 0.58, 'grad_norm': 0.8799837827682495, 'learning_rate': 9.850457479342942e-05, 'epoch': 0.37}
{'loss': 0.6269, 'grad_norm': 3.6032838821411133, 'learning_rate': 9.812684944775082e-05, 'epoch': 0.4}
{'loss': 0.6215, 'grad_norm': 0.6766691207885742, 'learning_rate': 9.770753059341306e-05, 'epoch': 0.43}
{'loss': 0.5965, 'grad_norm': 0.877606213092804, 'learning_rate': 9.724698062568196e-05, 'epoch': 0.45}
{'loss': 0.591, 'grad_norm': 0.773507833480835, 'learning_rate': 9.674559757370947e-05, 'epoch': 0.48}
{'loss': 0.6072, 'grad_norm': 0.8200927972793579, 'learning_rate': 9.620381475653791e-05, 'epoch': 0.51}
{'loss': 0.6153, 'grad_norm': 0.7408896088600159, 'learning_rate': 9.562210040860518e-05, 'epoch': 0.53}
{'eval_loss': 0.6234452128410339, 'eval_runtime': 61.2216, 'eval_samples_per_second': 32.668, 'eval_steps_per_second': 16.334, 'epoch': 0.53}
{'loss': 0.5873, 'grad_norm': 0.9633743166923523, 'learning_rate': 9.50009572750742e-05, 'epoch': 0.56}
{'loss': 0.6041, 'grad_norm': 0.6662806868553162, 'learning_rate': 9.434092217733677e-05, 'epoch': 0.59}
{'loss': 0.5756, 'grad_norm': 0.7343398928642273, 'learning_rate': 9.364256554906699e-05, 'epoch': 0.61}
{'loss': 0.5771, 'grad_norm': 0.9305400848388672, 'learning_rate': 9.290649094322538e-05, 'epoch': 0.64}
{'loss': 0.596, 'grad_norm': 0.9429876208305359, 'learning_rate': 9.213333451043981e-05, 'epoch': 0.67}
{'loss': 0.5899, 'grad_norm': 0.7689723372459412, 'learning_rate': 9.132376444921379e-05, 'epoch': 0.69}
{'loss': 0.5682, 'grad_norm': 0.8368207216262817, 'learning_rate': 9.047848042843774e-05, 'epoch': 0.72}
{'loss': 0.5821, 'grad_norm': 0.9328218698501587, 'learning_rate': 8.959821298270183e-05, 'epoch': 0.75}
{'loss': 0.593, 'grad_norm': 0.8141748905181885, 'learning_rate': 8.868372288093334e-05, 'epoch': 0.77}
{'loss': 0.5929, 'grad_norm': 1.0393892526626587, 'learning_rate': 8.773580046890396e-05, 'epoch': 0.8}
{'eval_loss': 0.6120176911354065, 'eval_runtime': 61.2336, 'eval_samples_per_second': 32.662, 'eval_steps_per_second': 16.331, 'epoch': 0.8}
{'loss': 0.5744, 'grad_norm': 0.7232457399368286, 'learning_rate': 8.675526498617548e-05, 'epoch': 0.83}
{'loss': 0.6118, 'grad_norm': 0.7242682576179504, 'learning_rate': 8.57429638580741e-05, 'epoch': 0.85}
{'loss': 0.5909, 'grad_norm': 0.8935680985450745, 'learning_rate': 8.469977196330519e-05, 'epoch': 0.88}
{'loss': 0.6042, 'grad_norm': 0.8337541818618774, 'learning_rate': 8.362659087784153e-05, 'epoch': 0.91}
{'loss': 0.6139, 'grad_norm': 0.7079495787620544, 'learning_rate': 8.252434809573857e-05, 'epoch': 0.93}
{'loss': 0.5744, 'grad_norm': 0.7022823691368103, 'learning_rate': 8.139399622755006e-05, 'epoch': 0.96}
{'loss': 0.5836, 'grad_norm': 0.6613590121269226, 'learning_rate': 8.023651217703671e-05, 'epoch': 0.99}
{'loss': 0.5866, 'grad_norm': 0.6705599427223206, 'learning_rate': 7.905289629687964e-05, 'epoch': 1.01}
{'loss': 0.5064, 'grad_norm': 0.7679206132888794, 'learning_rate': 7.784417152412801e-05, 'epoch': 1.04}
{'loss': 0.5126, 'grad_norm': 0.840488612651825, 'learning_rate': 7.661138249612833e-05, 'epoch': 1.07}
{'eval_loss': 0.6067472696304321, 'eval_runtime': 61.2822, 'eval_samples_per_second': 32.636, 'eval_steps_per_second': 16.318, 'epoch': 1.07}
{'loss': 0.5162, 'grad_norm': 0.6422352194786072, 'learning_rate': 7.535559464769916e-05, 'epoch': 1.09}
{'loss': 0.5194, 'grad_norm': 0.7235034108161926, 'learning_rate': 7.407789329033188e-05, 'epoch': 1.12}
{'loss': 0.5097, 'grad_norm': 0.8158812522888184, 'learning_rate': 7.277938267421285e-05, 'epoch': 1.15}
{'loss': 0.5058, 'grad_norm': 1.0317970514297485, 'learning_rate': 7.146118503387795e-05, 'epoch': 1.17}
{'loss': 0.5002, 'grad_norm': 1.1717193126678467, 'learning_rate': 7.012443961832434e-05, 'epoch': 1.2}
{'loss': 0.4998, 'grad_norm': 1.0553076267242432, 'learning_rate': 6.877030170641722e-05, 'epoch': 1.23}
{'loss': 0.5387, 'grad_norm': 0.817009687423706, 'learning_rate': 6.73999416084431e-05, 'epoch': 1.25}
{'loss': 0.5194, 'grad_norm': 0.8308261632919312, 'learning_rate': 6.601454365467196e-05, 'epoch': 1.28}
{'loss': 0.5033, 'grad_norm': 0.7952234148979187, 'learning_rate': 6.46153051718029e-05, 'epoch': 1.31}
{'loss': 0.5071, 'grad_norm': 0.8270005583763123, 'learning_rate': 6.320343544817749e-05, 'epoch': 1.33}
{'eval_loss': 0.6048730611801147, 'eval_runtime': 61.5374, 'eval_samples_per_second': 32.501, 'eval_steps_per_second': 16.25, 'epoch': 1.33}
{'loss': 0.5022, 'grad_norm': 1.0846461057662964, 'learning_rate': 6.178015468865534e-05, 'epoch': 1.36}
{'loss': 0.5261, 'grad_norm': 0.8316018581390381, 'learning_rate': 6.034669296005522e-05, 'epoch': 1.39}
{'loss': 0.5332, 'grad_norm': 1.0325623750686646, 'learning_rate': 5.8904289128072745e-05, 'epoch': 1.41}
{'loss': 0.5105, 'grad_norm': 1.0406121015548706, 'learning_rate': 5.745418978659398e-05, 'epoch': 1.44}
{'loss': 0.4937, 'grad_norm': 0.9484491348266602, 'learning_rate': 5.599764818032969e-05, 'epoch': 1.47}
{'loss': 0.5481, 'grad_norm': 0.9177021980285645, 'learning_rate': 5.453592312170179e-05, 'epoch': 1.49}
{'loss': 0.5401, 'grad_norm': 0.8210644721984863, 'learning_rate': 5.307027790291787e-05, 'epoch': 1.52}
{'loss': 0.5394, 'grad_norm': 0.8452796339988708, 'learning_rate': 5.160197920417409e-05, 'epoch': 1.55}
{'loss': 0.524, 'grad_norm': 0.7469618916511536, 'learning_rate': 5.013229599892998e-05, 'epoch': 1.57}
{'loss': 0.5027, 'grad_norm': 0.7975105047225952, 'learning_rate': 4.866249845720133e-05, 'epoch': 1.6}
{'eval_loss': 0.6005339622497559, 'eval_runtime': 61.122, 'eval_samples_per_second': 32.721, 'eval_steps_per_second': 16.361, 'epoch': 1.6}
{'loss': 0.5278, 'grad_norm': 0.9940559267997742, 'learning_rate': 4.7193856847818996e-05, 'epoch': 1.63}
{'loss': 0.5344, 'grad_norm': 0.85591721534729, 'learning_rate': 4.5727640440602174e-05, 'epoch': 1.65}
{'loss': 0.5192, 'grad_norm': 0.9411985278129578, 'learning_rate': 4.426511640939515e-05, 'epoch': 1.68}
{'loss': 0.5151, 'grad_norm': 0.9989086389541626, 'learning_rate': 4.2807548736915565e-05, 'epoch': 1.71}
{'loss': 0.5086, 'grad_norm': 0.6789267063140869, 'learning_rate': 4.13561971223605e-05, 'epoch': 1.73}
{'loss': 0.4717, 'grad_norm': 0.6539506316184998, 'learning_rate': 3.991231589271458e-05, 'epoch': 1.76}
{'loss': 0.5226, 'grad_norm': 0.9034648537635803, 'learning_rate': 3.8477152918701056e-05, 'epoch': 1.79}
{'loss': 0.5054, 'grad_norm': 0.7943661212921143, 'learning_rate': 3.7051948536312654e-05, 'epoch': 1.81}
{'loss': 0.4988, 'grad_norm': 0.8172390460968018, 'learning_rate': 3.5637934474854334e-05, 'epoch': 1.84}
{'loss': 0.5167, 'grad_norm': 0.9168936014175415, 'learning_rate': 3.423633279242433e-05, 'epoch': 1.87}
{'eval_loss': 0.597382664680481, 'eval_runtime': 61.5734, 'eval_samples_per_second': 32.482, 'eval_steps_per_second': 16.241, 'epoch': 1.87}
{'loss': 0.5269, 'grad_norm': 0.9540205001831055, 'learning_rate': 3.2848354819753455e-05, 'epoch': 1.89}
{'loss': 0.4868, 'grad_norm': 0.7839095592498779, 'learning_rate': 3.147520011331566e-05, 'epoch': 1.92}
{'loss': 0.4995, 'grad_norm': 1.0602422952651978, 'learning_rate': 3.01180554186143e-05, 'epoch': 1.95}
{'loss': 0.5133, 'grad_norm': 0.8803828358650208, 'learning_rate': 2.877809364454032e-05, 'epoch': 1.97}
{'loss': 0.507, 'grad_norm': 0.941525399684906, 'learning_rate': 2.7456472849688708e-05, 'epoch': 2.0}
{'loss': 0.4484, 'grad_norm': 0.9557188749313354, 'learning_rate': 2.6154335241509287e-05, 'epoch': 2.03}
{'loss': 0.4333, 'grad_norm': 0.9439886808395386, 'learning_rate': 2.4872806189156745e-05, 'epoch': 2.05}
{'loss': 0.4389, 'grad_norm': 1.0551886558532715, 'learning_rate': 2.3612993250893185e-05, 'epoch': 2.08}
{'loss': 0.4608, 'grad_norm': 1.0013893842697144, 'learning_rate': 2.2375985216883755e-05, 'epoch': 2.11}
{'loss': 0.4152, 'grad_norm': 0.8843963146209717, 'learning_rate': 2.1162851168212354e-05, 'epoch': 2.13}
{'eval_loss': 0.614214301109314, 'eval_runtime': 61.0807, 'eval_samples_per_second': 32.744, 'eval_steps_per_second': 16.372, 'epoch': 2.13}
{'loss': 0.4214, 'grad_norm': 1.9185285568237305, 'learning_rate': 1.9974639552931145e-05, 'epoch': 2.16}
{'loss': 0.4266, 'grad_norm': 0.8716311454772949, 'learning_rate': 1.881237727994181e-05, 'epoch': 2.19}
{'loss': 0.409, 'grad_norm': 0.987064778804779, 'learning_rate': 1.7677068831492223e-05, 'epoch': 2.21}
{'loss': 0.4318, 'grad_norm': 1.1459959745407104, 'learning_rate': 1.6569695395055107e-05, 'epoch': 2.24}
{'loss': 0.4419, 'grad_norm': 1.0926830768585205, 'learning_rate': 1.549121401533935e-05, 'epoch': 2.27}
{'loss': 0.4484, 'grad_norm': 1.026294469833374, 'learning_rate': 1.444255676716637e-05, 'epoch': 2.29}
{'loss': 0.44, 'grad_norm': 1.2085893154144287, 'learning_rate': 1.3424629949926931e-05, 'epoch': 2.32}
{'loss': 0.4543, 'grad_norm': 1.0371630191802979, 'learning_rate': 1.2438313304314048e-05, 'epoch': 2.35}
{'loss': 0.4268, 'grad_norm': 1.3317700624465942, 'learning_rate': 1.1484459252009421e-05, 'epoch': 2.37}
{'loss': 0.4278, 'grad_norm': 0.7777818441390991, 'learning_rate': 1.0563892158980033e-05, 'epoch': 2.4}
{'eval_loss': 0.6141378879547119, 'eval_runtime': 61.0955, 'eval_samples_per_second': 32.736, 'eval_steps_per_second': 16.368, 'epoch': 2.4}
{'loss': 0.4371, 'grad_norm': 1.1636240482330322, 'learning_rate': 9.677407623022039e-06, 'epoch': 2.43}
{'loss': 0.4379, 'grad_norm': 0.9095163345336914, 'learning_rate': 8.825771786167269e-06, 'epoch': 2.45}
{'loss': 0.4293, 'grad_norm': 1.0969843864440918, 'learning_rate': 8.009720672547e-06, 'epoch': 2.48}
{'loss': 0.4422, 'grad_norm': 1.1228094100952148, 'learning_rate': 7.229959552284849e-06, 'epoch': 2.51}
{'loss': 0.4147, 'grad_norm': 0.9574878215789795, 'learning_rate': 6.487162331968943e-06, 'epoch': 2.53}
{'loss': 0.4392, 'grad_norm': 1.1168960332870483, 'learning_rate': 5.781970972229766e-06, 'epoch': 2.56}
{'loss': 0.4237, 'grad_norm': 1.1768244504928589, 'learning_rate': 5.114994932927353e-06, 'epoch': 2.59}
{'loss': 0.4604, 'grad_norm': 1.2046852111816406, 'learning_rate': 4.486810646427092e-06, 'epoch': 2.61}
{'loss': 0.417, 'grad_norm': 1.0849932432174683, 'learning_rate': 3.897961019419516e-06, 'epoch': 2.64}
{'loss': 0.4334, 'grad_norm': 1.1712595224380493, 'learning_rate': 3.3489549637145958e-06, 'epoch': 2.67}
{'eval_loss': 0.6156598329544067, 'eval_runtime': 61.313, 'eval_samples_per_second': 32.62, 'eval_steps_per_second': 16.31, 'epoch': 2.67}
{'loss': 0.4278, 'grad_norm': 1.2639057636260986, 'learning_rate': 2.8402669564159323e-06, 'epoch': 2.69}
{'loss': 0.4464, 'grad_norm': 0.9153949022293091, 'learning_rate': 2.3723366298551652e-06, 'epoch': 2.72}
{'loss': 0.4405, 'grad_norm': 1.2988377809524536, 'learning_rate': 1.945568391640773e-06, 'epoch': 2.75}
{'loss': 0.4262, 'grad_norm': 1.0584248304367065, 'learning_rate': 1.560331075149879e-06, 'epoch': 2.77}
{'loss': 0.4262, 'grad_norm': 1.4264615774154663, 'learning_rate': 1.2169576207648857e-06, 'epoch': 2.8}
{'loss': 0.4224, 'grad_norm': 1.1305670738220215, 'learning_rate': 9.15744788130618e-07, 'epoch': 2.83}
{'loss': 0.4228, 'grad_norm': 1.105709433555603, 'learning_rate': 6.56952899680513e-07, 'epoch': 2.85}
{'loss': 0.4405, 'grad_norm': 0.9564218521118164, 'learning_rate': 4.408056156536555e-07, 'epoch': 2.88}
{'loss': 0.4125, 'grad_norm': 1.0999220609664917, 'learning_rate': 2.6748974079692235e-07, 'epoch': 2.91}
{'loss': 0.4229, 'grad_norm': 0.8480565547943115, 'learning_rate': 1.3715506291951395e-07, 'epoch': 2.93}
{'eval_loss': 0.6154065132141113, 'eval_runtime': 61.5313, 'eval_samples_per_second': 32.504, 'eval_steps_per_second': 16.252, 'epoch': 2.93}
{'loss': 0.4554, 'grad_norm': 1.1107913255691528, 'learning_rate': 4.991422343914587e-08, 'epoch': 2.96}
{'loss': 0.424, 'grad_norm': 1.065019130706787, 'learning_rate': 5.842620032053825e-09, 'epoch': 2.99}
{'train_runtime': 11329.4323, 'train_samples_per_second': 7.944, 'train_steps_per_second': 0.496, 'train_loss': 0.5246473676893446, 'epoch': 3.0}

Saving adapter to ../models/wmt_adapters/llama3_zh_en_4M_Lora

Training complete.
Adapter saved to: ../models/wmt_adapters/llama3_zh_en_4M_Lora
Done at: Sun Nov 30 03:54:14 CET 2025
