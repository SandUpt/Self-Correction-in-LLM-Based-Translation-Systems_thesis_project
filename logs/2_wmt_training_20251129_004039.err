`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                                           | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|████████████████████▊                                                              | 1/4 [00:00<00:02,  1.18it/s]Loading checkpoint shards:  50%|█████████████████████████████████████████▌                                         | 2/4 [00:01<00:01,  1.31it/s]Loading checkpoint shards:  75%|██████████████████████████████████████████████████████████████▎                    | 3/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.44it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.39it/s]
Tokenizing:   0%|                                                                                               | 0/30000 [00:00<?, ? examples/s]Tokenizing:   3%|██▋                                                                               | 1000/30000 [00:00<00:14, 2066.41 examples/s]Tokenizing:   7%|█████▍                                                                            | 2000/30000 [00:00<00:10, 2560.17 examples/s]Tokenizing:  10%|████████▏                                                                         | 3000/30000 [00:01<00:09, 2785.29 examples/s]Tokenizing:  13%|██████████▉                                                                       | 4000/30000 [00:01<00:08, 2902.79 examples/s]Tokenizing:  17%|█████████████▋                                                                    | 5000/30000 [00:01<00:08, 2980.85 examples/s]Tokenizing:  20%|████████████████▍                                                                 | 6000/30000 [00:02<00:07, 3023.61 examples/s]Tokenizing:  23%|███████████████████▏                                                              | 7000/30000 [00:02<00:07, 3048.68 examples/s]Tokenizing:  27%|█████████████████████▊                                                            | 8000/30000 [00:02<00:07, 3055.78 examples/s]Tokenizing:  30%|████████████████████████▌                                                         | 9000/30000 [00:03<00:06, 3081.93 examples/s]Tokenizing:  33%|███████████████████████████                                                      | 10000/30000 [00:03<00:06, 3092.26 examples/s]Tokenizing:  37%|█████████████████████████████▋                                                   | 11000/30000 [00:03<00:06, 3107.50 examples/s]Tokenizing:  40%|████████████████████████████████▍                                                | 12000/30000 [00:04<00:05, 3073.22 examples/s]Tokenizing:  43%|███████████████████████████████████                                              | 13000/30000 [00:04<00:05, 3080.57 examples/s]Tokenizing:  47%|█████████████████████████████████████▊                                           | 14000/30000 [00:04<00:05, 3082.87 examples/s]Tokenizing:  50%|████████████████████████████████████████▌                                        | 15000/30000 [00:05<00:04, 3075.11 examples/s]Tokenizing:  53%|███████████████████████████████████████████▏                                     | 16000/30000 [00:05<00:04, 3084.35 examples/s]Tokenizing:  57%|█████████████████████████████████████████████▉                                   | 17000/30000 [00:05<00:04, 3092.67 examples/s]Tokenizing:  60%|████████████████████████████████████████████████▌                                | 18000/30000 [00:05<00:03, 3109.13 examples/s]Tokenizing:  63%|███████████████████████████████████████████████████▎                             | 19000/30000 [00:06<00:03, 3109.39 examples/s]Tokenizing:  67%|██████████████████████████████████████████████████████                           | 20000/30000 [00:06<00:03, 3114.54 examples/s]Tokenizing:  70%|████████████████████████████████████████████████████████▋                        | 21000/30000 [00:06<00:02, 3114.62 examples/s]Tokenizing:  73%|███████████████████████████████████████████████████████████▍                     | 22000/30000 [00:07<00:02, 3117.75 examples/s]Tokenizing:  77%|██████████████████████████████████████████████████████████████                   | 23000/30000 [00:07<00:02, 3099.67 examples/s]Tokenizing:  80%|████████████████████████████████████████████████████████████████▊                | 24000/30000 [00:07<00:01, 3027.62 examples/s]Tokenizing:  83%|███████████████████████████████████████████████████████████████████▌             | 25000/30000 [00:08<00:01, 3039.16 examples/s]Tokenizing:  87%|██████████████████████████████████████████████████████████████████████▏          | 26000/30000 [00:08<00:01, 3055.15 examples/s]Tokenizing:  90%|████████████████████████████████████████████████████████████████████████▉        | 27000/30000 [00:08<00:00, 3072.59 examples/s]Tokenizing:  93%|███████████████████████████████████████████████████████████████████████████▌     | 28000/30000 [00:09<00:00, 3091.28 examples/s]Tokenizing:  97%|██████████████████████████████████████████████████████████████████████████████▎  | 29000/30000 [00:09<00:00, 2628.29 examples/s]Tokenizing: 100%|█████████████████████████████████████████████████████████████████████████████████| 30000/30000 [00:10<00:00, 2764.25 examples/s]Tokenizing: 100%|█████████████████████████████████████████████████████████████████████████████████| 30000/30000 [00:10<00:00, 2983.83 examples/s]
Tokenizing:   0%|                                                                                                | 0/2000 [00:00<?, ? examples/s]Tokenizing:  50%|█████████████████████████████████████████▌                                         | 1000/2000 [00:00<00:00, 3174.59 examples/s]Tokenizing: 100%|███████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 3169.03 examples/s]Tokenizing: 100%|███████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 3156.16 examples/s]
The model is already on multiple devices. Skipping the move to device specified in `args`.
  0%|                                                                                                                   | 0/5625 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/scripts/2_wmt_training.py", line 306, in <module>
    main()
  File "/sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/scripts/2_wmt_training.py", line 294, in main
    trainer.train()
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/accelerate/accelerator.py", line 2473, in backward
    loss.backward(**kwargs)
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/sc/home/sandeep.uprety/.local/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
  0%|                                                                                                                   | 0/5625 [00:00<?, ?it/s]
