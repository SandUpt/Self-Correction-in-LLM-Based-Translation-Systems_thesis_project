Using GPU inside JupyterLab session
stdout log: ../logs/2_wmt_training_20251129_004410.log
stderr log: ../logs/2_wmt_training_20251129_004410.err
Start Time: Sat Nov 29 00:44:10 CET 2025
Node: gx04
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sat Nov 29 00:44:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:81:00.0 Off |                    0 |
| N/A   32C    P0             38W /  250W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
============================================================
WMT Fine-tuning
Model: mistral
Language Pair: zh_en
============================================================

Loading training data
  Train: ../../data/processed/zh_en/train_news_un_balanced_30000.tsv
  Val: ../../data/processed/zh_en/mix2k_dev.tsv
  Train samples: 30000
  Val samples: 2000

Loading model: mistralai/Mistral-7B-v0.1

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.0484, 'grad_norm': 2.45611310005188, 'learning_rate': 1.7375886524822697e-05, 'epoch': 0.03}
{'loss': 0.6255, 'grad_norm': 2.241424560546875, 'learning_rate': 3.5106382978723407e-05, 'epoch': 0.05}
{'loss': 0.6033, 'grad_norm': 1.6647404432296753, 'learning_rate': 5.283687943262412e-05, 'epoch': 0.08}
{'loss': 0.5614, 'grad_norm': 1.7761989831924438, 'learning_rate': 7.056737588652482e-05, 'epoch': 0.11}
{'loss': 0.5576, 'grad_norm': 1.3487167358398438, 'learning_rate': 8.829787234042553e-05, 'epoch': 0.13}
{'loss': 0.5727, 'grad_norm': 1.4438267946243286, 'learning_rate': 9.999750216565724e-05, 'epoch': 0.16}
{'loss': 0.5664, 'grad_norm': 1.3212437629699707, 'learning_rate': 9.996120615348041e-05, 'epoch': 0.19}
{'loss': 0.5844, 'grad_norm': 1.6813615560531616, 'learning_rate': 9.988173129447252e-05, 'epoch': 0.21}
{'loss': 0.568, 'grad_norm': 1.538673996925354, 'learning_rate': 9.975914627458066e-05, 'epoch': 0.24}
{'loss': 0.5504, 'grad_norm': 1.3455936908721924, 'learning_rate': 9.959355703760014e-05, 'epoch': 0.27}
{'eval_loss': 0.5827239751815796, 'eval_runtime': 53.198, 'eval_samples_per_second': 37.595, 'eval_steps_per_second': 18.798, 'epoch': 0.27}
{'loss': 0.5731, 'grad_norm': 1.4867658615112305, 'learning_rate': 9.93851066936128e-05, 'epoch': 0.29}
{'loss': 0.5329, 'grad_norm': 0.907328724861145, 'learning_rate': 9.913397539530444e-05, 'epoch': 0.32}
{'loss': 0.5367, 'grad_norm': 1.4648946523666382, 'learning_rate': 9.884038018226838e-05, 'epoch': 0.35}
{'loss': 0.5179, 'grad_norm': 1.2090140581130981, 'learning_rate': 9.850457479342942e-05, 'epoch': 0.37}
{'loss': 0.5586, 'grad_norm': 1.3935786485671997, 'learning_rate': 9.812684944775082e-05, 'epoch': 0.4}
{'loss': 0.5573, 'grad_norm': 1.3451989889144897, 'learning_rate': 9.770753059341306e-05, 'epoch': 0.43}
{'loss': 0.5311, 'grad_norm': 1.5966448783874512, 'learning_rate': 9.724698062568196e-05, 'epoch': 0.45}
{'loss': 0.5222, 'grad_norm': 1.2319973707199097, 'learning_rate': 9.674559757370947e-05, 'epoch': 0.48}
{'loss': 0.5427, 'grad_norm': 1.331871509552002, 'learning_rate': 9.620381475653791e-05, 'epoch': 0.51}
{'loss': 0.5517, 'grad_norm': 1.0976905822753906, 'learning_rate': 9.562210040860518e-05, 'epoch': 0.53}
{'eval_loss': 0.5615265965461731, 'eval_runtime': 53.3768, 'eval_samples_per_second': 37.469, 'eval_steps_per_second': 18.735, 'epoch': 0.53}
{'loss': 0.521, 'grad_norm': 1.4577926397323608, 'learning_rate': 9.50009572750742e-05, 'epoch': 0.56}
{'loss': 0.5389, 'grad_norm': 1.417912244796753, 'learning_rate': 9.434092217733677e-05, 'epoch': 0.59}
{'loss': 0.511, 'grad_norm': 1.167643427848816, 'learning_rate': 9.364256554906699e-05, 'epoch': 0.61}
{'loss': 0.5161, 'grad_norm': 1.9457069635391235, 'learning_rate': 9.290649094322538e-05, 'epoch': 0.64}
{'loss': 0.5352, 'grad_norm': 1.7957361936569214, 'learning_rate': 9.213333451043981e-05, 'epoch': 0.67}
{'loss': 0.5211, 'grad_norm': 1.202733039855957, 'learning_rate': 9.132376444921379e-05, 'epoch': 0.69}
{'loss': 0.5001, 'grad_norm': 1.3697370290756226, 'learning_rate': 9.047848042843774e-05, 'epoch': 0.72}
{'loss': 0.5194, 'grad_norm': 1.4608725309371948, 'learning_rate': 8.959821298270183e-05, 'epoch': 0.75}
{'loss': 0.5273, 'grad_norm': 1.4292031526565552, 'learning_rate': 8.868372288093334e-05, 'epoch': 0.77}
{'loss': 0.5221, 'grad_norm': 1.9190926551818848, 'learning_rate': 8.773580046890396e-05, 'epoch': 0.8}
{'eval_loss': 0.552983283996582, 'eval_runtime': 52.6446, 'eval_samples_per_second': 37.991, 'eval_steps_per_second': 18.995, 'epoch': 0.8}
{'loss': 0.5075, 'grad_norm': 1.3041796684265137, 'learning_rate': 8.675526498617548e-05, 'epoch': 0.83}
{'loss': 0.5468, 'grad_norm': 1.1169779300689697, 'learning_rate': 8.57429638580741e-05, 'epoch': 0.85}
{'loss': 0.5233, 'grad_norm': 1.5250202417373657, 'learning_rate': 8.469977196330519e-05, 'epoch': 0.88}
{'loss': 0.54, 'grad_norm': 1.2937345504760742, 'learning_rate': 8.362659087784153e-05, 'epoch': 0.91}
{'loss': 0.5465, 'grad_norm': 1.1537836790084839, 'learning_rate': 8.252434809573857e-05, 'epoch': 0.93}
{'loss': 0.5051, 'grad_norm': 1.2223232984542847, 'learning_rate': 8.139399622755006e-05, 'epoch': 0.96}
{'loss': 0.5132, 'grad_norm': 0.9415689706802368, 'learning_rate': 8.023651217703671e-05, 'epoch': 0.99}
{'loss': 0.5023, 'grad_norm': 1.1230822801589966, 'learning_rate': 7.905289629687964e-05, 'epoch': 1.01}
{'loss': 0.4138, 'grad_norm': 1.2235804796218872, 'learning_rate': 7.784417152412801e-05, 'epoch': 1.04}
{'loss': 0.418, 'grad_norm': 1.2946988344192505, 'learning_rate': 7.661138249612833e-05, 'epoch': 1.07}
{'eval_loss': 0.5506938695907593, 'eval_runtime': 53.1441, 'eval_samples_per_second': 37.634, 'eval_steps_per_second': 18.817, 'epoch': 1.07}
{'loss': 0.4233, 'grad_norm': 1.0303406715393066, 'learning_rate': 7.535559464769916e-05, 'epoch': 1.09}
{'loss': 0.4222, 'grad_norm': 1.1672396659851074, 'learning_rate': 7.407789329033188e-05, 'epoch': 1.12}
{'loss': 0.4172, 'grad_norm': 1.4281340837478638, 'learning_rate': 7.277938267421285e-05, 'epoch': 1.15}
{'loss': 0.416, 'grad_norm': 1.7463432550430298, 'learning_rate': 7.146118503387795e-05, 'epoch': 1.17}
{'loss': 0.4031, 'grad_norm': 1.4599385261535645, 'learning_rate': 7.012443961832434e-05, 'epoch': 1.2}
{'loss': 0.4046, 'grad_norm': 1.6144410371780396, 'learning_rate': 6.877030170641722e-05, 'epoch': 1.23}
{'loss': 0.4424, 'grad_norm': 1.4154512882232666, 'learning_rate': 6.73999416084431e-05, 'epoch': 1.25}
{'loss': 0.4211, 'grad_norm': 1.266618013381958, 'learning_rate': 6.601454365467196e-05, 'epoch': 1.28}
{'loss': 0.4131, 'grad_norm': 1.4444270133972168, 'learning_rate': 6.46153051718029e-05, 'epoch': 1.31}
{'loss': 0.4089, 'grad_norm': 1.3764643669128418, 'learning_rate': 6.320343544817749e-05, 'epoch': 1.33}
{'eval_loss': 0.5528239607810974, 'eval_runtime': 53.3157, 'eval_samples_per_second': 37.512, 'eval_steps_per_second': 18.756, 'epoch': 1.33}
{'loss': 0.4124, 'grad_norm': 1.6476751565933228, 'learning_rate': 6.178015468865534e-05, 'epoch': 1.36}
{'loss': 0.4341, 'grad_norm': 1.292175531387329, 'learning_rate': 6.034669296005522e-05, 'epoch': 1.39}
{'loss': 0.4394, 'grad_norm': 1.8642749786376953, 'learning_rate': 5.8904289128072745e-05, 'epoch': 1.41}
{'loss': 0.4197, 'grad_norm': 1.728386402130127, 'learning_rate': 5.745418978659398e-05, 'epoch': 1.44}
{'loss': 0.3978, 'grad_norm': 1.6721163988113403, 'learning_rate': 5.599764818032969e-05, 'epoch': 1.47}
{'loss': 0.4526, 'grad_norm': 1.3665598630905151, 'learning_rate': 5.453592312170179e-05, 'epoch': 1.49}
{'loss': 0.446, 'grad_norm': 1.2342734336853027, 'learning_rate': 5.307027790291787e-05, 'epoch': 1.52}
{'loss': 0.4372, 'grad_norm': 1.3096563816070557, 'learning_rate': 5.160197920417409e-05, 'epoch': 1.55}
{'loss': 0.4274, 'grad_norm': 1.1472054719924927, 'learning_rate': 5.013229599892998e-05, 'epoch': 1.57}
{'loss': 0.411, 'grad_norm': 1.2263853549957275, 'learning_rate': 4.866249845720133e-05, 'epoch': 1.6}
{'eval_loss': 0.5450695753097534, 'eval_runtime': 52.8815, 'eval_samples_per_second': 37.82, 'eval_steps_per_second': 18.91, 'epoch': 1.6}
{'loss': 0.4326, 'grad_norm': 1.5174213647842407, 'learning_rate': 4.7193856847818996e-05, 'epoch': 1.63}
{'loss': 0.4352, 'grad_norm': 1.4530425071716309, 'learning_rate': 4.5727640440602174e-05, 'epoch': 1.65}
{'loss': 0.4207, 'grad_norm': 1.4885586500167847, 'learning_rate': 4.426511640939515e-05, 'epoch': 1.68}
{'loss': 0.4234, 'grad_norm': 1.5760884284973145, 'learning_rate': 4.2807548736915565e-05, 'epoch': 1.71}
{'loss': 0.4143, 'grad_norm': 1.0438122749328613, 'learning_rate': 4.13561971223605e-05, 'epoch': 1.73}
{'loss': 0.3806, 'grad_norm': 1.056230068206787, 'learning_rate': 3.991231589271458e-05, 'epoch': 1.76}
{'loss': 0.4312, 'grad_norm': 1.6721062660217285, 'learning_rate': 3.8477152918701056e-05, 'epoch': 1.79}
{'loss': 0.4142, 'grad_norm': 1.312085747718811, 'learning_rate': 3.7051948536312654e-05, 'epoch': 1.81}
{'loss': 0.4088, 'grad_norm': 1.3642234802246094, 'learning_rate': 3.5637934474854334e-05, 'epoch': 1.84}
{'loss': 0.4178, 'grad_norm': 1.418262243270874, 'learning_rate': 3.423633279242433e-05, 'epoch': 1.87}
{'eval_loss': 0.5437020659446716, 'eval_runtime': 53.0464, 'eval_samples_per_second': 37.703, 'eval_steps_per_second': 18.851, 'epoch': 1.87}
{'loss': 0.4267, 'grad_norm': 1.4016754627227783, 'learning_rate': 3.2848354819753455e-05, 'epoch': 1.89}
{'loss': 0.3944, 'grad_norm': 1.3508504629135132, 'learning_rate': 3.147520011331566e-05, 'epoch': 1.92}
{'loss': 0.4099, 'grad_norm': 1.693893313407898, 'learning_rate': 3.01180554186143e-05, 'epoch': 1.95}
{'loss': 0.4165, 'grad_norm': 1.3300107717514038, 'learning_rate': 2.877809364454032e-05, 'epoch': 1.97}
{'loss': 0.4122, 'grad_norm': 1.5474447011947632, 'learning_rate': 2.7456472849688708e-05, 'epoch': 2.0}
{'loss': 0.3231, 'grad_norm': 1.5080711841583252, 'learning_rate': 2.6154335241509287e-05, 'epoch': 2.03}
{'loss': 0.3067, 'grad_norm': 1.7507991790771484, 'learning_rate': 2.4872806189156745e-05, 'epoch': 2.05}
{'loss': 0.3092, 'grad_norm': 1.68772292137146, 'learning_rate': 2.3612993250893185e-05, 'epoch': 2.08}
{'loss': 0.3287, 'grad_norm': 1.6361087560653687, 'learning_rate': 2.2375985216883755e-05, 'epoch': 2.11}
{'loss': 0.2925, 'grad_norm': 1.3296318054199219, 'learning_rate': 2.1162851168212354e-05, 'epoch': 2.13}
{'eval_loss': 0.5785232186317444, 'eval_runtime': 53.0255, 'eval_samples_per_second': 37.718, 'eval_steps_per_second': 18.859, 'epoch': 2.13}
{'loss': 0.2983, 'grad_norm': 1.7166885137557983, 'learning_rate': 1.9974639552931145e-05, 'epoch': 2.16}
{'loss': 0.303, 'grad_norm': 1.4018750190734863, 'learning_rate': 1.881237727994181e-05, 'epoch': 2.19}
{'loss': 0.2886, 'grad_norm': 1.3614671230316162, 'learning_rate': 1.7677068831492223e-05, 'epoch': 2.21}
{'loss': 0.3074, 'grad_norm': 1.7463724613189697, 'learning_rate': 1.6569695395055107e-05, 'epoch': 2.24}
{'loss': 0.3168, 'grad_norm': 1.933592438697815, 'learning_rate': 1.549121401533935e-05, 'epoch': 2.27}
{'loss': 0.3186, 'grad_norm': 1.5484793186187744, 'learning_rate': 1.444255676716637e-05, 'epoch': 2.29}
{'loss': 0.3095, 'grad_norm': 2.056375741958618, 'learning_rate': 1.3424629949926931e-05, 'epoch': 2.32}
{'loss': 0.3261, 'grad_norm': 1.7213749885559082, 'learning_rate': 1.2438313304314048e-05, 'epoch': 2.35}
{'loss': 0.3029, 'grad_norm': 2.3725943565368652, 'learning_rate': 1.1484459252009421e-05, 'epoch': 2.37}
{'loss': 0.3004, 'grad_norm': 1.1839739084243774, 'learning_rate': 1.0563892158980033e-05, 'epoch': 2.4}
{'eval_loss': 0.5795459151268005, 'eval_runtime': 52.8749, 'eval_samples_per_second': 37.825, 'eval_steps_per_second': 18.913, 'epoch': 2.4}
{'loss': 0.3102, 'grad_norm': 2.305710554122925, 'learning_rate': 9.677407623022039e-06, 'epoch': 2.43}
{'loss': 0.3152, 'grad_norm': 1.4384459257125854, 'learning_rate': 8.825771786167269e-06, 'epoch': 2.45}
{'loss': 0.3035, 'grad_norm': 1.9236031770706177, 'learning_rate': 8.009720672547e-06, 'epoch': 2.48}
{'loss': 0.3154, 'grad_norm': 1.8017475605010986, 'learning_rate': 7.229959552284849e-06, 'epoch': 2.51}
{'loss': 0.2922, 'grad_norm': 1.6000440120697021, 'learning_rate': 6.487162331968943e-06, 'epoch': 2.53}
{'loss': 0.3145, 'grad_norm': 1.8927966356277466, 'learning_rate': 5.781970972229766e-06, 'epoch': 2.56}
{'loss': 0.2985, 'grad_norm': 1.708153247833252, 'learning_rate': 5.114994932927353e-06, 'epoch': 2.59}
{'loss': 0.3258, 'grad_norm': 2.1807096004486084, 'learning_rate': 4.486810646427092e-06, 'epoch': 2.61}
{'loss': 0.2912, 'grad_norm': 1.6300798654556274, 'learning_rate': 3.897961019419516e-06, 'epoch': 2.64}
{'loss': 0.3068, 'grad_norm': 1.8058662414550781, 'learning_rate': 3.3489549637145958e-06, 'epoch': 2.67}
{'eval_loss': 0.5816329717636108, 'eval_runtime': 53.0098, 'eval_samples_per_second': 37.729, 'eval_steps_per_second': 18.864, 'epoch': 2.67}
{'loss': 0.3032, 'grad_norm': 2.283952236175537, 'learning_rate': 2.8402669564159323e-06, 'epoch': 2.69}
{'loss': 0.3123, 'grad_norm': 1.6610281467437744, 'learning_rate': 2.3723366298551652e-06, 'epoch': 2.72}
{'loss': 0.3108, 'grad_norm': 1.7482792139053345, 'learning_rate': 1.945568391640773e-06, 'epoch': 2.75}
{'loss': 0.2988, 'grad_norm': 1.5139554738998413, 'learning_rate': 1.560331075149879e-06, 'epoch': 2.77}
{'loss': 0.2997, 'grad_norm': 2.5045952796936035, 'learning_rate': 1.2169576207648857e-06, 'epoch': 2.8}
{'loss': 0.3008, 'grad_norm': 1.8559465408325195, 'learning_rate': 9.15744788130618e-07, 'epoch': 2.83}
{'loss': 0.2994, 'grad_norm': 1.7645390033721924, 'learning_rate': 6.56952899680513e-07, 'epoch': 2.85}
{'loss': 0.3091, 'grad_norm': 1.4769411087036133, 'learning_rate': 4.408056156536555e-07, 'epoch': 2.88}
{'loss': 0.2885, 'grad_norm': 1.6303602457046509, 'learning_rate': 2.6748974079692235e-07, 'epoch': 2.91}
{'loss': 0.3013, 'grad_norm': 1.3970623016357422, 'learning_rate': 1.3715506291951395e-07, 'epoch': 2.93}
{'eval_loss': 0.5827398300170898, 'eval_runtime': 53.0871, 'eval_samples_per_second': 37.674, 'eval_steps_per_second': 18.837, 'epoch': 2.93}
{'loss': 0.3254, 'grad_norm': 1.8160308599472046, 'learning_rate': 4.991422343914587e-08, 'epoch': 2.96}
{'loss': 0.3011, 'grad_norm': 1.7088956832885742, 'learning_rate': 5.842620032053825e-09, 'epoch': 2.99}
{'train_runtime': 9671.0067, 'train_samples_per_second': 9.306, 'train_steps_per_second': 0.582, 'train_loss': 0.42760169050428604, 'epoch': 3.0}

Saving adapter to ../models/wmt_adapters/mistral_zh_en

Training complete.
Adapter saved to: ../models/wmt_adapters/mistral_zh_en
Done at: Sat Nov 29 03:25:58 CET 2025
