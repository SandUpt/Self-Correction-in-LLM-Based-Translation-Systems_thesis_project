Using GPU inside JupyterLab session
stdout log: ../logs/2_wmt_training_20251129_010424.log
stderr log: ../logs/2_wmt_training_20251129_010424.err
Start Time: Sat Nov 29 01:04:24 CET 2025
Node: gx06
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sat Nov 29 01:04:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:81:00.0 Off |                    0 |
| N/A   32C    P0             37W /  250W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
============================================================
WMT Fine-tuning
Model: llama2
Language Pair: de_en
============================================================

Loading training data
  Train: ../../data/processed/de_en/train_europarl_newstest_balanced_26000.tsv
  Val: ../../data/processed/de_en/mix2k_dev.tsv
  Train samples: 26000
  Val samples: 1999

Loading model: meta-llama/Llama-2-7b-hf

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.2484

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.626, 'grad_norm': 1.0246267318725586, 'learning_rate': 2.0081967213114755e-05, 'epoch': 0.03}
{'loss': 0.8856, 'grad_norm': 0.5364629626274109, 'learning_rate': 4.057377049180328e-05, 'epoch': 0.06}
{'loss': 0.8386, 'grad_norm': 0.664587140083313, 'learning_rate': 6.10655737704918e-05, 'epoch': 0.09}
{'loss': 0.8103, 'grad_norm': 0.48802077770233154, 'learning_rate': 8.155737704918032e-05, 'epoch': 0.12}
{'loss': 0.8175, 'grad_norm': 0.6055290699005127, 'learning_rate': 9.999971237291203e-05, 'epoch': 0.15}
{'loss': 0.8381, 'grad_norm': 0.5291378498077393, 'learning_rate': 9.996520112627602e-05, 'epoch': 0.18}
{'loss': 0.7973, 'grad_norm': 0.5073606967926025, 'learning_rate': 9.987320995457625e-05, 'epoch': 0.22}
{'loss': 0.7916, 'grad_norm': 0.5661869049072266, 'learning_rate': 9.972384468437874e-05, 'epoch': 0.25}
{'loss': 0.7957, 'grad_norm': 0.46296578645706177, 'learning_rate': 9.951727714536386e-05, 'epoch': 0.28}
{'loss': 0.8317, 'grad_norm': 0.5505102872848511, 'learning_rate': 9.925374497265355e-05, 'epoch': 0.31}
{'eval_loss': 0.7848919630050659, 'eval_runtime': 47.5343, 'eval_samples_per_second': 42.054, 'eval_steps_per_second': 21.037, 'epoch': 0.31}
{'loss': 0.8027, 'grad_norm': 0.4906916916370392, 'learning_rate': 9.893355133343611e-05, 'epoch': 0.34}
{'loss': 0.7949, 'grad_norm': 0.450988233089447, 'learning_rate': 9.8557064578203e-05, 'epoch': 0.37}
{'loss': 0.7857, 'grad_norm': 0.4095750153064728, 'learning_rate': 9.812471781699857e-05, 'epoch': 0.4}
{'loss': 0.7715, 'grad_norm': 0.43039146065711975, 'learning_rate': 9.76370084211708e-05, 'epoch': 0.43}
{'loss': 0.8, 'grad_norm': 0.42710259556770325, 'learning_rate': 9.709449745119565e-05, 'epoch': 0.46}
{'loss': 0.8192, 'grad_norm': 0.491473525762558, 'learning_rate': 9.649780901123357e-05, 'epoch': 0.49}
{'loss': 0.7735, 'grad_norm': 0.4556505084037781, 'learning_rate': 9.58476295311606e-05, 'epoch': 0.52}
{'loss': 0.7837, 'grad_norm': 0.4270026981830597, 'learning_rate': 9.51447069769e-05, 'epoch': 0.55}
{'loss': 0.7836, 'grad_norm': 0.4630260467529297, 'learning_rate': 9.438984998996298e-05, 'epoch': 0.58}
{'loss': 0.794, 'grad_norm': 0.37622711062431335, 'learning_rate': 9.358392695718805e-05, 'epoch': 0.62}
{'eval_loss': 0.7697737812995911, 'eval_runtime': 47.4789, 'eval_samples_per_second': 42.103, 'eval_steps_per_second': 21.062, 'epoch': 0.62}
{'loss': 0.7738, 'grad_norm': 0.4681991934776306, 'learning_rate': 9.272786501174964e-05, 'epoch': 0.65}
{'loss': 0.7795, 'grad_norm': 0.37938880920410156, 'learning_rate': 9.182264896658486e-05, 'epoch': 0.68}
{'loss': 0.7799, 'grad_norm': 0.4940265715122223, 'learning_rate': 9.08693201814655e-05, 'epoch': 0.71}
{'loss': 0.7985, 'grad_norm': 0.5105140209197998, 'learning_rate': 8.986897536501864e-05, 'epoch': 0.74}
{'loss': 0.7835, 'grad_norm': 0.5194947123527527, 'learning_rate': 8.8822765313074e-05, 'epoch': 0.77}
{'loss': 0.7659, 'grad_norm': 0.41869404911994934, 'learning_rate': 8.77318935847894e-05, 'epoch': 0.8}
{'loss': 0.7801, 'grad_norm': 0.4482268989086151, 'learning_rate': 8.659761511807727e-05, 'epoch': 0.83}
{'loss': 0.7781, 'grad_norm': 0.4276168644428253, 'learning_rate': 8.542123478592518e-05, 'epoch': 0.86}
{'loss': 0.7931, 'grad_norm': 0.5375182032585144, 'learning_rate': 8.420410589527104e-05, 'epoch': 0.89}
{'loss': 0.7741, 'grad_norm': 0.45757898688316345, 'learning_rate': 8.294762863015995e-05, 'epoch': 0.92}
{'eval_loss': 0.7589661478996277, 'eval_runtime': 47.4565, 'eval_samples_per_second': 42.123, 'eval_steps_per_second': 21.072, 'epoch': 0.92}
{'loss': 0.7706, 'grad_norm': 0.44504514336586, 'learning_rate': 8.165324844097368e-05, 'epoch': 0.95}
{'loss': 0.7732, 'grad_norm': 0.4582686126232147, 'learning_rate': 8.032245438158576e-05, 'epoch': 0.98}
{'loss': 0.7445, 'grad_norm': 0.4425172507762909, 'learning_rate': 7.895677739635517e-05, 'epoch': 1.02}
{'loss': 0.7234, 'grad_norm': 0.5501317977905273, 'learning_rate': 7.755778855892922e-05, 'epoch': 1.05}
{'loss': 0.7319, 'grad_norm': 0.4966755211353302, 'learning_rate': 7.612709726488175e-05, 'epoch': 1.08}
{'loss': 0.7194, 'grad_norm': 0.5515857338905334, 'learning_rate': 7.466634938026594e-05, 'epoch': 1.11}
{'loss': 0.7285, 'grad_norm': 0.4553854167461395, 'learning_rate': 7.317722534821117e-05, 'epoch': 1.14}
{'loss': 0.7372, 'grad_norm': 0.43756231665611267, 'learning_rate': 7.166143825574297e-05, 'epoch': 1.17}
{'loss': 0.7341, 'grad_norm': 0.5932044982910156, 'learning_rate': 7.012073186304886e-05, 'epoch': 1.2}
{'loss': 0.7017, 'grad_norm': 0.5492939949035645, 'learning_rate': 6.855687859745827e-05, 'epoch': 1.23}
{'eval_loss': 0.7585421204566956, 'eval_runtime': 47.9152, 'eval_samples_per_second': 41.72, 'eval_steps_per_second': 20.87, 'epoch': 1.23}
{'loss': 0.7411, 'grad_norm': 0.5328091382980347, 'learning_rate': 6.697167751444366e-05, 'epoch': 1.26}
{'loss': 0.7279, 'grad_norm': 0.6399336457252502, 'learning_rate': 6.536695222798851e-05, 'epoch': 1.29}
{'loss': 0.7286, 'grad_norm': 0.5265846848487854, 'learning_rate': 6.374454881270344e-05, 'epoch': 1.32}
{'loss': 0.7442, 'grad_norm': 0.6870509386062622, 'learning_rate': 6.210633368010352e-05, 'epoch': 1.35}
{'loss': 0.7055, 'grad_norm': 0.5404271483421326, 'learning_rate': 6.045419143148997e-05, 'epoch': 1.38}
{'loss': 0.7178, 'grad_norm': 0.49755364656448364, 'learning_rate': 5.879002268990653e-05, 'epoch': 1.42}
{'loss': 0.7239, 'grad_norm': 0.530316948890686, 'learning_rate': 5.7115741913664264e-05, 'epoch': 1.45}
{'loss': 0.6933, 'grad_norm': 0.49553653597831726, 'learning_rate': 5.5433275193950326e-05, 'epoch': 1.48}
{'loss': 0.7344, 'grad_norm': 0.5354853272438049, 'learning_rate': 5.3744558039054296e-05, 'epoch': 1.51}
{'loss': 0.7005, 'grad_norm': 0.598698079586029, 'learning_rate': 5.2051533147761155e-05, 'epoch': 1.54}
{'eval_loss': 0.7576363682746887, 'eval_runtime': 47.4686, 'eval_samples_per_second': 42.112, 'eval_steps_per_second': 21.067, 'epoch': 1.54}
{'loss': 0.7184, 'grad_norm': 0.49495750665664673, 'learning_rate': 5.035614817447212e-05, 'epoch': 1.57}
{'loss': 0.7285, 'grad_norm': 0.5583341717720032, 'learning_rate': 4.866035348862476e-05, 'epoch': 1.6}
{'loss': 0.7165, 'grad_norm': 0.5294485092163086, 'learning_rate': 4.696609993098965e-05, 'epoch': 1.63}
{'loss': 0.7052, 'grad_norm': 0.4807742238044739, 'learning_rate': 4.527533656942472e-05, 'epoch': 1.66}
{'loss': 0.7324, 'grad_norm': 0.5379554033279419, 'learning_rate': 4.359000845666936e-05, 'epoch': 1.69}
{'loss': 0.704, 'grad_norm': 0.6070799231529236, 'learning_rate': 4.191205439275729e-05, 'epoch': 1.72}
{'loss': 0.7436, 'grad_norm': 0.6559717059135437, 'learning_rate': 4.02434046946227e-05, 'epoch': 1.75}
{'loss': 0.7126, 'grad_norm': 0.5707792043685913, 'learning_rate': 3.858597897546526e-05, 'epoch': 1.78}
{'loss': 0.7088, 'grad_norm': 0.5626094341278076, 'learning_rate': 3.6941683936428716e-05, 'epoch': 1.82}
{'loss': 0.7075, 'grad_norm': 0.5569483041763306, 'learning_rate': 3.531241117313359e-05, 'epoch': 1.85}
{'eval_loss': 0.7524351477622986, 'eval_runtime': 47.5637, 'eval_samples_per_second': 42.028, 'eval_steps_per_second': 21.024, 'epoch': 1.85}
{'loss': 0.7165, 'grad_norm': 0.5058115124702454, 'learning_rate': 3.370003499958703e-05, 'epoch': 1.88}
{'loss': 0.7186, 'grad_norm': 0.6109083890914917, 'learning_rate': 3.210641029197368e-05, 'epoch': 1.91}
{'loss': 0.7208, 'grad_norm': 0.5762142539024353, 'learning_rate': 3.053337035480765e-05, 'epoch': 1.94}
{'loss': 0.7035, 'grad_norm': 0.5425072908401489, 'learning_rate': 2.8982724811900564e-05, 'epoch': 1.97}
{'loss': 0.716, 'grad_norm': 0.5619975328445435, 'learning_rate': 2.7456257524571888e-05, 'epoch': 2.0}
{'loss': 0.6682, 'grad_norm': 0.5810692310333252, 'learning_rate': 2.5955724539496262e-05, 'epoch': 2.03}
{'loss': 0.66, 'grad_norm': 0.6680434942245483, 'learning_rate': 2.4482852068549046e-05, 'epoch': 2.06}
{'loss': 0.6502, 'grad_norm': 0.5358119010925293, 'learning_rate': 2.3039334502973542e-05, 'epoch': 2.09}
{'loss': 0.6685, 'grad_norm': 0.6072347164154053, 'learning_rate': 2.1626832464154785e-05, 'epoch': 2.12}
{'loss': 0.6461, 'grad_norm': 0.694184422492981, 'learning_rate': 2.024697089324208e-05, 'epoch': 2.15}
{'eval_loss': 0.7649136781692505, 'eval_runtime': 48.0822, 'eval_samples_per_second': 41.575, 'eval_steps_per_second': 20.798, 'epoch': 2.15}
{'loss': 0.6458, 'grad_norm': 0.7807544469833374, 'learning_rate': 1.89013371818181e-05, 'epoch': 2.18}
{'loss': 0.6599, 'grad_norm': 0.7115089893341064, 'learning_rate': 1.7591479345764973e-05, 'epoch': 2.22}
{'loss': 0.656, 'grad_norm': 0.9259116649627686, 'learning_rate': 1.6318904244428028e-05, 'epoch': 2.25}
{'loss': 0.6515, 'grad_norm': 0.6607456803321838, 'learning_rate': 1.5085075847126213e-05, 'epoch': 2.28}
{'loss': 0.6634, 'grad_norm': 0.6797041893005371, 'learning_rate': 1.389141354900294e-05, 'epoch': 2.31}
{'loss': 0.6377, 'grad_norm': 0.6797680258750916, 'learning_rate': 1.2739290538155147e-05, 'epoch': 2.34}
{'loss': 0.6531, 'grad_norm': 0.7867493033409119, 'learning_rate': 1.1630032215918862e-05, 'epoch': 2.37}
{'loss': 0.6541, 'grad_norm': 0.6980944871902466, 'learning_rate': 1.0564914672128639e-05, 'epoch': 2.4}
{'loss': 0.6526, 'grad_norm': 0.5864299535751343, 'learning_rate': 9.54516321710488e-06, 'epoch': 2.43}
{'loss': 0.677, 'grad_norm': 0.741034984588623, 'learning_rate': 8.57195097205789e-06, 'epoch': 2.46}
{'eval_loss': 0.7635994553565979, 'eval_runtime': 47.4571, 'eval_samples_per_second': 42.122, 'eval_steps_per_second': 21.072, 'epoch': 2.46}
{'loss': 0.6456, 'grad_norm': 0.5935344099998474, 'learning_rate': 7.64639751953023e-06, 'epoch': 2.49}
{'loss': 0.6302, 'grad_norm': 0.7381717562675476, 'learning_rate': 6.769567615429912e-06, 'epoch': 2.52}
{'loss': 0.6495, 'grad_norm': 0.6902995705604553, 'learning_rate': 5.942469964136055e-06, 'epoch': 2.55}
{'loss': 0.6213, 'grad_norm': 0.6409229636192322, 'learning_rate': 5.166056058086349e-06, 'epoch': 2.58}
{'loss': 0.6585, 'grad_norm': 0.6632272005081177, 'learning_rate': 4.441219083180786e-06, 'epoch': 2.62}
{'loss': 0.6817, 'grad_norm': 0.7099536061286926, 'learning_rate': 3.768792891261497e-06, 'epoch': 2.65}
{'loss': 0.6573, 'grad_norm': 0.778481662273407, 'learning_rate': 3.1495510408502404e-06, 'epoch': 2.68}
{'loss': 0.6391, 'grad_norm': 0.6861171722412109, 'learning_rate': 2.584205907247339e-06, 'epoch': 2.71}
{'loss': 0.6235, 'grad_norm': 0.7060630917549133, 'learning_rate': 2.0734078630157304e-06, 'epoch': 2.74}
{'loss': 0.6439, 'grad_norm': 0.7600111961364746, 'learning_rate': 1.6177445297929527e-06, 'epoch': 2.77}
{'eval_loss': 0.7647590637207031, 'eval_runtime': 47.562, 'eval_samples_per_second': 42.029, 'eval_steps_per_second': 21.025, 'epoch': 2.77}
{'loss': 0.6501, 'grad_norm': 0.7161873579025269, 'learning_rate': 1.2177401022916756e-06, 'epoch': 2.8}
{'loss': 0.6396, 'grad_norm': 0.7206705808639526, 'learning_rate': 8.738547452665446e-07, 'epoch': 2.83}
{'loss': 0.6377, 'grad_norm': 0.711741030216217, 'learning_rate': 5.864840641410907e-07, 'epoch': 2.86}
{'loss': 0.6725, 'grad_norm': 0.8357288837432861, 'learning_rate': 3.5595864990352056e-07, 'epoch': 2.89}
{'loss': 0.6568, 'grad_norm': 0.6830883026123047, 'learning_rate': 1.825436987951512e-07, 'epoch': 2.92}
{'loss': 0.645, 'grad_norm': 0.6936382055282593, 'learning_rate': 6.643870722889411e-08, 'epoch': 2.95}
{'loss': 0.6664, 'grad_norm': 0.7733055353164673, 'learning_rate': 7.777242288725672e-09, 'epoch': 2.98}
{'train_runtime': 7500.0104, 'train_samples_per_second': 10.4, 'train_steps_per_second': 0.65, 'train_loss': 0.7309504903157552, 'epoch': 3.0}

Saving adapter to ../models/wmt_adapters/llama2_de_en

Training complete.
Adapter saved to: ../models/wmt_adapters/llama2_de_en
Done at: Sat Nov 29 03:10:35 CET 2025
