Using GPU inside JupyterLab session
stdout log: ../logs/2_wmt_training_20251129_010807.log
stderr log: ../logs/2_wmt_training_20251129_010807.err
Start Time: Sat Nov 29 01:08:07 CET 2025
Node: gx06
Working Directory: /sc/home/sandeep.uprety/thesis_project/self_correction_llm_based_translation_thesis/thesis_project/sh_files
Sat Nov 29 01:08:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:25:00.0 Off |                    0 |
| N/A   32C    P0             38W /  250W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
============================================================
WMT Fine-tuning
Model: llama3
Language Pair: de_en
============================================================

Loading training data
  Train: ../../data/processed/de_en/train_europarl_newstest_balanced_26000.tsv
  Val: ../../data/processed/de_en/mix2k_dev.tsv
  Train samples: 26000
  Val samples: 1999

Loading model: meta-llama/Meta-Llama-3-8B

Setting up LoRA
  Rank: 16, Alpha: 32
  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196

Preparing datasets

Starting training
  Epochs: 3
  Effective batch size: 16
  Learning rate: 0.0001
{'loss': 1.4375, 'grad_norm': 1.18000328540802, 'learning_rate': 2.0081967213114755e-05, 'epoch': 0.03}
{'loss': 0.8612, 'grad_norm': 0.9492285847663879, 'learning_rate': 4.057377049180328e-05, 'epoch': 0.06}
{'loss': 0.8496, 'grad_norm': 1.0445971488952637, 'learning_rate': 6.10655737704918e-05, 'epoch': 0.09}
{'loss': 0.8259, 'grad_norm': 1.0660992860794067, 'learning_rate': 8.155737704918032e-05, 'epoch': 0.12}
{'loss': 0.8413, 'grad_norm': 0.9699179530143738, 'learning_rate': 9.999971237291203e-05, 'epoch': 0.15}
{'loss': 0.8587, 'grad_norm': 0.955994725227356, 'learning_rate': 9.996520112627602e-05, 'epoch': 0.18}
{'loss': 0.8184, 'grad_norm': 0.903451681137085, 'learning_rate': 9.987320995457625e-05, 'epoch': 0.22}
{'loss': 0.8172, 'grad_norm': 0.846505343914032, 'learning_rate': 9.972384468437874e-05, 'epoch': 0.25}
{'loss': 0.8143, 'grad_norm': 0.7934271693229675, 'learning_rate': 9.951727714536386e-05, 'epoch': 0.28}
{'loss': 0.862, 'grad_norm': 0.8725359439849854, 'learning_rate': 9.925374497265355e-05, 'epoch': 0.31}
{'eval_loss': 0.8161860704421997, 'eval_runtime': 64.7159, 'eval_samples_per_second': 30.889, 'eval_steps_per_second': 15.452, 'epoch': 0.31}
{'loss': 0.8255, 'grad_norm': 0.8344128131866455, 'learning_rate': 9.893355133343611e-05, 'epoch': 0.34}
{'loss': 0.8241, 'grad_norm': 0.8313395977020264, 'learning_rate': 9.8557064578203e-05, 'epoch': 0.37}
{'loss': 0.8122, 'grad_norm': 0.707967221736908, 'learning_rate': 9.812471781699857e-05, 'epoch': 0.4}
{'loss': 0.796, 'grad_norm': 0.8971372246742249, 'learning_rate': 9.76370084211708e-05, 'epoch': 0.43}
{'loss': 0.83, 'grad_norm': 0.7628297209739685, 'learning_rate': 9.709449745119565e-05, 'epoch': 0.46}
{'loss': 0.8477, 'grad_norm': 0.9339847564697266, 'learning_rate': 9.649780901123357e-05, 'epoch': 0.49}
{'loss': 0.8002, 'grad_norm': 0.8125378489494324, 'learning_rate': 9.58476295311606e-05, 'epoch': 0.52}
{'loss': 0.8124, 'grad_norm': 0.7539128661155701, 'learning_rate': 9.51447069769e-05, 'epoch': 0.55}
{'loss': 0.8089, 'grad_norm': 0.8236351609230042, 'learning_rate': 9.438984998996298e-05, 'epoch': 0.58}
{'loss': 0.8271, 'grad_norm': 0.7242188453674316, 'learning_rate': 9.358392695718805e-05, 'epoch': 0.62}
{'eval_loss': 0.7973976731300354, 'eval_runtime': 64.539, 'eval_samples_per_second': 30.974, 'eval_steps_per_second': 15.495, 'epoch': 0.62}
{'loss': 0.8031, 'grad_norm': 0.8395701050758362, 'learning_rate': 9.272786501174964e-05, 'epoch': 0.65}
{'loss': 0.8035, 'grad_norm': 0.7467700839042664, 'learning_rate': 9.182264896658486e-05, 'epoch': 0.68}
{'loss': 0.8166, 'grad_norm': 0.9756788611412048, 'learning_rate': 9.08693201814655e-05, 'epoch': 0.71}
{'loss': 0.8285, 'grad_norm': 0.7943572402000427, 'learning_rate': 8.986897536501864e-05, 'epoch': 0.74}
{'loss': 0.8123, 'grad_norm': 0.9831289052963257, 'learning_rate': 8.8822765313074e-05, 'epoch': 0.77}
{'loss': 0.7983, 'grad_norm': 0.7361637949943542, 'learning_rate': 8.77318935847894e-05, 'epoch': 0.8}
{'loss': 0.8123, 'grad_norm': 1.0200042724609375, 'learning_rate': 8.659761511807727e-05, 'epoch': 0.83}
{'loss': 0.8071, 'grad_norm': 0.877037525177002, 'learning_rate': 8.542123478592518e-05, 'epoch': 0.86}
{'loss': 0.8277, 'grad_norm': 0.9243825078010559, 'learning_rate': 8.420410589527104e-05, 'epoch': 0.89}
{'loss': 0.8065, 'grad_norm': 0.9018673300743103, 'learning_rate': 8.294762863015995e-05, 'epoch': 0.92}
{'eval_loss': 0.7895411252975464, 'eval_runtime': 64.753, 'eval_samples_per_second': 30.871, 'eval_steps_per_second': 15.443, 'epoch': 0.92}
{'loss': 0.7984, 'grad_norm': 0.816098690032959, 'learning_rate': 8.165324844097368e-05, 'epoch': 0.95}
{'loss': 0.8078, 'grad_norm': 0.9249463081359863, 'learning_rate': 8.032245438158576e-05, 'epoch': 0.98}
{'loss': 0.7267, 'grad_norm': 0.8744983077049255, 'learning_rate': 7.895677739635517e-05, 'epoch': 1.02}
{'loss': 0.6465, 'grad_norm': 0.991624116897583, 'learning_rate': 7.755778855892922e-05, 'epoch': 1.05}
{'loss': 0.6463, 'grad_norm': 1.056301474571228, 'learning_rate': 7.612709726488175e-05, 'epoch': 1.08}
{'loss': 0.633, 'grad_norm': 1.2619727849960327, 'learning_rate': 7.466634938026594e-05, 'epoch': 1.11}
{'loss': 0.6465, 'grad_norm': 0.9546632170677185, 'learning_rate': 7.317722534821117e-05, 'epoch': 1.14}
{'loss': 0.6556, 'grad_norm': 1.0734409093856812, 'learning_rate': 7.166143825574297e-05, 'epoch': 1.17}
{'loss': 0.6509, 'grad_norm': 1.289646029472351, 'learning_rate': 7.012073186304886e-05, 'epoch': 1.2}
{'loss': 0.622, 'grad_norm': 1.15953528881073, 'learning_rate': 6.855687859745827e-05, 'epoch': 1.23}
{'eval_loss': 0.8203549385070801, 'eval_runtime': 64.5977, 'eval_samples_per_second': 30.945, 'eval_steps_per_second': 15.48, 'epoch': 1.23}
{'loss': 0.653, 'grad_norm': 1.1058762073516846, 'learning_rate': 6.697167751444366e-05, 'epoch': 1.26}
{'loss': 0.6466, 'grad_norm': 1.1129292249679565, 'learning_rate': 6.536695222798851e-05, 'epoch': 1.29}
{'loss': 0.6355, 'grad_norm': 1.2623612880706787, 'learning_rate': 6.374454881270344e-05, 'epoch': 1.32}
{'loss': 0.654, 'grad_norm': 1.4507161378860474, 'learning_rate': 6.210633368010352e-05, 'epoch': 1.35}
{'loss': 0.6209, 'grad_norm': 1.2246125936508179, 'learning_rate': 6.045419143148997e-05, 'epoch': 1.38}
{'loss': 0.6391, 'grad_norm': 1.247246265411377, 'learning_rate': 5.879002268990653e-05, 'epoch': 1.42}
{'loss': 0.6339, 'grad_norm': 1.1386317014694214, 'learning_rate': 5.7115741913664264e-05, 'epoch': 1.45}
{'loss': 0.6001, 'grad_norm': 1.055040717124939, 'learning_rate': 5.5433275193950326e-05, 'epoch': 1.48}
{'loss': 0.6453, 'grad_norm': 1.313206434249878, 'learning_rate': 5.3744558039054296e-05, 'epoch': 1.51}
{'loss': 0.6073, 'grad_norm': 1.1567975282669067, 'learning_rate': 5.2051533147761155e-05, 'epoch': 1.54}
{'eval_loss': 0.81778883934021, 'eval_runtime': 65.3141, 'eval_samples_per_second': 30.606, 'eval_steps_per_second': 15.311, 'epoch': 1.54}
{'loss': 0.6348, 'grad_norm': 0.9682826399803162, 'learning_rate': 5.035614817447212e-05, 'epoch': 1.57}
{'loss': 0.6357, 'grad_norm': 1.1501178741455078, 'learning_rate': 4.866035348862476e-05, 'epoch': 1.6}
{'loss': 0.6256, 'grad_norm': 1.187319040298462, 'learning_rate': 4.696609993098965e-05, 'epoch': 1.63}
{'loss': 0.6164, 'grad_norm': 1.034209966659546, 'learning_rate': 4.527533656942472e-05, 'epoch': 1.66}
{'loss': 0.6388, 'grad_norm': 1.161155104637146, 'learning_rate': 4.359000845666936e-05, 'epoch': 1.69}
{'loss': 0.6126, 'grad_norm': 1.1510140895843506, 'learning_rate': 4.191205439275729e-05, 'epoch': 1.72}
{'loss': 0.6511, 'grad_norm': 1.4992308616638184, 'learning_rate': 4.02434046946227e-05, 'epoch': 1.75}
{'loss': 0.6169, 'grad_norm': 1.173160195350647, 'learning_rate': 3.858597897546526e-05, 'epoch': 1.78}
{'loss': 0.6216, 'grad_norm': 1.1288188695907593, 'learning_rate': 3.6941683936428716e-05, 'epoch': 1.82}
{'loss': 0.6154, 'grad_norm': 1.059301495552063, 'learning_rate': 3.531241117313359e-05, 'epoch': 1.85}
{'eval_loss': 0.8110549449920654, 'eval_runtime': 64.7544, 'eval_samples_per_second': 30.87, 'eval_steps_per_second': 15.443, 'epoch': 1.85}
{'loss': 0.6219, 'grad_norm': 1.0719398260116577, 'learning_rate': 3.370003499958703e-05, 'epoch': 1.88}
{'loss': 0.6249, 'grad_norm': 1.2627333402633667, 'learning_rate': 3.210641029197368e-05, 'epoch': 1.91}
{'loss': 0.625, 'grad_norm': 1.146325707435608, 'learning_rate': 3.053337035480765e-05, 'epoch': 1.94}
{'loss': 0.6121, 'grad_norm': 1.1366363763809204, 'learning_rate': 2.8982724811900564e-05, 'epoch': 1.97}
{'loss': 0.6199, 'grad_norm': 1.294213056564331, 'learning_rate': 2.7456257524571888e-05, 'epoch': 2.0}
{'loss': 0.4149, 'grad_norm': 1.8649184703826904, 'learning_rate': 2.5955724539496262e-05, 'epoch': 2.03}
{'loss': 0.3891, 'grad_norm': 1.6977964639663696, 'learning_rate': 2.4482852068549046e-05, 'epoch': 2.06}
{'loss': 0.3851, 'grad_norm': 1.415626883506775, 'learning_rate': 2.3039334502973542e-05, 'epoch': 2.09}
{'loss': 0.4002, 'grad_norm': 1.4438899755477905, 'learning_rate': 2.1626832464154785e-05, 'epoch': 2.12}
{'loss': 0.3774, 'grad_norm': 1.7728962898254395, 'learning_rate': 2.024697089324208e-05, 'epoch': 2.15}
{'eval_loss': 0.93965083360672, 'eval_runtime': 65.3942, 'eval_samples_per_second': 30.568, 'eval_steps_per_second': 15.292, 'epoch': 2.15}
{'loss': 0.3761, 'grad_norm': 1.80861496925354, 'learning_rate': 1.89013371818181e-05, 'epoch': 2.18}
{'loss': 0.3951, 'grad_norm': 1.5859729051589966, 'learning_rate': 1.7591479345764973e-05, 'epoch': 2.22}
{'loss': 0.3836, 'grad_norm': 1.9933902025222778, 'learning_rate': 1.6318904244428028e-05, 'epoch': 2.25}
{'loss': 0.3838, 'grad_norm': 1.830864667892456, 'learning_rate': 1.5085075847126213e-05, 'epoch': 2.28}
{'loss': 0.397, 'grad_norm': 1.4382374286651611, 'learning_rate': 1.389141354900294e-05, 'epoch': 2.31}
{'loss': 0.3787, 'grad_norm': 1.536794662475586, 'learning_rate': 1.2739290538155147e-05, 'epoch': 2.34}
{'loss': 0.3787, 'grad_norm': 1.738635540008545, 'learning_rate': 1.1630032215918862e-05, 'epoch': 2.37}
{'loss': 0.3783, 'grad_norm': 1.788270354270935, 'learning_rate': 1.0564914672128639e-05, 'epoch': 2.4}
{'loss': 0.3833, 'grad_norm': 1.590194582939148, 'learning_rate': 9.54516321710488e-06, 'epoch': 2.43}
{'loss': 0.4007, 'grad_norm': 1.923818588256836, 'learning_rate': 8.57195097205789e-06, 'epoch': 2.46}
{'eval_loss': 0.9463776350021362, 'eval_runtime': 64.6634, 'eval_samples_per_second': 30.914, 'eval_steps_per_second': 15.465, 'epoch': 2.46}
{'loss': 0.3872, 'grad_norm': 1.3585044145584106, 'learning_rate': 7.64639751953023e-06, 'epoch': 2.49}
{'loss': 0.3691, 'grad_norm': 1.539961338043213, 'learning_rate': 6.769567615429912e-06, 'epoch': 2.52}
{'loss': 0.3865, 'grad_norm': 2.0734803676605225, 'learning_rate': 5.942469964136055e-06, 'epoch': 2.55}
{'loss': 0.3595, 'grad_norm': 1.5341887474060059, 'learning_rate': 5.166056058086349e-06, 'epoch': 2.58}
{'loss': 0.3808, 'grad_norm': 1.5171470642089844, 'learning_rate': 4.441219083180786e-06, 'epoch': 2.62}
{'loss': 0.4026, 'grad_norm': 1.6633884906768799, 'learning_rate': 3.768792891261497e-06, 'epoch': 2.65}
{'loss': 0.3806, 'grad_norm': 1.874773383140564, 'learning_rate': 3.1495510408502404e-06, 'epoch': 2.68}
{'loss': 0.372, 'grad_norm': 1.4993289709091187, 'learning_rate': 2.584205907247339e-06, 'epoch': 2.71}
{'loss': 0.3589, 'grad_norm': 1.8328241109848022, 'learning_rate': 2.0734078630157304e-06, 'epoch': 2.74}
{'loss': 0.3767, 'grad_norm': 1.9660993814468384, 'learning_rate': 1.6177445297929527e-06, 'epoch': 2.77}
{'eval_loss': 0.9590736031532288, 'eval_runtime': 64.6535, 'eval_samples_per_second': 30.919, 'eval_steps_per_second': 15.467, 'epoch': 2.77}
{'loss': 0.3747, 'grad_norm': 1.6054110527038574, 'learning_rate': 1.2177401022916756e-06, 'epoch': 2.8}
{'loss': 0.3722, 'grad_norm': 2.0299625396728516, 'learning_rate': 8.738547452665446e-07, 'epoch': 2.83}
{'loss': 0.3595, 'grad_norm': 1.794176459312439, 'learning_rate': 5.864840641410907e-07, 'epoch': 2.86}
{'loss': 0.4003, 'grad_norm': 2.133349895477295, 'learning_rate': 3.5595864990352056e-07, 'epoch': 2.89}
{'loss': 0.3863, 'grad_norm': 1.6519654989242554, 'learning_rate': 1.825436987951512e-07, 'epoch': 2.92}
{'loss': 0.3778, 'grad_norm': 1.6433733701705933, 'learning_rate': 6.643870722889411e-08, 'epoch': 2.95}
{'loss': 0.3807, 'grad_norm': 1.8293927907943726, 'learning_rate': 7.777242288725672e-09, 'epoch': 2.98}
{'train_runtime': 10632.3573, 'train_samples_per_second': 7.336, 'train_steps_per_second': 0.459, 'train_loss': 0.6180912428635817, 'epoch': 3.0}

Saving adapter to ../models/wmt_adapters/llama3_de_en

Training complete.
Adapter saved to: ../models/wmt_adapters/llama3_de_en
Done at: Sat Nov 29 04:06:09 CET 2025
